UQ enabled - method : MC_dropout | step : 4
num_forward_passes : 16
DEBUG : args.distributed : False / args.rank : 0 / args.local_rank : -1 / args.world_size : -1 / args.gpu : 0
starting testing
UQ : True / UQ_method : MC_dropout
/pscratch/sd/y/ycryu/ENIGMA_OCD_MBBN/MBBN-main/splits/ENIGMA_OCD/ENIGMA_OCD_OCD_ROI_316_seq_len_100_split11.txt
loading splits
length of train_idx: 1467
length of val_idx: 312
length of test_idx: 314
workers: 1
length of training generator is: 366
workers: 1
length of valid generator is: 78
workers: 1
length of test generator is: 78
Number of training batches: 366
Number of validation batches: 78
Number of test batches: 78
self.task: test
Number of heads: 4
loading parameters onto new model...
Single GPU or CPU training
self.gpu: 0
self.device: cuda
moved model to: cuda:0
loaded model weights:
model location - /pscratch/sd/y/ycryu/ENIGMA_OCD_MBBN/MBBN-main/experiments/ENIGMA_OCD_mbbn_OCD_vmd_four_ch_seed11/ENIGMA_OCD_mbbn_OCD_vmd_four_ch_seed1_epoch_50_BEST_val_ACC.pth
last learning rate - 2.738019e-05
validation loss - 14.459459928854917
validation AUROC - 0.6443805218820629val_threshold - 0.4834045469760895
test
using binary_classification loss
using spatial_difference loss
test
Enabling MC Dropout for layer Dropout(p=0.3, inplace=False) - p=0.3
Enabling MC Dropout for layer Dropout(p=0.1, inplace=False) - p=0.1
Enabling MC Dropout for layer Dropout(p=0.3, inplace=False) - p=0.3
Enabling MC Dropout for layer Dropout(p=0.3, inplace=False) - p=0.3
Enabling MC Dropout for layer Dropout(p=0.1, inplace=False) - p=0.1
Enabling MC Dropout for layer Dropout(p=0.3, inplace=False) - p=0.3
Enabling MC Dropout for layer Dropout(p=0.3, inplace=False) - p=0.3
Enabling MC Dropout for layer Dropout(p=0.1, inplace=False) - p=0.1
Enabling MC Dropout for layer Dropout(p=0.3, inplace=False) - p=0.3
Enabling MC Dropout for layer Dropout(p=0.3, inplace=False) - p=0.3
Enabling MC Dropout for layer Dropout(p=0.1, inplace=False) - p=0.1
Enabling MC Dropout for layer Dropout(p=0.3, inplace=False) - p=0.3
Enabling MC Dropout for layer Dropout(p=0.3, inplace=False) - p=0.3
Enabling MC Dropout for layer Dropout(p=0.1, inplace=False) - p=0.1
Enabling MC Dropout for layer Dropout(p=0.3, inplace=False) - p=0.3
Enabling MC Dropout for layer Dropout(p=0.3, inplace=False) - p=0.3
Enabling MC Dropout for layer Dropout(p=0.1, inplace=False) - p=0.1
Enabling MC Dropout for layer Dropout(p=0.3, inplace=False) - p=0.3
Enabling MC Dropout for layer Dropout(p=0.3, inplace=False) - p=0.3
Enabling MC Dropout for layer Dropout(p=0.1, inplace=False) - p=0.1
Enabling MC Dropout for layer Dropout(p=0.3, inplace=False) - p=0.3
Enabling MC Dropout for layer Dropout(p=0.3, inplace=False) - p=0.3
Enabling MC Dropout for layer Dropout(p=0.1, inplace=False) - p=0.1
Enabling MC Dropout for layer Dropout(p=0.3, inplace=False) - p=0.3
Enabling MC Dropout for layer Dropout(p=0.3, inplace=False) - p=0.3
Enabling MC Dropout for layer Dropout(p=0.0, inplace=False) - p=0.0
Enabling MC Dropout for layer Dropout(p=0.0, inplace=False) - p=0.0
Enabling MC Dropout for layer Dropout(p=0.0, inplace=False) - p=0.0
Enabling MC Dropout for layer Dropout(p=0.0, inplace=False) - p=0.0
Enabling MC Dropout for layer Dropout(p=0.6, inplace=False) - p=0.6
calculating best thresholds with roc_curve
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  83086 KiB | 239657 KiB |    799 GiB |    799 GiB |
|       from large pool |  68992 KiB | 209641 KiB |    371 GiB |    371 GiB |
|       from small pool |  14094 KiB |  35275 KiB |    428 GiB |    428 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  83086 KiB | 239657 KiB |    799 GiB |    799 GiB |
|       from large pool |  68992 KiB | 209641 KiB |    371 GiB |    371 GiB |
|       from small pool |  14094 KiB |  35275 KiB |    428 GiB |    428 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  83045 KiB | 236845 KiB |    784 GiB |    784 GiB |
|       from large pool |  68992 KiB | 206906 KiB |    355 GiB |    355 GiB |
|       from small pool |  14053 KiB |  35198 KiB |    428 GiB |    428 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 276480 KiB | 276480 KiB | 276480 KiB |      0 B   |
|       from large pool | 237568 KiB | 237568 KiB | 237568 KiB |      0 B   |
|       from small pool |  38912 KiB |  38912 KiB |  38912 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  15218 KiB |  77261 KiB |    912 GiB |    912 GiB |
|       from large pool |  12928 KiB |  66160 KiB |    448 GiB |    448 GiB |
|       from small pool |   2290 KiB |  11101 KiB |    463 GiB |    463 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     152    |     289    |     992 K  |     992 K  |
|       from large pool |      17    |      43    |     123 K  |     123 K  |
|       from small pool |     135    |     248    |     869 K  |     869 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     152    |     289    |     992 K  |     992 K  |
|       from large pool |      17    |      43    |     123 K  |     123 K  |
|       from small pool |     135    |     248    |     869 K  |     869 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      33    |      33    |      33    |       0    |
|       from large pool |      14    |      14    |      14    |       0    |
|       from small pool |      19    |      19    |      19    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      11    |      59    |  393086    |  393075    |
|       from large pool |       4    |      16    |  110480    |  110476    |
|       from small pool |       7    |      43    |  282606    |  282599    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

