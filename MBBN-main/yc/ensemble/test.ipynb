{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "num_gpus = torch.cuda.device_count()\n",
    "for i in range(num_gpus):\n",
    "    device = torch.device(f'cuda:{i}')\n",
    "print(torch.cuda.current_device())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/pscratch/sd/y/ycryu/ENIGMA_OCD_MBBN/MBBN-main')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UQ_Writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dill\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from torch import nn\n",
    "\n",
    "from utils import *\n",
    "from trainer import Trainer\n",
    "from loss_writer import Writer\n",
    "from metrics import Metrics\n",
    "\n",
    "class UQWriter(Writer):\n",
    "    def __init__(self, sets, val_threshold, **kwargs):\n",
    "        super().__init__(sets, val_threshold, **kwargs)\n",
    "        self.confidence_list = []\n",
    "        self.is_correct_list = []\n",
    "        self.uncertainty_statistics_dict = {}\n",
    "        self.uncertainty_quantification_stat = {}\n",
    "\n",
    "    def compute_confidence(self, confidence_list: list, is_correct_list: list):\n",
    "        num_bins = 10\n",
    "        bin_edges = np.linspace(0.0, 1.0, num_bins + 1)  # Bin edges from 0 to 1\n",
    "        bin_indices = np.digitize(confidence_list, bin_edges, right=True)\n",
    "        bin_middlepoint = (bin_edges[1:] + bin_edges[:-1])/2\n",
    "\n",
    "        bin_confidences = []\n",
    "        bin_accuracies = []\n",
    "        bin_gaps = []\n",
    "        confidence_nparray = np.array(confidence_list)\n",
    "        is_correct_nparray = np.array(is_correct_list)\n",
    "        # ECE is weighted average of calibration error in each bin\n",
    "        # MCE is maximum calibration error in each bin\n",
    "        cum_ce = 0\n",
    "        mce = 0\n",
    "\n",
    "        # organizing bin elements\n",
    "        for i in range(1, num_bins + 1):\n",
    "            indices = np.where(bin_indices == i)[0]  # Get indices of elements in the bin\n",
    "            if len(indices) > 0:\n",
    "                avg_confidence = np.mean(confidence_nparray[indices])  # Average confidence\n",
    "                avg_accuracy = np.mean(is_correct_nparray[indices])  # Accuracy as mean of correct labels\n",
    "                gap = avg_confidence - avg_accuracy  # Gap between confidence and accuracy\n",
    "\n",
    "                bin_confidences.append(avg_confidence)\n",
    "                bin_accuracies.append(avg_accuracy)\n",
    "                bin_gaps.append(gap)\n",
    "                cum_ce += np.abs(gap) * len(indices)\n",
    "                mce = max(mce, np.abs(gap))\n",
    "            else:\n",
    "                bin_confidences.append(0)\n",
    "                bin_accuracies.append(0)\n",
    "                bin_gaps.append(0)\n",
    "        \n",
    "        ece = cum_ce / len(confidence_list)\n",
    "\n",
    "\n",
    "        # FAR95 statistics\n",
    "        far95, threshold, fpr, tpr = self.metrics.compute_far95(confidence_list, is_correct_list) # Returns None if no threshold found\n",
    "        if far95 is None:\n",
    "            print(\"\\nFalse Acceptance Rate at 95% Recall: No threshold found\")\n",
    "        \n",
    "        # Drawing ROC curve\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(fpr, tpr, marker='o', linestyle='-', label='ROC curve')\n",
    "        plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Chance')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # Save the ROC plot\n",
    "        roc_save_path = os.path.join(self.kwargs.get(\"experiment_folder\"), 'roc_curve.png')\n",
    "        plt.savefig(roc_save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        # Save the ECE/MCE/FAR95 statistics\n",
    "        self.uncertainty_quantification_stat['ece'] = ece\n",
    "        self.uncertainty_quantification_stat['mce'] = mce\n",
    "        self.uncertainty_quantification_stat['far95'] = far95\n",
    "        self.uncertainty_quantification_stat['threshold'] = threshold\n",
    "\n",
    "        stat_save_path = os.path.join(self.kwargs.get(\"experiment_folder\"), 'statistics.txt')\n",
    "        with open(stat_save_path, 'w') as f:\n",
    "            f.write(\"==========All samples evaluated==========\\n\")\n",
    "            f.write(f\"Expected Calibration Error: {ece}\\n\")\n",
    "            f.write(f\"Maximum Calibration Error: {mce}\\n\")\n",
    "            if far95 is not None:\n",
    "                f.write(f\"False Acceptance Rate at 95% Recall: {far95} (threshold: {threshold})\\n\")\n",
    "\n",
    "        # drawing plot\n",
    "        bar_width = 0.08  # Width of the bars\n",
    "        plt.figure(figsize=(8, 6))\n",
    "\n",
    "        plt.bar(bin_edges[:-1], bin_accuracies, width=bar_width, align='edge', color='blue', edgecolor='black', label=\"Outputs\")\n",
    "        plt.bar(bin_edges[:-1], bin_gaps, width=bar_width, align='edge', color='pink', alpha=0.7, label=\"Gap\", bottom=bin_accuracies)\n",
    "        plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label=\"Perfect Calibration\")\n",
    "\n",
    "        plt.text(0.7, 0.1, f'ECE={ece:.4f}', fontsize=14, bbox=dict(facecolor='lightgray', alpha=0.5))\n",
    "\n",
    "        plt.xlabel('Confidence')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title('Reliability Diagram')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.xlim([0, 1])\n",
    "        plt.ylim([0, 1])\n",
    "\n",
    "        diagram_save_path = os.path.join(self.kwargs.get(\"experiment_folder\"), 'reliability_diagram.png')\n",
    "        plt.savefig(diagram_save_path, dpi=300, bbox_inches='tight')\n",
    "\n",
    "    def sample_uncertainty_statistic(self, subj_name: int, subj_dict: dict, subj_truth: int):\n",
    "\n",
    "        sample_prediction = 1 if subj_dict['score'].mean().item() > 0.5 else 0\n",
    "        is_correct = 1 if subj_truth == sample_prediction else 0\n",
    "\n",
    "        # probabilities list for predicted sample \n",
    "        # (which 0 or 1, prob of 1 when sample_prediction == 1 and prob of 0 when sample_prediction == 0)\n",
    "        if sample_prediction == 1:\n",
    "            sample_pred_probabilities_list = subj_dict['score'].tolist()\n",
    "        else:\n",
    "            sample_pred_probabilities_list = (1 - subj_dict['score']).tolist()\n",
    "        \n",
    "        # Calculate uncertainty\n",
    "        mean = torch.mean(torch.tensor(sample_pred_probabilities_list), axis=0)\n",
    "        variance = torch.var(torch.tensor(sample_pred_probabilities_list), axis=0)\n",
    "\n",
    "        confidence_intervals_dict = {}\n",
    "        confidence_levels = [0.9, 0.95]\n",
    "        for confidence_level in confidence_levels:\n",
    "            lower_percentile = (1 - confidence_level) / 2 * 100  # 2.5% for 95% CI\n",
    "            upper_percentile = (1 + confidence_level) / 2 * 100  # 97.5% for 95% CI\n",
    "\n",
    "            # Compute confidence intervals for each class\n",
    "            probabilities_list = torch.stack([(1 - subj_dict['score']), subj_dict['score']], dim=1)\n",
    "            confidence_intervals = np.percentile(probabilities_list, [lower_percentile, upper_percentile], axis=0)\n",
    "            confidence_intervals_dict[confidence_level] = confidence_intervals\n",
    "\n",
    "        stat_save_path = os.path.join(self.kwargs.get(\"experiment_folder\"), 'sample_statistics.txt')\n",
    "        with open(stat_save_path, 'a') as f:\n",
    "            f.write(f\"\\nStatistics for sample {subj_name}\\n\")\n",
    "            f.write(f\"No. of forward passes: {len(sample_pred_probabilities_list)}\\n\")\n",
    "            f.write(f\"Final prediction: {sample_prediction}\\n\")\n",
    "            f.write(f\"True label: {subj_truth}\\n\")\n",
    "            f.write(f\"Correct: {True if is_correct else False}\\n\")\n",
    "            f.write(f\"Prediction probability: {mean}\\n\")\n",
    "            f.write(f\"Variance: {variance}\\n\")\n",
    "            for confidence_level in confidence_levels:\n",
    "                for i in range(2):\n",
    "                    f.write(f\"Class {i}: {confidence_level * 100}% CI = [{confidence_intervals_dict[confidence_level][0, i]}, {confidence_intervals_dict[confidence_level][1, i]}]\\n\")\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "        self.uncertainty_statistics_dict[subj_name] = {\n",
    "            'mean': mean,\n",
    "            'variance': variance,\n",
    "            'confidence_intervals': confidence_intervals_dict,\n",
    "            'is_correct': is_correct,\n",
    "            'sample_prediction': sample_prediction,\n",
    "            'sample_pred_probabilities_list': sample_pred_probabilities_list,\n",
    "            'truth': subj_truth\n",
    "        }\n",
    "\n",
    "        return mean, is_correct\n",
    "\n",
    "    \n",
    "    def accuracy_summary(self, mid_epoch, mean, std):\n",
    "        pred_all_sets = {x:[] for x in self.sets}   # dictionary to store predictions\n",
    "        truth_all_sets = {x:[] for x in self.sets}  # dictionary to store ground truth values\n",
    "        std_all_sets = {x:[] for x in self.sets}  # dictionary to store prediction errors\n",
    "        metrics = {}\n",
    "        confidence_list = []\n",
    "        is_correct_list = []\n",
    "        \n",
    "        for subj_name,subj_dict in self.subject_accuracy.items():  # per-subject prediction scores (score), ground truth labels (truth), and the set (mode) they belong to\n",
    "            \n",
    "            if self.fine_tune_task == 'binary_classification':\n",
    "                subj_dict['score'] = torch.sigmoid(subj_dict['score'].float())\n",
    "\n",
    "            # subj_dict['score'] denotes the logits for sequences for a subject\n",
    "            subj_pred = subj_dict['score'].mean().item() \n",
    "            subj_error = subj_dict['score'].std().item()\n",
    "\n",
    "            subj_truth = subj_dict['truth'].item()\n",
    "            subj_mode = subj_dict['mode'] # train, val, test\n",
    "\n",
    "            conf, is_corr = self.sample_uncertainty_statistic(subj_name, subj_dict, subj_truth)\n",
    "            confidence_list.append(conf)\n",
    "            is_correct_list.append(is_corr)\n",
    "\n",
    "            # with open(os.path.join(self.per_subject_predictions,'iter_{}.txt'.format(self.eval_iter)),'a+') as f:\n",
    "            #     f.write('subject:{} ({})\\noutputs: {:.4f}\\u00B1{:.4f}  -  truth: {}\\n'.format(subj_name,subj_mode,subj_pred,subj_error,subj_truth))\n",
    "            \n",
    "            pred_all_sets[subj_mode].append(subj_pred) # don't use std in computing AUROC, ACC\n",
    "            std_all_sets[subj_mode].append(subj_error)\n",
    "            truth_all_sets[subj_mode].append(subj_truth)\n",
    "\n",
    "        for (name,pred),(_, std),(_,truth) in zip(pred_all_sets.items(), std_all_sets.items(), truth_all_sets.items()):\n",
    "            if len(pred) == 0:\n",
    "                continue\n",
    "\n",
    "            if self.fine_tune_task == 'regression':\n",
    "                ## return to original scale ##\n",
    "                unnormalized_pred = [i * std + mean for i in pred]\n",
    "                unnormalized_truth = [i * std + mean for i in truth]\n",
    "\n",
    "                metrics[name + '_MAE'] = self.metrics.MAE(unnormalized_truth,unnormalized_pred)\n",
    "                metrics[name + '_MSE'] = self.metrics.MSE(unnormalized_truth,unnormalized_pred)\n",
    "                metrics[name +'_NMSE'] = self.metrics.NMSE(unnormalized_truth,unnormalized_pred)\n",
    "                metrics[name + '_R2_score'] = self.metrics.R2_score(unnormalized_truth,unnormalized_pred)\n",
    "                \n",
    "            else:\n",
    "                metrics[name + '_Balanced_Accuracy'] = self.metrics.BAC(truth,[x>0.5 for x in torch.Tensor(pred)])\n",
    "                metrics[name + '_Regular_Accuracy'] = self.metrics.RAC(truth,[x>0.5 for x in torch.Tensor(pred)]) # Stella modified it\n",
    "                metrics[name + '_AUROC'] = self.metrics.AUROC(truth,pred)             \n",
    "                metrics[name +'_best_bal_acc'], metrics[name + '_best_threshold'],metrics[name + '_gmean'],metrics[name + '_specificity'],metrics[name + '_sensitivity'],metrics[name + '_f1_score'] = self.metrics.ROC_CURVE(truth,pred,name,self.val_threshold)\n",
    "\n",
    "            self.current_metrics = metrics\n",
    "            \n",
    "            \n",
    "        for name,value in metrics.items():\n",
    "            self.scalar_to_tensorboard(name,value)\n",
    "            if hasattr(self,name):\n",
    "                l = getattr(self,name)\n",
    "                l.append(value)\n",
    "                setattr(self,name,l)\n",
    "            else:\n",
    "                setattr(self, name, [value])\n",
    "                \n",
    "        self.eval_iter += 1\n",
    "        if mid_epoch and len(self.subject_accuracy) > 0:\n",
    "            self.subject_accuracy = {k: v for k, v in self.subject_accuracy.items() if v['mode'] == 'train'}\n",
    "        else:\n",
    "            self.subject_accuracy = {}\n",
    "\n",
    "        self.confidence_list = confidence_list\n",
    "        self.is_correct_list = is_correct_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UQ_Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mUQTrainer\u001b[39;00m(\u001b[43mTrainer\u001b[49m):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, sets, model_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(sets, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Trainer' is not defined"
     ]
    }
   ],
   "source": [
    "class UQTrainer(Trainer):\n",
    "\n",
    "    def __init__(self, sets, model_idx = None, **kwargs):\n",
    "        super().__init__(sets, **kwargs)\n",
    "        self.model_idx = model_idx\n",
    "        self.writer = UQWriter(sets, self.val_threshold, **kwargs)\n",
    "        print(f\"model_idx: {model_idx}\")\n",
    "    \n",
    "    ## Should be changed to save at asdf_epoch{1}/checkpoint_model{0}.pth\n",
    "    ## YC : CHANGED\n",
    "    def save_checkpoint_(self, epoch, batch_idx, scaler):\n",
    "        model_idx = self.model_idx\n",
    "\n",
    "        loss = self.get_last_loss()\n",
    "        #accuracy = self.get_last_AUROC()\n",
    "        val_ACC = self.get_last_ACC()\n",
    "        val_best_ACC = self.get_last_best_ACC()\n",
    "        val_AUROC = self.get_last_AUROC()\n",
    "        val_MAE = self.get_last_MAE()\n",
    "        val_threshold = self.get_last_val_threshold()\n",
    "\n",
    "        if self.method == 'ensemble':\n",
    "            if model_idx is None:\n",
    "                raise ValueError(\"model_idx must be provided for ensemble method.\")\n",
    "                \n",
    "            title = str(self.writer.experiment_title) + '_epoch_' + str(epoch)\n",
    "            directory = os.path.join(self.writer.experiment_folder, '_model_{}'.format(model_idx))\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "            if self.amp:\n",
    "                amp_state = scaler.state_dict()\n",
    "        \n",
    "        else:\n",
    "            title = str(self.writer.experiment_title) + '_epoch_' + str(int(epoch))\n",
    "            directory = self.writer.experiment_folder\n",
    "\n",
    "            # Create directory to save to\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "            if self.amp:\n",
    "                amp_state = scaler.state_dict()\n",
    "\n",
    "        # Build checkpoint dict to save.\n",
    "        ckpt_dict = {\n",
    "            # 'model_state_dict':self.model.module.state_dict(),  # Distributed case\n",
    "            'model_state_dict':self.model.module.state_dict() if hasattr(self.model, \"module\") else self.model.state_dict(),\n",
    "            'optimizer_state_dict':self.optimizer.state_dict() if self.optimizer is not None else None,\n",
    "            'epoch':epoch,\n",
    "            'loss_value':loss,\n",
    "            'amp_state': amp_state}\n",
    "\n",
    "        # if val_ACC is not None:\n",
    "        #     ckpt_dict['val_ACC'] = val_ACC\n",
    "        if val_AUROC is not None:\n",
    "            ckpt_dict['val_AUROC'] = val_AUROC\n",
    "        if val_threshold is not None:\n",
    "            ckpt_dict['val_threshold'] = val_threshold\n",
    "        if val_MAE is not None:\n",
    "            ckpt_dict['val_MAE'] = val_MAE\n",
    "        if self.lr_handler.schedule is not None:\n",
    "            ckpt_dict['schedule_state_dict'] = self.lr_handler.schedule.state_dict()\n",
    "            ckpt_dict['lr'] = self.optimizer.param_groups[0]['lr']\n",
    "            print(f\"current_lr:{self.optimizer.param_groups[0]['lr']}\")\n",
    "        if hasattr(self,'loaded_model_weights_path'):\n",
    "            ckpt_dict['loaded_model_weights_path'] = self.loaded_model_weights_path\n",
    "        \n",
    "        # classification\n",
    "        if val_AUROC is not None:\n",
    "            if self.best_AUROC < val_AUROC:\n",
    "                self.best_AUROC = val_AUROC\n",
    "                name = \"{}_BEST_val_AUROC.pth\".format(title)\n",
    "                torch.save(ckpt_dict, os.path.join(directory, name))\n",
    "                print(f'updating best saved model with AUROC:{val_AUROC}')\n",
    "\n",
    "                if self.best_ACC < val_ACC:\n",
    "                    self.best_ACC = val_ACC\n",
    "            elif self.best_AUROC >= val_AUROC:\n",
    "                # If model is not improved in val AUROC, but improved in val ACC.\n",
    "                if self.best_ACC < val_ACC:\n",
    "                    self.best_ACC = val_ACC\n",
    "                    name = \"{}_BEST_val_ACC.pth\".format(title)\n",
    "                    torch.save(ckpt_dict, os.path.join(directory, name))\n",
    "                    print(f'updating best saved model with ACC:{val_ACC}')\n",
    "\n",
    "        # regression\n",
    "        elif val_AUROC is None and val_MAE is not None:\n",
    "            if self.best_MAE > val_MAE:\n",
    "                self.best_MAE = val_MAE\n",
    "                name = \"{}_BEST_val_MAE.pth\".format(title)\n",
    "                torch.save(ckpt_dict, os.path.join(directory, name))\n",
    "                print(f'updating best saved model with MAE: {val_MAE}')\n",
    "            else:\n",
    "                pass\n",
    "                \n",
    "        else:\n",
    "            if self.best_loss > loss:\n",
    "                self.best_loss = loss\n",
    "                name = \"{}_BEST_val_loss.pth\".format(title)\n",
    "                torch.save(ckpt_dict, os.path.join(directory, name))\n",
    "                print(f'updating best saved model with loss: {loss}')\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "    def set_model_device(self):  # assigns the model to appropriate devices (e.g., GPU or CPU)\n",
    "        if self.distributed:\n",
    "            # For multiprocessing distributed, DistributedDataParallel constructor\n",
    "            # should always set the single device scope, otherwise,\n",
    "            # DistributedDataParallel will use all available devices.\n",
    "            \n",
    "            ### DEBUG STATEMENT ###\n",
    "            print(f\"self.gpu: {self.gpu}\")\n",
    "            if self.gpu is None:\n",
    "                print(\"self.gpu is None\")\n",
    "            #######################\n",
    "            \n",
    "            if self.gpu is not None:\n",
    "                print('id of gpu is:', self.gpu)\n",
    "                self.device = torch.device('cuda:{}'.format(self.gpu))\n",
    "                torch.cuda.set_device(self.gpu)\n",
    "                self.model.cuda(self.gpu)\n",
    "                self.model = torch.nn.parallel.DistributedDataParallel(self.model, device_ids=[self.gpu], broadcast_buffers=False, find_unused_parameters=True) \n",
    "                net_without_ddp = self.model.module\n",
    "            else:\n",
    "                \n",
    "                ### DEBUG STATEMENT ###\n",
    "                print(\"Distributed training without specific GPU assignment\")\n",
    "                #######################\n",
    "                \n",
    "                self.device = torch.device(\"cuda\" if self.cuda else \"cpu\")\n",
    "                self.model.cuda()\n",
    "                if 'reconstruction' in self.task.lower():\n",
    "                    self.model = torch.nn.parallel.DistributedDataParallel(self.model) \n",
    "                else: # having unused parameter (classifier token)\n",
    "                    self.model = torch.nn.parallel.DistributedDataParallel(self.model,find_unused_parameters=True) \n",
    "                model_without_ddp = self.model.module\n",
    "        else:\n",
    "            \n",
    "            ### DEBUG STATEMENT ###\n",
    "            print(\"Single GPU or CPU training\")\n",
    "            #######################\n",
    "            \n",
    "            self.device = torch.device(\"cuda\" if self.cuda else \"cpu\")\n",
    "            \n",
    "            ### DEBUG STATEMENT ###\n",
    "            print(f\"self.gpu: {self.gpu}\")\n",
    "            print(f\"self.device: {self.device}\")\n",
    "            #######################\n",
    "            \n",
    "            #self.model = DataParallel(self.model).to(self.device)\n",
    "            \n",
    "            ### DEBUG STATEMENT ###\n",
    "            self.device = torch.device(\"cuda:0\")   # added for debugging\n",
    "            self.model = self.model.to(self.device)  \n",
    "            #######################\n",
    "            \n",
    "            ### DEBUG STATEMENT ###\n",
    "            print(f\"moved model to: {self.device}\")\n",
    "            #######################\n",
    "\n",
    "\n",
    "    def eval(self,set):\n",
    "        ## If set == 'MC_dropout', then set dropout to True\n",
    "        if set not in ['MC_dropout', 'train', 'val', 'test']:\n",
    "            raise ValueError(f\"Invalid set: {set}\")\n",
    "        self.mode = set\n",
    "        if set == 'MC_dropout':\n",
    "            for layer in self.model.modules():\n",
    "                if isinstance(layer, nn.Dropout):\n",
    "                    print(f\"Enabling MC Dropout for layer {layer} - p={layer.p}\")\n",
    "                    layer.train()\n",
    "        else:\n",
    "            self.model = self.model.eval()\n",
    "\n",
    "    def finish_eval(self, set):\n",
    "        if set not in ['MC_dropout', 'train', 'val', 'test']:\n",
    "            raise ValueError(f\"Invalid set: {set}\")\n",
    "        if set == 'MC_dropout':\n",
    "            self.model = self.model.eval()\n",
    "\n",
    "    def concat_batch_results(self, inout_batches: list):\n",
    "        inout_keys = inout_batches[0].keys()\n",
    "        concat_inout = dict()\n",
    "        for inout in inout_batches:\n",
    "            for key in inout_keys:\n",
    "                if key not in concat_inout:\n",
    "                    concat_inout[key] = inout[key]\n",
    "                else:\n",
    "                    if isinstance(inout[key], list):\n",
    "                        concat_inout[key] += inout[key]\n",
    "                    elif isinstance(inout[key], torch.Tensor):\n",
    "                        concat_inout[key] = torch.cat((concat_inout[key], inout[key]), dim=0)\n",
    "                    else:\n",
    "                        raise ValueError(f\"Invalid inout type: {type(inout[key])}\")\n",
    "        \n",
    "        return concat_inout\n",
    "    \n",
    "    def forward_pass(self,input_dict): \n",
    "        input_dict = {\n",
    "            k: (\n",
    "                v.to(self.device) if (self.cuda and torch.is_tensor(v)) else v\n",
    "            ) for k, v in input_dict.items()\n",
    "        }\n",
    "        for k, v in input_dict.items():\n",
    "            if torch.is_tensor(v):\n",
    "                if not v.is_contiguous():\n",
    "                    v = v.contiguous()\n",
    "        \n",
    "        if self.task.lower() == 'test':\n",
    "            if self.fmri_type in ['timeseries', 'frequency', 'time_domain_high', 'time_domain_low', 'time_domain_ultralow', 'frequency_domain_low', 'frequency_domain_ultralow', 'frequency_domain_high']:\n",
    "                output_dict = self.model(input_dict['fmri_sequence'])\n",
    "            elif self.fmri_type == 'divided_timeseries':\n",
    "                if self.fmri_dividing_type == 'two_channels':\n",
    "                    output_dict = self.model(input_dict['fmri_lowfreq_sequence'], input_dict['fmri_ultralowfreq_sequence'])\n",
    "                elif self.fmri_dividing_type == 'three_channels':\n",
    "                    output_dict = self.model(input_dict['fmri_highfreq_sequence'], input_dict['fmri_lowfreq_sequence'], input_dict['fmri_ultralowfreq_sequence'])\n",
    "                elif self.fmri_dividing_type == 'four_channels':\n",
    "                    output_dict = self.model(input_dict['fmri_imf1_sequence'], input_dict['fmri_imf2_sequence'], input_dict['fmri_imf3_sequence'], input_dict['fmri_imf4_sequence'])\n",
    "\n",
    "        \n",
    "        #### train & valid ####\n",
    "        else:\n",
    "            if self.fmri_type in ['timeseries', 'frequency', 'time_domain_high', 'time_domain_low', 'time_domain_ultralow', 'frequency_domain_low', 'frequency_domain_ultralow', 'frequency_domain_high']:\n",
    "                output_dict = self.model(input_dict['fmri_sequence'])\n",
    "            elif self.fmri_type == 'divided_timeseries':\n",
    "                if self.fmri_dividing_type == 'two_channels':\n",
    "                    output_dict = self.model(input_dict['fmri_lowfreq_sequence'], input_dict['fmri_ultralowfreq_sequence'])\n",
    "                elif self.fmri_dividing_type == 'three_channels':\n",
    "                    output_dict = self.model(input_dict['fmri_highfreq_sequence'], input_dict['fmri_lowfreq_sequence'], input_dict['fmri_ultralowfreq_sequence'])\n",
    "                elif self.fmri_dividing_type == 'four_channels':\n",
    "                    output_dict = self.model(input_dict['fmri_imf1_sequence'], input_dict['fmri_imf2_sequence'], input_dict['fmri_imf3_sequence'], input_dict['fmri_imf4_sequence'])\n",
    "                    \n",
    "                    torch.cuda.synchronize()\n",
    "                                \n",
    "        return input_dict, output_dict\n",
    "\n",
    "    def eval_epoch(self,set):  # evaluates the model for a single epoch\n",
    "        loader = self.test_loader\n",
    "        subset_indices = list(range(len(self.test_loader.dataset))) * self.num_forward_passes\n",
    "        subset = Subset(self.test_loader.dataset, subset_indices)\n",
    "        loader = DataLoader(subset, batch_size=8, shuffle=False, num_workers=0)\n",
    "        subject_names = [data['subject_name'] for data in loader.dataset]\n",
    "\n",
    "        self.eval(set)\n",
    "        input_batches = []\n",
    "        output_batches = []\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, input_dict in enumerate(tqdm(loader, position=0, leave=True)):\n",
    "                with autocast():\n",
    "                    input_dict, output_dict = self.forward_pass(input_dict)\n",
    "                    input_batches.append(input_dict)\n",
    "                    output_batches.append(output_dict)\n",
    "\n",
    "        self.finish_eval(set)\n",
    "        return input_batches, output_batches\n",
    "\n",
    "    def testing(self):  # manages the testing phase of the model\n",
    "        # options = ['MC_dropout']\n",
    "        roc_save_path = os.path.join(self.kwargs.get(\"experiment_folder\"), 'roc_curve.png')\n",
    "        stat_save_path = os.path.join(self.kwargs.get(\"experiment_folder\"), 'statistics.txt')\n",
    "        samp_stat_save_path = os.path.join(self.kwargs.get(\"experiment_folder\"), 'sample_statistics.txt')\n",
    "        if os.path.exists(roc_save_path):\n",
    "            os.remove(roc_save_path)\n",
    "        if os.path.exists(stat_save_path):\n",
    "            os.remove(stat_save_path)\n",
    "        if os.path.exists(samp_stat_save_path):\n",
    "            os.remove(samp_stat_save_path)\n",
    "\n",
    "        input_batches, output_batches = self.eval_epoch('MC_dropout')\n",
    "        inputs = self.concat_batch_results(input_batches)\n",
    "        outputs = self.concat_batch_results(output_batches)\n",
    "\n",
    "        self.compute_accuracy(inputs, outputs)\n",
    "        self.writer.accuracy_summary(mid_epoch=False, mean=None, std=None)\n",
    "        self.writer.compute_confidence(self.writer.confidence_list, self.writer.is_correct_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *  #including 'init_distributed', 'weight_loader'\n",
    "from trainer import Trainer\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "## YC : CHANGED\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "\n",
    "def get_arguments(base_path):\n",
    "    \"\"\"\n",
    "    handle arguments from commandline.\n",
    "    some other hyper parameters can only be changed manually (such as model architecture,dropout,etc)\n",
    "    notice some arguments are global and take effect for the entire three phase training process, while others are determined per phase\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--exp_name', type=str,default=\"baseline\") \n",
    "    parser.add_argument('--dataset_name', type=str, choices=['HCP1200', 'ABCD', 'ABIDE', 'UKB', 'ENIGMA_OCD'], default=\"ENIGMA_OCD\")\n",
    "    parser.add_argument('--fmri_type', type=str, choices=['timeseries', 'frequency', 'divided_timeseries', 'time_domain_low', 'time_domain_ultralow', 'time_domain_high', 'frequency_domain_low', 'frequency_domain_ultralow', 'frequency_domain_high'], default=\"divided_timeseries\")\n",
    "    parser.add_argument('--intermediate_vec', type=int, default=400)\n",
    "    parser.add_argument('--abcd_path', default='/scratch/connectome/stellasybae/ABCD_ROI/7.ROI') ## labserver\n",
    "    parser.add_argument('--ukb_path', default='/scratch/connectome/stellasybae/UKB_ROI') ## labserver\n",
    "    parser.add_argument('--abide_path', default='/scratch/connectome/stellasybae/ABIDE_ROI') ## labserver\n",
    "    parser.add_argument('--enigma_path', default='/pscratch/sd/p/pakmasha/MBBN_data') ## Perlmutter \n",
    "    parser.add_argument('--base_path', default=base_path) # where your main.py, train.py, model.py are in.\n",
    "    parser.add_argument('--step', default='1', choices=['1','2','3','4'], help='which step you want to run') # YC : Step 1 : vanilla_BERT / Step 2 : MBBN / Step 3 : divfreqBERT_reconstruction / Step 4 : test\n",
    "    \n",
    "    \n",
    "    parser.add_argument('--target', type=str, default='OCD')\n",
    "    parser.add_argument('--fine_tune_task',\n",
    "                        choices=['regression','binary_classification'],\n",
    "                        help='fine tune model objective. choose binary_classification in case of a binary classification task')\n",
    "    parser.add_argument('--seed', type=int, default=1)\n",
    "    parser.add_argument('--visualization', action='store_true')\n",
    "    parser.add_argument('--prepare_visualization', action='store_true')\n",
    "    parser.add_argument('--weightwatcher', action='store_true')\n",
    "    parser.add_argument('--weightwatcher_save_dir', default=None)\n",
    "\n",
    "    \n",
    "    \n",
    "    parser.add_argument('--norm_axis', default=1, type=int, choices=[0,1,None])\n",
    "    \n",
    "    parser.add_argument('--cuda', default=True)\n",
    "    parser.add_argument('--log_dir', type=str, default=os.path.join(base_path, 'runs'))\n",
    "\n",
    "    parser.add_argument('--transformer_hidden_layers', type=int,default=8)\n",
    "    \n",
    "    # DDP configs:\n",
    "    parser.add_argument('--world_size', default=-1, type=int, \n",
    "                        help='number of nodes for distributed training')\n",
    "    parser.add_argument('--rank', default=-1, type=int, \n",
    "                        help='node rank for distributed training')\n",
    "    parser.add_argument('--local_rank', default=-1, type=int, \n",
    "                        help='local rank for distributed training')\n",
    "    parser.add_argument('--dist_backend', default='nccl', type=str, \n",
    "                        help='distributed backend')\n",
    "    parser.add_argument('--init_method', default='file', type=str, choices=['file','env'], help='DDP init method')\n",
    "    parser.add_argument('--distributed', default=True)\n",
    "\n",
    "    # AMP configs:\n",
    "    parser.add_argument('--amp', action='store_false')\n",
    "    parser.add_argument('--gradient_clipping', action='store_true')\n",
    "    parser.add_argument('--clip_max_norm', type=float, default=1.0)\n",
    "    \n",
    "    # Gradient accumulation\n",
    "    parser.add_argument(\"--accumulation_steps\", default=1, type=int,required=False,help='mini batch size == accumulation_steps * args.train_batch_size')\n",
    "    \n",
    "    # Nsight profiling\n",
    "    parser.add_argument(\"--profiling\", action='store_true')\n",
    "    \n",
    "    #wandb related\n",
    "    parser.add_argument('--wandb_key', default='d0330ca06936eecd637c3470c47af6d33e1cb277', type=str,  help='default: key for ycryu')\n",
    "    parser.add_argument('--wandb_mode', default='online', type=str,  help='online|offline')\n",
    "    parser.add_argument('--wandb_entity', default='youngchanryu-seoul-national-university', type=str)\n",
    "    parser.add_argument('--wandb_project', default='enigma-ocd_mbbn', type=str)\n",
    "\n",
    "    \n",
    "    # dividing\n",
    "    parser.add_argument('--filtering_type', default='Boxcar', choices=['FIR', 'Boxcar'])\n",
    "    parser.add_argument('--use_high_freq', action='store_true')\n",
    "    parser.add_argument('--divide_by_lorentzian', action='store_true')\n",
    "    parser.add_argument('--use_raw_knee', action='store_true')\n",
    "    parser.add_argument('--seq_part', type=str, default='head')\n",
    "    parser.add_argument('--fmri_dividing_type', default='three_channels', choices=['two_channels', 'three_channels', 'four_channels'])\n",
    "    \n",
    "    # Dropouts\n",
    "    parser.add_argument('--transformer_dropout_rate', type=float, default=0.3) \n",
    "\n",
    "    # Architecture\n",
    "    parser.add_argument('--num_heads', type=int, default=12,\n",
    "                        help='number of heads for BERT network (default: 12)')\n",
    "    parser.add_argument('--attn_mask', action='store_false',\n",
    "                        help='use attention mask for Transformer (default: true)')\n",
    "                        \n",
    "    \n",
    "    ## for finetune\n",
    "    parser.add_argument('--pretrained_model_weights_path', default=None)\n",
    "    parser.add_argument('--finetune', action='store_true')\n",
    "    parser.add_argument('--finetune_test', action='store_true', help='test phase of finetuning task')\n",
    "    \n",
    "    \n",
    "    ## spatiotemporal\n",
    "    parser.add_argument('--spatiotemporal', action = 'store_true')\n",
    "    parser.add_argument('--spat_diff_loss_type', type=str, default='minus_log', choices=['minus_log', 'reciprocal_log', 'exp_minus', 'log_loss', 'exp_whole'])\n",
    "    parser.add_argument('--spatial_loss_factor', type=float, default=0.1)\n",
    "    \n",
    "    ## ablation\n",
    "    parser.add_argument('--ablation', type=str, choices=['convolution', 'no_high_freq'])\n",
    "    \n",
    "    ## YC : Phase means step\n",
    "    ## phase 1 vanilla BERT\n",
    "    parser.add_argument('--task_phase1', type=str, default='vanilla_BERT')\n",
    "    parser.add_argument('--batch_size_phase1', type=int, default=8, help='for DDP, each GPU processes batch_size_pahse1 samples')\n",
    "    parser.add_argument('--validation_frequency_phase1', type=int, default=10000000)\n",
    "    parser.add_argument('--nEpochs_phase1', type=int, default=2)  # initially, default=100\n",
    "    parser.add_argument('--optim_phase1', default='AdamW')\n",
    "    parser.add_argument('--weight_decay_phase1', type=float, default=1e-2)\n",
    "    parser.add_argument('--lr_policy_phase1', default='SGDR', help='learning rate policy: step|SGDR')\n",
    "    parser.add_argument('--lr_init_phase1', type=float, default=1e-3)\n",
    "    parser.add_argument('--lr_gamma_phase1', type=float, default=0.97)\n",
    "    parser.add_argument('--lr_step_phase1', type=int, default=3000)\n",
    "    parser.add_argument('--lr_warmup_phase1', type=int, default=500)\n",
    "    parser.add_argument('--sequence_length_phase1', type=int ,default=300) # ABCD 348 ABIDE 280 UKB 464\n",
    "    parser.add_argument('--workers_phase1', type=int,default=4)\n",
    "    parser.add_argument('--num_heads_2DBert', type=int, default=12)\n",
    "    \n",
    "    ## phase 2 MBBN\n",
    "    parser.add_argument('--task_phase2', type=str, default='MBBN')\n",
    "    parser.add_argument('--batch_size_phase2', type=int, default=8, help='for DDP, each GPU processes batch_size_pahse1 samples')\n",
    "    parser.add_argument('--nEpochs_phase2', type=int, default=100)  # initially, default=100\n",
    "    parser.add_argument('--optim_phase2', default='AdamW')\n",
    "    parser.add_argument('--weight_decay_phase2', type=float, default=1e-2)\n",
    "    parser.add_argument('--lr_policy_phase2', default='SGDR', help='learning rate policy: step|SGDR')\n",
    "    parser.add_argument('--lr_init_phase2', type=float, default=1e-3)\n",
    "    parser.add_argument('--lr_gamma_phase2', type=float, default=0.97)\n",
    "    parser.add_argument('--lr_step_phase2', type=int, default=3000)\n",
    "    parser.add_argument('--lr_warmup_phase2', type=int, default=500)\n",
    "    parser.add_argument('--sequence_length_phase2', type=int ,default=300) # ABCD 348 ABIDE 280 UKB 464\n",
    "    parser.add_argument('--workers_phase2', type=int, default=4)   # default=4\n",
    "    \n",
    "    ##phase 3 pretraining\n",
    "    parser.add_argument('--task_phase3', type=str, default='MBBN_pretraining')\n",
    "    parser.add_argument('--batch_size_phase3', type=int, default=8, help='for DDP, each GPU processes batch_size_pahse1 samples')\n",
    "    parser.add_argument('--validation_frequency_phase3', type=int, default=10000000)\n",
    "    parser.add_argument('--nEpochs_phase3', type=int, default=1000)\n",
    "    parser.add_argument('--optim_phase3', default='AdamW')\n",
    "    parser.add_argument('--weight_decay_phase3', type=float, default=1e-2)\n",
    "    parser.add_argument('--lr_policy_phase3', default='SGDR', help='learning rate policy: step|SGDR')\n",
    "    parser.add_argument('--lr_init_phase3', type=float, default=1e-3)\n",
    "    parser.add_argument('--lr_gamma_phase3', type=float, default=0.97)\n",
    "    parser.add_argument('--lr_step_phase3', type=int, default=3000)\n",
    "    parser.add_argument('--lr_warmup_phase3', type=int, default=500)\n",
    "    parser.add_argument('--sequence_length_phase3', type=int ,default=300)\n",
    "    parser.add_argument('--workers_phase3', type=int,default=4)\n",
    "    parser.add_argument('--use_recon_loss', action='store_true')\n",
    "    parser.add_argument('--use_mask_loss', action='store_true') \n",
    "    parser.add_argument('--use_cont_loss', action='store_true')\n",
    "    parser.add_argument('--masking_rate', type=float, default=0.1)\n",
    "    parser.add_argument('--masking_method', type=str, default='spatiotemporal', choices=['temporal', 'spatial', 'spatiotemporal'])\n",
    "    parser.add_argument('--temporal_masking_type', type=str, default='time_window', choices=['single_point','time_window'])\n",
    "    parser.add_argument('--temporal_masking_window_size', type=int, default=20)\n",
    "    parser.add_argument('--window_interval_rate', type=int, default=2)\n",
    "    parser.add_argument('--spatial_masking_type', type=str, default='random_ROIs', choices=['hub_ROIs', 'random_ROIs'])\n",
    "    parser.add_argument('--communicability_option', type=str, default='remove_high_comm_node', choices=['remove_high_comm_node', 'remove_low_comm_node'])\n",
    "    parser.add_argument('--num_hub_ROIs', type=int, default=5)\n",
    "    parser.add_argument('--num_random_ROIs', type=int, default=5)\n",
    "    parser.add_argument('--spatiotemporal_masking_type', type=str, default='whole', choices=['whole', 'separate'])\n",
    "    \n",
    "    \n",
    "    ## phase 4 (test)\n",
    "    parser.add_argument('--task_phase4', type=str, default='test')\n",
    "    parser.add_argument('--model_weights_path_phase4', default=None)\n",
    "    parser.add_argument('--batch_size_phase4', type=int, default=4)\n",
    "    parser.add_argument('--nEpochs_phase4', type=int, default=1)\n",
    "    parser.add_argument('--optim_phase4', default='AdamW')\n",
    "    parser.add_argument('--weight_decay_phase4', type=float, default=1e-2)\n",
    "    parser.add_argument('--lr_policy_phase4', default='SGDR', help='learning rate policy: step|SGDR')\n",
    "    parser.add_argument('--lr_init_phase4', type=float, default=1e-4)\n",
    "    parser.add_argument('--lr_gamma_phase4', type=float, default=0.9)\n",
    "    parser.add_argument('--lr_step_phase4', type=int, default=3000)\n",
    "    parser.add_argument('--lr_warmup_phase4', type=int, default=100)\n",
    "    parser.add_argument('--sequence_length_phase4', type=int,default=300) # ABCD 348 ABIDE 280 UKB 464\n",
    "    parser.add_argument('--workers_phase4', type=int, default=4)\n",
    "                        \n",
    "    ## Uncertainty Quantification\n",
    "    ## YC : CHANGED\n",
    "    parser.add_argument('--UQ', action='store_true')\n",
    "    parser.add_argument('--UQ_method', type=str, default='none', choices=['MC_dropout', 'ensemble'])\n",
    "    parser.add_argument('--num_forward_passes', type=int, default=0) # for MC_dropout\n",
    "    parser.add_argument('--num_ensemble_models', type=int, default=0) # for ensemble, should use same number when training and testing\n",
    "    parser.add_argument('--ensemble_models_per_gpu', type=int, default=1)\n",
    "    parser.add_argument('--UQ_model_weights_path', default=None)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "        \n",
    "    return args\n",
    "\n",
    "def setup_folders(base_path): \n",
    "    os.makedirs(os.path.join(base_path,'experiments'),exist_ok=True) \n",
    "    os.makedirs(os.path.join(base_path,'runs'),exist_ok=True)\n",
    "    os.makedirs(os.path.join(base_path, 'splits'), exist_ok=True)\n",
    "    return None\n",
    "\n",
    "\n",
    "def run_phase(args,loaded_model_weights_path,phase_num,phase_name, model_idx = None):\n",
    "    experiment_folder = '{}_{}_{}_{}'.format(args.dataset_name,phase_name,args.target,args.exp_name)\n",
    "    experiment_folder = Path(os.path.join(args.base_path,'experiments',experiment_folder))\n",
    "    os.makedirs(experiment_folder, exist_ok=True)\n",
    "    setattr(args,'loaded_model_weights_path_phase' + phase_num,loaded_model_weights_path)\n",
    "    args.experiment_folder = experiment_folder\n",
    "    args.experiment_title = experiment_folder.name\n",
    "    \n",
    "    print(f'saving the results at {args.experiment_folder}')\n",
    "    \n",
    "    # save hyperparameters\n",
    "    args_logger(args)\n",
    "    \n",
    "    # make args to dict. + detach phase numbers from args\n",
    "    kwargs = sort_args(phase_num, vars(args))\n",
    "    if args.prepare_visualization:\n",
    "        S = ['train','val']\n",
    "    else:\n",
    "        S = ['train','val','test']\n",
    "\n",
    "    trainer = Trainer(sets=S,model_idx=model_idx,**kwargs)\n",
    "    trainer.training()\n",
    "\n",
    "    #S = ['train','val']\n",
    "\n",
    "    if phase_num == '3' and not fine_tune_task == 'regression':\n",
    "        critical_metric = 'accuracy'\n",
    "    else:\n",
    "        critical_metric = 'loss'\n",
    "    model_weights_path = os.path.join(trainer.writer.experiment_folder,trainer.writer.experiment_title + '_BEST_val_{}.pth'.format(critical_metric)) \n",
    "\n",
    "    return model_weights_path\n",
    "\n",
    "\n",
    "## YC : CHANGED\n",
    "def test(args,phase_num,model_weights_path):\n",
    "    UQ = args.UQ\n",
    "    UQ_method = args.UQ_method\n",
    "    print(f\"UQ : {UQ} / UQ_method : {UQ_method}\")\n",
    "    \n",
    "    experiment_folder = '{}_{}_{}'.format(args.dataset_name, 'test_{}'.format(args.fine_tune_task), args.exp_name) #, datestamp())\n",
    "    experiment_folder = Path(os.path.join(args.base_path,'tests', experiment_folder))\n",
    "    os.makedirs(experiment_folder,exist_ok=True)\n",
    "    \n",
    "    args.experiment_folder = experiment_folder\n",
    "    args.experiment_title = experiment_folder.name\n",
    "\n",
    "    if UQ:\n",
    "        S = [UQ_method]\n",
    "        if UQ_method == 'MC_dropout':\n",
    "            # YC : Retrieve the last checkpoint from directory\n",
    "            file_name_and_time_lst = []\n",
    "            for f_name in os.listdir(model_weights_path):\n",
    "                if f_name.endswith('.pth'):\n",
    "                    written_time = os.path.getctime(os.path.join(model_weights_path,f_name))\n",
    "                    file_name_and_time_lst.append((f_name, written_time))\n",
    "            # Backward order of file creation time\n",
    "            sorted_file_lst = sorted(file_name_and_time_lst, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "            if len(sorted_file_lst) == 0:\n",
    "                raise Exception('No model weights found')\n",
    "            loaded_model_weights_path = os.path.join(model_weights_path,sorted_file_lst[0][0])\n",
    "            setattr(args,'loaded_model_weights_path_phase' + phase_num, loaded_model_weights_path)\n",
    "            args_logger(args)\n",
    "            args = sort_args(args.step, vars(args))\n",
    "            trainer = UQTrainer(sets=S,**args)\n",
    "\n",
    "    else:\n",
    "        # YC : Retrieve the most recent checkpoint from directory\n",
    "        file_name_and_time_lst = []\n",
    "        for f_name in os.listdir(model_weights_path):\n",
    "            if f_name.endswith('.pth'):\n",
    "                written_time = os.path.getctime(os.path.join(model_weights_path,f_name))\n",
    "                file_name_and_time_lst.append((f_name, written_time))\n",
    "        # Backward order of file creation time\n",
    "        sorted_file_lst = sorted(file_name_and_time_lst, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        if len(sorted_file_lst) == 0:\n",
    "            raise Exception('No model weights found')\n",
    "        loaded_model_weights_path = os.path.join(model_weights_path,sorted_file_lst[0][0])\n",
    "        setattr(args,'loaded_model_weights_path_phase' + phase_num, loaded_model_weights_path)\n",
    "        S = ['test']\n",
    "        args_logger(args)\n",
    "        args = sort_args(args.step, vars(args))\n",
    "        trainer = Trainer(sets=S,**args)\n",
    "    \n",
    "    trainer.testing()\n",
    "    \n",
    "\n",
    "## YC : CHANGED\n",
    "# if __name__ == '__main__':\n",
    "def main():\n",
    "    base_path = os.getcwd() \n",
    "    setup_folders(base_path) \n",
    "    args = get_arguments(base_path)\n",
    "\n",
    "    # UQ condition check\n",
    "    if args.UQ:\n",
    "        if args.UQ_method == 'none':\n",
    "            raise Exception('UQ method is not specified')\n",
    "        elif args.UQ_method == 'MC_dropout':\n",
    "            if args.num_forward_passes == 0:\n",
    "                raise Exception('num_forward_passes is not specified')\n",
    "            elif args.num_ensemble_models != 0:\n",
    "                raise Exception('num_ensemble_models should not be set for MC_dropout')\n",
    "            if args.step != '4':\n",
    "                raise Exception('MC_dropout is only available for testing')\n",
    "        elif args.UQ_method == 'ensemble':\n",
    "            if args.num_ensemble_models == 0:\n",
    "                raise Exception('num_ensemble_models is not specified')\n",
    "            elif args.num_forward_passes != 0:\n",
    "                raise Exception('num_forward_passes should not be set for ensemble')\n",
    "        \n",
    "        print(f'UQ enabled - method : {args.UQ_method} | step : {args.step}')\n",
    "        if args.UQ_method == 'ensemble':\n",
    "            print(f'num_ensemble_models : {args.num_ensemble_models}')\n",
    "            if args.step == '2':\n",
    "                args.distributed = False\n",
    "                print('distributed set False due to manual distributed setting in ensemble method')\n",
    "        elif args.UQ_method == 'MC_dropout':\n",
    "            print(f'num_forward_passes : {args.num_forward_passes}')\n",
    "\n",
    "    # DDP initialization\n",
    "    if not (args.step == '2' and args.UQ):\n",
    "        init_distributed(args)\n",
    "\n",
    "    # load weights that you specified at the Argument\n",
    "    model_weights_path, step, task = weight_loader(args)\n",
    "\n",
    "    if step == '4' :\n",
    "        print(f'starting testing')\n",
    "        phase_num = '4'\n",
    "        if args.UQ:\n",
    "            model_weights_path = args.UQ_model_weights_path\n",
    "        test(args, phase_num, model_weights_path)\n",
    "    else:\n",
    "        print(f'starting phase{step}: {task}')\n",
    "        if args.UQ and args.UQ_method == 'ensemble':\n",
    "            if args.UQ_model_weights_path is not None:\n",
    "                model_weights_path = args.UQ_model_weights_path\n",
    "                print(f'UQ ensemble model weights loaded from {model_weights_path}')    \n",
    "            mp.set_start_method(\"spawn\", force=True)\n",
    "            run_disributed_phase(args,model_weights_path,step,task)\n",
    "        else:\n",
    "            run_phase(args,model_weights_path,step,task)\n",
    "        print(f'finishing phase{step}: {task}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random seed for torch and np at utils.reproducibility()\n",
    "\n",
    "for dataset, use         self.seed = kwargs.get('seed')  # random seed for reproducibility at dataloaders.py \n",
    "at train_test_split from sklearn. \n",
    "\n",
    "So I can use same seed for dataset split, and use different seed for model training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "\"\"\"\n",
    "def run_phase(args,loaded_model_weights_path,phase_num,phase_name):\n",
    "    experiment_folder = '{}_{}_{}_{}'.format(args.dataset_name,phase_name,args.target,args.exp_name)\n",
    "    experiment_folder = Path(os.path.join(args.base_path,'experiments',experiment_folder))\n",
    "    os.makedirs(experiment_folder, exist_ok=True)\n",
    "    setattr(args,'loaded_model_weights_path_phase' + phase_num,loaded_model_weights_path)\n",
    "    args.experiment_folder = experiment_folder\n",
    "    args.experiment_title = experiment_folder.name\n",
    "    \n",
    "    print(f'saving the results at {args.experiment_folder}')\n",
    "    \n",
    "    # save hyperparameters\n",
    "    args_logger(args)\n",
    "    \n",
    "    # make args to dict. + detach phase numbers from args\n",
    "    kwargs = sort_args(phase_num, vars(args))\n",
    "    if args.prepare_visualization:\n",
    "        S = ['train','val']\n",
    "    else:\n",
    "        S = ['train','val','test']\n",
    "\n",
    "    trainer = Trainer(sets=S,**kwargs)\n",
    "    trainer.training()\n",
    "\n",
    "    #S = ['train','val']\n",
    "\n",
    "    if phase_num == '3' and not fine_tune_task == 'regression':\n",
    "        critical_metric = 'accuracy'\n",
    "    else:\n",
    "        critical_metric = 'loss'\n",
    "    model_weights_path = os.path.join(trainer.writer.experiment_folder,trainer.writer.experiment_title + '_BEST_val_{}.pth'.format(critical_metric)) \n",
    "\n",
    "    return model_weights_path\n",
    "\"\"\"\n",
    "def train_single_model(args, loaded_model_weights_path, phase_num, phase_name, model_idx, device_id):\n",
    "    # Set the current GPU for this process\n",
    "    torch.cuda.set_device(device_id)\n",
    "    # Optionally update args with the device info so Trainer uses the correct device.\n",
    "    args.device = f\"cuda:{device_id}\"\n",
    "    print(f\"Starting training for model {model_idx} on GPU {device_id}\")\n",
    "    # Call the original run_phase which trains the model.\n",
    "    model_path = run_phase(args, loaded_model_weights_path, phase_num, phase_name, model_idx)\n",
    "    print(f\"Completed training for model {model_idx}, saved to {model_path}\")\n",
    "\n",
    "def run_disributed_phase(args,loaded_model_weights_path,phase_num,phase_name):\n",
    "        # torchrun: sbatch script에서 WORLD_SIZE를 지정해준 경우 (노드 당 gpu * 노드의 수)\n",
    "    if \"WORLD_SIZE\" in os.environ: # for torchrun\n",
    "        args.world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "        #print('args.world_size:',args.world_size)\n",
    "    elif 'SLURM_NTASKS' in os.environ: # for slurm scheduler\n",
    "        args.world_size = int(os.environ['SLURM_NTASKS'])\n",
    "    else:\n",
    "        pass # torch.distributed.launch\n",
    "        \n",
    "    args.distributed = args.world_size > 1 # default: world_size = -1 \n",
    "    \n",
    "    num_gpus = torch.cuda.device_count()\n",
    "\n",
    "    ### DEBUG STATEMENT ###\n",
    "    print(f'world_size: {args.world_size}')\n",
    "    print(f'distributed: {args.distributed}')\n",
    "    print(f'num_gpus: {num_gpus}')\n",
    "    #######################\n",
    "    \n",
    "    # Determine how many ensemble models to train concurrently per GPU.\n",
    "    # For instance, if you want two models per GPU at a time, then:\n",
    "    models_per_gpu = args.ensemble_models_per_gpu\n",
    "    concurrent_models = num_gpus * models_per_gpu  # e.g. 4 GPUs * 2 = 8 models concurrently.\n",
    "\n",
    "    # List all ensemble model indices (for example: [0, 1, 2, ..., args.num_ensemble_models-1])\n",
    "    ensemble_indices = list(range(args.num_ensemble_models))\n",
    "\n",
    "    ### DEBUG STATEMENT ###\n",
    "    print(f'models_per_gpu: {models_per_gpu}')\n",
    "    print(f'concurrent_models: {concurrent_models}')\n",
    "    print(f'ensemble_indices: {ensemble_indices}')\n",
    "    #######################\n",
    "\n",
    "    # Iterate over ensemble indices in batches of 'concurrent_models'\n",
    "    for batch_start in range(0, args.num_ensemble_models, concurrent_models):\n",
    "        processes = []\n",
    "        batch_indices = ensemble_indices[batch_start: batch_start + concurrent_models]\n",
    "        print(f\"#Training batch models: {batch_indices}\")\n",
    "        for slot, model_idx in enumerate(batch_indices):\n",
    "            # For assignment, rotate across GPUs. Adjust if you want a different scheduling.\n",
    "            device_id = slot % num_gpus\n",
    "            print(f\"##slot: {slot} / device_id: {device_id} / model_idx: {model_idx}\")\n",
    "            p = mp.Process(\n",
    "                target=train_single_model,\n",
    "                args=(args, loaded_model_weights_path, phase_num, phase_name, model_idx, device_id)\n",
    "            )\n",
    "            p.start()\n",
    "            processes.append(p)\n",
    "        # Wait for this batch of models to finish training.\n",
    "        for p in processes:\n",
    "            p.join()\n",
    "        print(f\"Finished training batch models: {batch_indices}\")\n",
    "\n",
    "    # Optionally, you can collect or post-process the model weight paths here.\n",
    "    print(\"All ensemble models trained.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sys.argv = ['main.py', '--dataset_name', 'ENIGMA_OCD', '--base_path', '/pscratch/sd/y/ycryu/ENIGMA_OCD_MBBN/MBBN-main', '--enigma_path', '/pscratch/sd/y/ycryu/MBBN_data_mini', '--step', '2', '--batch_size_phase2', '8', '--lr_init_phase2', '3e-5', '--lr_policy_phase2', 'step', '--workers_phase2', '8', '--fine_tune_task', 'binary_classification', '--target', 'OCD', '--fmri_type', 'divided_timeseries', '--transformer_hidden_layers', '8', '--divide_by_lorentzian', '--seq_part', 'head', '--use_raw_knee', '--fmri_dividing_type', 'three_channels', '--use_high_freq', '--spatiotemporal', '--spat_diff_loss_type', 'minus_log', '--spatial_loss_factor', '4.0', '--exp_name', 'from_scratch_seed101', '--seed', '101', '--sequence_length_phase2', '100', '--intermediate_vec', '316', '--nEpochs_phase2', '100', '--num_heads', '4', '--UQ', '--UQ_method', 'ensemble', '--num_ensemble_models', '16', '--UQ_model_weights_path', '/scratch/connectome/ycryu/ENIGMA_OCD_MBBN/MBBN-main/experiments/ENIGMA_OCD_mbbn_OCD_from_scratch_seed101']\n",
    "\n",
    "filtered_args = [arg for arg in base_args if arg not in ['--UQ']]\n",
    "['main.py', '--dataset_name', 'ENIGMA_OCD', '--base_path', '/pscratch/sd/y/ycryu/ENIGMA_OCD_MBBN/MBBN-main', '--enigma_path', '/pscratch/sd/y/ycryu/MBBN_data_mini', '--step', '2', '--batch_size_phase2', '8', '--lr_init_phase2', '3e-5', '--lr_policy_phase2', 'step', '--workers_phase2', '8', '--fine_tune_task', 'binary_classification', '--target', 'OCD', '--fmri_type', 'divided_timeseries', '--transformer_hidden_layers', '8', '--divide_by_lorentzian', '--seq_part', 'head', '--use_raw_knee', '--fmri_dividing_type', 'three_channels', '--use_high_freq', '--spatiotemporal', '--spat_diff_loss_type', 'minus_log', '--spatial_loss_factor', '4.0', '--exp_name', 'from_scratch_seed101', '--seed', '101', '--sequence_length_phase2', '100', '--intermediate_vec', '316', '--nEpochs_phase2', '100', '--num_heads', '4', '--UQ_method', 'ensemble', '--num_ensemble_models', '16', '--UQ_model_weights_path', '/scratch/connectome/ycryu/ENIGMA_OCD_MBBN/MBBN-main/experiments/ENIGMA_OCD_mbbn_OCD_from_scratch_seed101']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### dealing with Args\n",
    "\n",
    "    print(args)\n",
    "    args.new_seed = 1010\n",
    "    print(args)\n",
    "    delattr(args,'new_seed')\n",
    "    print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.argv = [\n",
    "    'main.py', '--dataset_name', 'ENIGMA_OCD', '--batch_size_phase2', '8', '--lr_init_phase2', '3e-5', \n",
    "    '--lr_policy_phase2', 'step', '--workers_phase2', '8', '--fine_tune_task', 'binary_classification', '--target', 'OCD', \n",
    "    '--fmri_type', 'divided_timeseries', '--transformer_hidden_layers', '8', '--divide_by_lorentzian', '--seq_part', 'head', \n",
    "    '--use_raw_knee', '--fmri_dividing_type', 'three_channels', '--use_high_freq', '--spatiotemporal', '--spat_diff_loss_type', 'minus_log', \n",
    "    '--spatial_loss_factor', '4.0', '--sequence_length_phase2', '100', \n",
    "    '--intermediate_vec', '316', '--nEpochs_phase2', '100', '--num_heads', '4', \n",
    "    '--base_path', '/pscratch/sd/y/ycryu/ENIGMA_OCD_MBBN/MBBN-main', \n",
    "    '--enigma_path', '/pscratch/sd/y/ycryu/MBBN_data_mini', \n",
    "    '--exp_name', 'ensemble_training_from_scratch_seed101',\n",
    "    '--seed', '101', \n",
    "    '--step', '2', \n",
    "    '--UQ', \n",
    "    '--UQ_method', 'ensemble', \n",
    "    '--num_ensemble_models', '4', \n",
    "    '--ensemble_models_per_gpu', '2',\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# sys.argv = [\n",
    "#     'main.py', '--dataset_name', 'ENIGMA_OCD', '--fine_tune_task', 'binary_classification', '--target', 'OCD',\n",
    "#     '--fmri_type', 'divided_timeseries', '--transformer_hidden_layers', '8', '--divide_by_lorentzian', '--seq_part', 'head',\n",
    "#     '--use_raw_knee', '--fmri_dividing_type', 'three_channels', '--use_high_freq', '--spatiotemporal', '--spat_diff_loss_type', 'minus_log',\n",
    "#     '--spatial_loss_factor', '4.0', '--intermediate_vec', '316', '--num_heads', '4', '--sequence_length_phase4', '100', '--lr_warmup_phase4', '1', '--workers_phase4', '1',\n",
    "#     '--wandb_mode', 'disabled',\n",
    "#     '--base_path', '/pscratch/sd/y/ycryu/ENIGMA_OCD_MBBN/MBBN-main', \n",
    "#     '--enigma_path', '/pscratch/sd/y/ycryu/MBBN_data_mini',\n",
    "#     '--exp_name', 'test_evaluation_seed101', \n",
    "#     '--seed', '101', \n",
    "#     '--step', '4',\n",
    "#     '--UQ', \n",
    "#     '--UQ_method', 'ensemble', \n",
    "#     '--num_ensemble_models', '16', \n",
    "#     '--UQ_model_weights_path', '/pscratch/sd/y/ycryu/ENIGMA_OCD_MBBN/MBBN-main/experiments/ENIGMA_OCD_mbbn_OCD_from_scratch_seed101_1gpu_perlmutter',\n",
    "# ]\n",
    "\n",
    "\n",
    "\n",
    "# sys.argv = [\n",
    "#     'main.py', '--dataset_name', 'ENIGMA_OCD', '--fine_tune_task', 'binary_classification', '--target', 'OCD',\n",
    "#     '--fmri_type', 'divided_timeseries', '--transformer_hidden_layers', '8', '--divide_by_lorentzian', '--seq_part', 'head',\n",
    "#     '--use_raw_knee', '--fmri_dividing_type', 'three_channels', '--use_high_freq', '--spatiotemporal', '--spat_diff_loss_type', 'minus_log',\n",
    "#     '--spatial_loss_factor', '4.0', '--intermediate_vec', '316', '--num_heads', '4', '--sequence_length_phase4', '100', '--lr_warmup_phase4', '1', '--workers_phase4', '1',\n",
    "#     '--wandb_mode', 'disabled',\n",
    "#     '--base_path', '/pscratch/sd/y/ycryu/ENIGMA_OCD_MBBN/MBBN-main', \n",
    "#     '--enigma_path', '/pscratch/sd/y/ycryu/MBBN_data_mini',\n",
    "#     '--exp_name', 'test_evaluation_seed101', \n",
    "#     '--seed', '101', \n",
    "#     '--step', '4',\n",
    "#     '--UQ', \n",
    "#     '--UQ_method', 'MC_dropout', \n",
    "#     '--num_forward_pass', '16', \n",
    "#     '--UQ_model_weights_path', '/pscratch/sd/y/ycryu/ENIGMA_OCD_MBBN/MBBN-main/experiments/ENIGMA_OCD_mbbn_OCD_from_scratch_seed101',\n",
    "# ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UQ enabled - method : ensemble | step : 2\n",
      "num_ensemble_models : 4\n",
      "distributed set False due to manual distributed setting in ensemble method\n",
      "starting phase2: mbbn\n",
      "world_size: -1\n",
      "distributed: False\n",
      "num_gpus: 1\n",
      "models_per_gpu: 2\n",
      "concurrent_models: 2\n",
      "ensemble_indices: [0, 1, 2, 3]\n",
      "#Training batch models: [0, 1]\n",
      "##slot: 0 / device_id: 0 / model_idx: 0\n",
      "##slot: 1 / device_id: 0 / model_idx: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training batch models: [0, 1]\n",
      "#Training batch models: [2, 3]\n",
      "##slot: 0 / device_id: 0 / model_idx: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/global/homes/y/ycryu/.conda/envs/mbbn-env/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/global/homes/y/ycryu/.conda/envs/mbbn-env/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/global/homes/y/ycryu/.conda/envs/mbbn-env/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/global/homes/y/ycryu/.conda/envs/mbbn-env/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'train_single_model' on <module '__main__' (built-in)>\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'train_single_model' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##slot: 1 / device_id: 0 / model_idx: 3\n",
      "Finished training batch models: [2, 3]\n",
      "All ensemble models trained.\n",
      "finishing phase2: mbbn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/global/homes/y/ycryu/.conda/envs/mbbn-env/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/global/homes/y/ycryu/.conda/envs/mbbn-env/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'train_single_model' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/global/homes/y/ycryu/.conda/envs/mbbn-env/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/global/homes/y/ycryu/.conda/envs/mbbn-env/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'train_single_model' on <module '__main__' (built-in)>\n"
     ]
    }
   ],
   "source": [
    "main()\n",
    "## Should set distributed = False when training ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import sys\n",
    "# import subprocess\n",
    "\n",
    "# def launch_ensemble_processes(gpu_ids, base_seed=42):\n",
    "#     # Remove any argument you don't want the children to see, if needed.\n",
    "#     # For example, if you want the child processes to know they're not responsible for launching ensembles,\n",
    "#     # you might remove the `--method ensemble` flag.\n",
    "#     # One simple way is to filter sys.argv (or use parse_known_args) if needed.\n",
    "#     base_args = sys.argv[1:]  # All original command-line arguments except the script name.\n",
    "    \n",
    "#     # Optionally, filter out ensemble-specific arguments if you don't want children to spawn further ensembles.\n",
    "#     # For example:\n",
    "#     filtered_args = [arg for arg in base_args if arg not in ['--UQ']]\n",
    "    \n",
    "#     processes = []\n",
    "#     for idx, gpu in enumerate(gpu_ids):\n",
    "#         seed = base_seed + idx\n",
    "#         # Build the new command: start with 'python main.py' then the filtered arguments\n",
    "#         # then add GPU and seed.\n",
    "#         cmd = [\"python\", \"main.py\"] + filtered_args + [\"--gpu\", str(gpu), \"--seed\", str(seed)]\n",
    "#         print(\"Launching subprocess with command:\", \" \".join(cmd))\n",
    "#         proc = subprocess.Popen(cmd)\n",
    "#         processes.append(proc)\n",
    "    \n",
    "#     # Optionally wait for all processes to finish:\n",
    "#     for proc in processes:\n",
    "#         proc.wait()\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     # Setup folders, etc.\n",
    "#     base_path = os.getcwd()\n",
    "#     setup_folders(base_path)\n",
    "    \n",
    "#     # Parse your arguments\n",
    "#     args = get_arguments(base_path)\n",
    "    \n",
    "#     # Check if ensemble mode is requested:\n",
    "#     if args.method == 'ensemble':\n",
    "#         # Specify the list of GPU ids you want to use\n",
    "#         gpu_ids = [0, 1, 2, 3]  # for example, adjust based on your node\n",
    "#         launch_ensemble_processes(gpu_ids)\n",
    "#         # Optionally, exit the parent process if it is only responsible for spawning children.\n",
    "#         exit()\n",
    "    \n",
    "#     # Continue with the rest of your training/inference code if not in ensemble mode\n",
    "#     ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(mbbn-env) ycryu@login33:/pscratch/sd/y/ycryu/ENIGMA_OCD_MBBN/MBBN-main/yc/ensemble> python main.py --dataset_name ENIGMA_OCD --base_path /pscratch/sd/y/ycryu/ENIGMA_OCD_MBBN/MBBN-main --enigma_path /pscratch/sd/y/ycryu/MBBN_data_mini --step 2 --batch_size_phase2 8 --lr_init_phase2 3e-5 --lr_policy_phase2 step --workers_phase2 8 --fine_tune_task binary_classification --target OCD --fmri_type divided_timeseries --transformer_hidden_layers 8 --divide_by_lorentzian --seq_part head --use_raw_knee --fmri_dividing_type three_channels --use_high_freq --spatiotemporal --spat_diff_loss_type minus_log --spatial_loss_factor 4.0 --exp_name ensemble_training_from_scratch_seed101 --seed 101 --sequence_length_phase2 100 --intermediate_vec 316 --nEpochs_phase2 10 --num_heads 4 --UQ --UQ_method ensemble --num_ensemble_models 4 --ensemble_models_per_gpu 2  2> /pscratch/sd/y/ycryu/ENIGMA_OCD_MBBN/MBBN-main/failed_experiments/enigma_ocd_error_from_scratch_seed101.log\n",
    "UQ enabled - method : ensemble | step : 2\n",
    "num_ensemble_models : 4\n",
    "distributed set False due to manual distributed setting in ensemble method\n",
    "DEBUG : args.distributed : False / args.rank : 0 / args.local_rank : -1 / args.world_size : -1 / args.gpu : 0\n",
    "starting phase2: mbbn\n",
    "world_size: -1\n",
    "distributed: False\n",
    "num_gpus: 1\n",
    "models_per_gpu: 2\n",
    "concurrent_models: 2\n",
    "ensemble_indices: [0, 1, 2, 3]\n",
    "#Training batch models: [0, 1]\n",
    "##slot: 0 / device_id: 0 / model_idx: 0\n",
    "##slot: 1 / device_id: 0 / model_idx: 1\n",
    "Starting training for model 0 on GPU 0\n",
    "saving the results at /pscratch/sd/y/ycryu/ENIGMA_OCD_MBBN/MBBN-main/experiments/ENIGMA_OCD_mbbn_OCD_ensemble_training_from_scratch_seed101/model_0\n",
    "DEBUG : seed for ensemble model 0 is 101\n",
    "Starting training for model 1 on GPU 0\n",
    "saving the results at /pscratch/sd/y/ycryu/ENIGMA_OCD_MBBN/MBBN-main/experiments/ENIGMA_OCD_mbbn_OCD_ensemble_training_from_scratch_seed101/model_1\n",
    "DEBUG : seed for ensemble model 1 is 102\n",
    "generating splits...\n",
    "generating step 1\n",
    "generating splits...\n",
    "Number of subjects used for training: 112\n",
    "generating step 2\n",
    "generating step 1\n",
    "Training set class distribution: {1: 39, 2: 39}\n",
    "Validation set class distribution: {1: 9, 2: 8}\n",
    "Test set class distribution: {1: 9, 2: 8}\n",
    "distribution seed: 101\n",
    "generating step 3.. saving splits...\n",
    "Number of subjects used for training: 112\n",
    "generating step 2\n",
    "Training set class distribution: {1: 39, 2: 39}\n",
    "Validation set class distribution: {1: 9, 2: 8}\n",
    "Test set class distribution: {1: 9, 2: 8}\n",
    "distribution seed: 101\n",
    "generating step 3.. saving splits...\n",
    "Finished training batch models: [0, 1]\n",
    "#Training batch models: [2, 3]\n",
    "##slot: 0 / device_id: 0 / model_idx: 2\n",
    "##slot: 1 / device_id: 0 / model_idx: 3\n",
    "Starting training for model 2 on GPU 0\n",
    "saving the results at /pscratch/sd/y/ycryu/ENIGMA_OCD_MBBN/MBBN-main/experiments/ENIGMA_OCD_mbbn_OCD_ensemble_training_from_scratch_seed101/model_2\n",
    "DEBUG : seed for ensemble model 2 is 103\n",
    "Starting training for model 3 on GPU 0\n",
    "saving the results at /pscratch/sd/y/ycryu/ENIGMA_OCD_MBBN/MBBN-main/experiments/ENIGMA_OCD_mbbn_OCD_ensemble_training_from_scratch_seed101/model_3\n",
    "DEBUG : seed for ensemble model 3 is 104\n",
    "generating splits...\n",
    "generating step 1\n",
    "generating splits...\n",
    "generating step 1\n",
    "Number of subjects used for training: 112\n",
    "generating step 2\n",
    "Training set class distribution: {1: 39, 2: 39}\n",
    "Validation set class distribution: {1: 9, 2: 8}\n",
    "Test set class distribution: {1: 9, 2: 8}\n",
    "distribution seed: 101\n",
    "generating step 3.. saving splits...\n",
    "Number of subjects used for training: 112\n",
    "generating step 2\n",
    "Training set class distribution: {1: 39, 2: 39}\n",
    "Validation set class distribution: {1: 9, 2: 8}\n",
    "Test set class distribution: {1: 9, 2: 8}\n",
    "distribution seed: 101\n",
    "generating step 3.. saving splits...\n",
    "Finished training batch models: [2, 3]\n",
    "All ensemble models trained.\n",
    "finishing phase2: mbbn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
