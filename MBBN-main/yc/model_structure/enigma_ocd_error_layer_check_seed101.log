wandb: Currently logged in as: youngchanryu (youngchanryu-seoul-national-university). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.19.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/connectome/ycryu/ENIGMA_OCD_MBBN/MBBN-main/wandb/run-20250130_074735-h7hs8fwd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ENIGMA_OCD_mbbn_OCD_layer_check_seed101
wandb: ‚≠êÔ∏è View project at https://wandb.ai/youngchanryu-seoul-national-university/enigma-ocd_mbbn
wandb: üöÄ View run at https://wandb.ai/youngchanryu-seoul-national-university/enigma-ocd_mbbn/runs/h7hs8fwd
starting phase2: mbbn
saving the results at /scratch/connectome/ycryu/ENIGMA_OCD_MBBN/MBBN-main/experiments/ENIGMA_OCD_mbbn_OCD_layer_check_seed101
/scratch/connectome/ycryu/ENIGMA_OCD_MBBN/MBBN-main/splits/ENIGMA_OCD/ENIGMA_OCD_OCD_ROI_316_seq_len_92_split101.txt
loading splits
length of train_idx: 1467
length of val_idx: 312
length of test_idx: 314
workers: 8
length of training generator is: 183
workers: 8
length of valid generator is: 39
workers: 8
length of test generator is: 39
Number of training batches: 183
Number of validation batches: 39
Number of test batches: 39
self.task: MBBN
Number of heads: 4
Number of parameters of the model: 19082559
Number of dropout layers counted one by one of the model: 29
Model summary:
Transformer_Finetune_Three_Channels(
  (transformer): Transformer_Block(
    (bert): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(1, 316, padding_idx=0)
        (position_embeddings): Embedding(93, 316)
        (token_type_embeddings): Embedding(2, 316)
        (LayerNorm): LayerNorm((316,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.3, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0-7): 8 x BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=316, out_features=316, bias=True)
                (key): Linear(in_features=316, out_features=316, bias=True)
                (value): Linear(in_features=316, out_features=316, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=316, out_features=316, bias=True)
                (LayerNorm): LayerNorm((316,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.3, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=316, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=316, bias=True)
              (LayerNorm): LayerNorm((316,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.3, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=316, out_features=316, bias=True)
        (activation): Tanh()
      )
    )
    (cls_embedding): Sequential(
      (0): Linear(in_features=316, out_features=316, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
    )
  )
  (high_spatial_attention): Attention(
    (qkv): Linear(in_features=92, out_features=276, bias=False)
    (attn_drop): Dropout(p=0.0, inplace=False)
  )
  (low_spatial_attention): Attention(
    (qkv): Linear(in_features=92, out_features=276, bias=False)
    (attn_drop): Dropout(p=0.0, inplace=False)
  )
  (ultralow_spatial_attention): Attention(
    (qkv): Linear(in_features=92, out_features=276, bias=False)
    (attn_drop): Dropout(p=0.0, inplace=False)
  )
  (regression_head): Classifier(
    (linear): Linear(in_features=316, out_features=1, bias=True)
    (dropout): Dropout(p=0.6, inplace=False)
    (norm): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
)
Model parameters info:
Model parameters:
transformer.bert.embeddings.word_embeddings.weight
Parameter containing:
tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0.]], requires_grad=True)
Model parameters:
transformer.bert.embeddings.position_embeddings.weight
Parameter containing:
tensor([[ 0.0089, -0.0220,  0.0091,  ..., -0.0139,  0.0016,  0.0156],
        [ 0.0168, -0.0056,  0.0056,  ..., -0.0186,  0.0064, -0.0109],
        [ 0.0106, -0.0096, -0.0043,  ..., -0.0012, -0.0004, -0.0069],
        ...,
        [-0.0067,  0.0089, -0.0513,  ...,  0.0018,  0.0421, -0.0056],
        [ 0.0068, -0.0291,  0.0081,  ...,  0.0217, -0.0461, -0.0062],
        [-0.0215,  0.0109,  0.0251,  ...,  0.0154,  0.0009,  0.0069]],
       requires_grad=True)
Model parameters:
transformer.bert.embeddings.token_type_embeddings.weight
Parameter containing:
tensor([[-4.5099e-03, -1.7350e-03,  1.1072e-02,  2.7505e-02, -1.1873e-02,
          3.6087e-02, -5.2983e-03,  6.2244e-03,  1.0893e-02, -4.2664e-03,
          5.7429e-03, -1.3705e-02, -5.1842e-03, -9.3125e-03,  1.8813e-02,
         -1.7736e-02, -3.0907e-02,  1.9108e-02, -2.5762e-03, -1.3753e-03,
         -5.5304e-02,  2.4474e-02,  2.8487e-03, -1.9188e-02,  1.0604e-02,
         -6.8131e-03,  8.1959e-03, -3.0509e-03,  7.5708e-03,  3.1113e-03,
         -7.2525e-03,  6.9762e-03, -7.8225e-03,  4.9107e-04,  1.1784e-02,
         -1.4843e-03, -2.1972e-02, -2.5265e-03,  1.2929e-04,  1.8287e-02,
         -3.0598e-02,  2.6176e-04,  1.4980e-02, -2.9332e-04, -1.5027e-02,
         -3.2833e-02, -1.9976e-03, -9.8321e-03, -1.8831e-04, -2.3249e-02,
         -7.4821e-03, -1.9242e-02,  3.9932e-03, -2.4747e-03, -1.4374e-02,
          2.6953e-02, -3.6131e-03,  2.5853e-03, -8.0671e-03, -1.7560e-02,
          3.1316e-03,  6.3522e-03, -1.5340e-03,  3.3352e-02,  1.1844e-02,
          7.3550e-03, -8.0918e-03, -6.4623e-03,  7.0163e-03,  3.6219e-03,
         -2.4712e-02,  4.5056e-03,  2.5078e-02,  2.0980e-02, -2.0568e-03,
          3.2550e-02,  6.9507e-03, -4.8565e-02, -3.1841e-03,  8.8290e-04,
         -1.8715e-02, -2.4580e-02,  7.6472e-03, -1.2149e-02, -5.9624e-03,
         -1.5892e-02, -3.4483e-02, -1.8079e-02,  2.1887e-02,  1.4323e-02,
         -2.5151e-02, -2.7554e-02, -1.6024e-02,  1.2631e-02, -1.8330e-02,
         -9.4563e-04,  5.2180e-03,  1.9432e-02, -2.7543e-02,  9.2595e-03,
         -2.0713e-03,  5.4180e-03,  2.4945e-03,  3.6536e-04, -1.0230e-02,
         -6.7866e-03,  1.5746e-02,  2.8226e-02,  3.0499e-02, -1.3136e-03,
          1.5540e-02,  5.7068e-03,  5.8629e-03, -3.0875e-02,  9.7711e-03,
         -8.5005e-03,  1.2096e-02, -2.3149e-02,  4.6327e-03, -1.1834e-02,
          8.0118e-03,  6.4021e-03, -9.4609e-03,  2.9629e-03, -1.0601e-02,
         -1.3851e-02,  3.6106e-03,  1.0931e-02, -2.4075e-02,  4.3855e-03,
          2.3782e-02,  1.0359e-02, -1.5265e-02, -2.7396e-02,  2.3751e-02,
         -9.7420e-03,  1.4277e-04,  4.8733e-02, -5.1253e-03,  1.7460e-02,
          2.4425e-03,  2.6748e-02, -3.4674e-03, -1.8130e-02,  7.6357e-03,
         -2.3889e-03, -4.3963e-03, -1.1180e-02, -1.3133e-02,  7.5127e-03,
          1.1957e-02,  1.6645e-02,  1.3536e-02,  1.8604e-02,  1.4040e-02,
         -9.7940e-03,  1.1287e-02, -2.0865e-02, -1.5443e-02, -3.0715e-02,
         -8.0575e-03, -9.2234e-03, -1.9703e-03,  7.0808e-03,  1.1497e-02,
         -7.6545e-04,  2.7942e-02,  3.1704e-02,  1.3920e-02, -1.9808e-03,
         -2.7483e-02, -4.9136e-02, -2.0938e-02,  1.3150e-02,  2.1007e-02,
          8.1302e-04,  3.3334e-03, -5.1181e-03,  6.5606e-03,  1.2016e-02,
         -3.5753e-02, -4.9735e-02,  5.6593e-03,  8.8011e-03,  1.9299e-02,
         -2.8696e-02,  3.9484e-03,  1.4138e-02,  3.0232e-02,  7.3791e-03,
          2.0973e-02, -2.7236e-02, -1.3179e-02, -1.6794e-02, -4.9605e-02,
         -2.0782e-03, -5.7573e-02, -4.9305e-03, -5.2005e-03,  9.1688e-03,
          6.7168e-03, -2.5177e-02, -1.3705e-02, -3.5041e-02, -4.2770e-02,
          1.1680e-02, -2.0146e-02,  7.7599e-03, -1.0939e-03, -3.7052e-02,
          4.6452e-03,  2.4532e-02,  1.2252e-02, -6.3206e-03, -3.1084e-02,
         -1.8114e-02, -2.3629e-04, -1.9524e-03,  3.6442e-03, -4.1427e-03,
         -4.8883e-03, -9.3289e-04,  2.0140e-02,  3.1273e-02,  3.4317e-02,
         -1.0549e-02,  1.0745e-02, -2.6359e-03, -4.0143e-02,  2.7654e-02,
         -5.6818e-03,  4.4606e-03, -5.4939e-03, -1.8818e-02, -3.2801e-04,
          2.0112e-02, -3.1262e-02,  1.1373e-02,  4.3990e-02, -1.3981e-02,
          8.2236e-03, -2.5024e-02, -2.5629e-02,  1.3273e-02, -1.3679e-02,
         -2.1831e-02,  1.7441e-02, -1.5336e-02, -3.6061e-02,  2.6192e-02,
          1.9147e-02,  4.0796e-02,  2.3208e-02, -1.0671e-02, -3.8713e-02,
          3.3502e-02, -2.3056e-02, -3.9118e-02,  1.6380e-02, -4.4994e-02,
          2.0538e-03, -1.2089e-02,  4.4410e-03, -1.8508e-02, -4.1531e-03,
          2.1905e-02, -4.8286e-02, -8.4597e-03, -1.3667e-02,  6.1885e-03,
          2.5520e-03, -7.5954e-03, -5.6111e-02,  1.3135e-02, -5.4105e-02,
         -1.9990e-02, -4.3324e-04, -1.5568e-02,  2.6221e-03,  1.4578e-02,
         -4.8847e-02,  5.1711e-03, -9.1679e-03, -3.0276e-03, -2.0021e-02,
          1.5054e-02, -2.1138e-02,  2.8545e-02, -1.2163e-02,  1.1545e-02,
          4.6005e-03, -2.7444e-02,  1.7341e-02, -1.4919e-02,  1.7327e-02,
         -3.7164e-02, -9.3722e-03, -1.3107e-02, -1.0543e-02,  1.1678e-02,
          3.9875e-03, -3.8474e-02,  1.7344e-02, -5.0141e-03, -2.7508e-02,
          3.1591e-02,  1.9562e-02,  1.4454e-02,  9.0500e-03, -1.4923e-03,
         -5.3068e-03,  1.4693e-02,  1.2088e-02,  2.0413e-02,  4.2010e-02,
         -1.3828e-04],
        [ 5.6515e-03,  7.0374e-04,  2.0121e-02,  5.6892e-02, -2.7366e-02,
         -1.4013e-02, -2.0350e-02,  3.0347e-02,  8.6532e-03, -5.0440e-02,
         -2.9353e-02,  1.3666e-02, -2.5263e-02, -1.8834e-02,  4.4988e-03,
          3.2028e-02, -1.6532e-02,  5.9992e-03, -3.2896e-02,  1.8349e-02,
          1.5769e-05,  2.3966e-02,  5.1986e-03, -3.0173e-02, -1.5934e-03,
         -6.0171e-03,  1.3914e-02,  4.7614e-03, -1.3454e-02, -2.9845e-02,
         -1.7348e-02, -7.8089e-03, -7.6598e-03, -1.4551e-02,  9.6189e-03,
         -5.0239e-03, -2.2462e-03,  3.5514e-03,  9.9126e-03, -1.6819e-02,
          1.2922e-02,  3.5576e-02,  3.1071e-02,  1.3282e-02, -3.2146e-02,
         -2.2862e-02, -9.4217e-03, -1.0834e-02, -2.3900e-02, -2.9560e-03,
         -2.0999e-03,  2.0297e-04, -9.7557e-03,  3.1699e-04,  9.4315e-03,
         -1.6808e-02, -1.8748e-02,  1.2437e-03,  7.5990e-03, -1.0378e-02,
          8.3520e-03,  2.4491e-02, -5.6983e-02,  1.1597e-02, -1.8187e-02,
          5.4003e-03, -3.4048e-02, -3.6449e-02,  3.2509e-02,  2.6651e-02,
          9.9893e-03, -1.1584e-02,  1.2994e-02, -3.5542e-03, -3.3826e-02,
         -2.0177e-02, -1.6317e-02,  1.3428e-02, -3.0069e-02, -2.6710e-02,
         -3.7830e-03, -1.8405e-02,  2.1954e-02,  4.4155e-04, -7.0392e-03,
          7.1012e-03,  3.2063e-02,  8.6553e-03,  1.6221e-02,  1.5099e-02,
         -1.0175e-02,  2.6994e-02, -6.3817e-04, -2.6812e-02, -2.9158e-02,
          2.6253e-02, -5.0793e-02,  1.8048e-02, -2.5163e-02, -5.4497e-02,
         -5.5489e-03,  1.2627e-02,  1.1190e-02,  6.3199e-03, -1.4582e-02,
         -6.1542e-03,  3.5855e-02,  1.3340e-02, -1.5581e-03,  1.4431e-02,
          2.5141e-02, -2.1365e-02,  1.6258e-02,  2.3872e-02,  8.4958e-04,
          3.9522e-02,  9.9672e-03, -1.1338e-02,  7.7253e-03,  4.0911e-03,
         -4.0741e-04, -2.2980e-02, -1.2773e-02, -3.9377e-02, -6.2457e-03,
         -1.1334e-02, -9.0613e-03,  2.2775e-02, -3.4756e-02, -1.9679e-02,
          3.6793e-03,  2.8539e-02, -1.1193e-02, -1.0706e-02,  7.5423e-03,
          1.2746e-02,  8.2837e-04,  2.3147e-02,  3.4423e-02, -1.7283e-02,
         -1.0736e-02, -8.1480e-03, -2.1414e-02,  1.3991e-02, -3.3366e-02,
          1.6993e-02,  5.2033e-03,  3.5040e-02,  8.1171e-03, -1.1310e-02,
         -1.8638e-03, -6.4816e-03, -4.2381e-02, -1.5818e-02, -1.3440e-02,
         -4.7976e-03, -2.8552e-02,  4.3305e-02, -1.4014e-02, -5.5155e-03,
         -1.7094e-02,  5.9192e-03,  3.9065e-02,  1.2743e-02,  1.5933e-02,
         -1.5495e-02,  3.8010e-02, -2.2073e-02, -1.1175e-02,  2.0509e-02,
          2.4150e-02, -2.2238e-04,  3.2850e-02, -4.5130e-02,  2.4469e-03,
          1.2527e-03,  7.0960e-03, -1.7861e-03, -1.7583e-03,  3.8244e-02,
         -1.3928e-02, -9.6725e-03, -2.0341e-02, -4.6525e-03,  1.0184e-02,
          6.9434e-03,  1.8103e-02,  1.9596e-03, -1.8796e-03, -2.4752e-02,
          3.1013e-02,  2.1414e-02,  9.9111e-03, -2.8187e-02,  4.0548e-03,
         -2.4917e-02,  1.8773e-02, -9.4581e-04, -1.4108e-02, -2.3493e-02,
          8.9179e-03,  2.7315e-02,  2.3015e-02, -1.5916e-02,  2.8778e-02,
          7.0542e-03,  1.7033e-02,  1.0770e-02,  7.6044e-03,  7.8624e-03,
         -2.0462e-02, -1.5050e-02, -2.0424e-02,  7.5213e-03,  1.6806e-02,
          1.4187e-02, -2.0213e-02, -3.0056e-02,  1.1040e-02,  5.6994e-03,
          1.0109e-02, -7.0729e-03,  1.6578e-02,  4.2210e-02,  1.8396e-02,
         -2.2737e-02,  1.5762e-02, -7.8008e-03, -1.1580e-02,  4.5248e-02,
          9.3237e-04,  2.1066e-02, -1.4979e-02, -2.8888e-02, -2.5884e-02,
         -4.8810e-03, -1.5797e-02,  1.2242e-02,  3.3090e-02,  7.7176e-03,
          3.2986e-02,  7.9577e-03,  1.0320e-02,  3.5897e-04,  2.7125e-02,
         -4.9872e-02,  4.0632e-03, -4.7271e-03,  2.3765e-02,  2.4297e-02,
          1.1862e-02,  3.0327e-02, -1.1473e-03,  1.1017e-02,  1.5033e-02,
         -2.2644e-03, -3.0164e-03,  3.4586e-02, -4.1372e-03,  2.5405e-02,
         -1.7896e-03,  2.9603e-02, -7.6299e-03, -2.1136e-02,  2.8205e-02,
          1.4805e-02,  1.3159e-02,  1.8625e-03, -6.6989e-03, -1.2055e-02,
         -2.7566e-02, -6.6239e-03, -1.2949e-02, -1.3757e-02, -5.5379e-03,
         -1.8738e-02,  3.0886e-04,  1.1762e-02,  2.1700e-02,  2.4867e-03,
          4.2776e-02,  3.2414e-03, -3.5707e-03,  1.1368e-02, -7.5533e-03,
         -1.3668e-02, -2.6656e-02,  2.3914e-02,  9.8511e-03, -1.0341e-02,
          3.0864e-04, -1.0681e-02,  1.1352e-02,  2.6001e-02,  1.7046e-02,
         -3.2013e-02, -7.8305e-03, -3.2176e-02,  4.9278e-03,  1.6060e-02,
          1.0688e-02,  4.7758e-02, -6.5384e-03, -1.6177e-02,  6.2628e-03,
          9.4363e-04,  3.8568e-03, -3.1593e-02, -1.9245e-02, -2.7264e-02,
         -4.2712e-02,  5.8977e-03, -1.9478e-02,  2.3383e-02,  1.4450e-02,
         -1.1065e-02]], requires_grad=True)
Model parameters:
transformer.bert.embeddings.LayerNorm.weight
Parameter containing:
tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)
Model parameters:
transformer.bert.embeddings.LayerNorm.bias
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.0.attention.self.query.weight
Parameter containing:
tensor([[ 0.0223,  0.0323, -0.0301,  ...,  0.0217, -0.0087, -0.0004],
        [-0.0076,  0.0599, -0.0215,  ...,  0.0011, -0.0023, -0.0065],
        [ 0.0212,  0.0246, -0.0012,  ...,  0.0174,  0.0161, -0.0077],
        ...,
        [ 0.0029, -0.0090, -0.0169,  ...,  0.0013, -0.0017, -0.0399],
        [-0.0031, -0.0224,  0.0098,  ..., -0.0013,  0.0267, -0.0098],
        [ 0.0295, -0.0165,  0.0210,  ..., -0.0069,  0.0192,  0.0168]],
       requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.0.attention.self.query.bias
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.0.attention.self.key.weight
Parameter containing:
tensor([[-0.0293, -0.0071,  0.0065,  ..., -0.0081,  0.0221,  0.0070],
        [ 0.0248,  0.0011,  0.0250,  ...,  0.0076, -0.0020,  0.0080],
        [-0.0055,  0.0267,  0.0229,  ...,  0.0054, -0.0144,  0.0003],
        ...,
        [ 0.0055,  0.0050,  0.0114,  ..., -0.0318,  0.0018,  0.0087],
        [-0.0220,  0.0120, -0.0019,  ..., -0.0148,  0.0055, -0.0183],
        [-0.0100,  0.0037, -0.0001,  ..., -0.0171, -0.0152, -0.0055]],
       requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.0.attention.self.key.bias
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.0.attention.self.value.weight
Parameter containing:
tensor([[ 0.0290,  0.0225,  0.0284,  ...,  0.0031,  0.0262,  0.0064],
        [ 0.0012,  0.0031, -0.0287,  ...,  0.0071, -0.0478, -0.0106],
        [-0.0223,  0.0194,  0.0044,  ..., -0.0203, -0.0012, -0.0229],
        ...,
        [ 0.0311,  0.0325,  0.0321,  ...,  0.0281, -0.0207, -0.0137],
        [-0.0244, -0.0350, -0.0184,  ...,  0.0099,  0.0009, -0.0281],
        [ 0.0133,  0.0106, -0.0144,  ...,  0.0128,  0.0138,  0.0065]],
       requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.0.attention.self.value.bias
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.0.attention.output.dense.weight
Parameter containing:
tensor([[-0.0025,  0.0038,  0.0016,  ..., -0.0190,  0.0165, -0.0041],
        [ 0.0130, -0.0031,  0.0252,  ..., -0.0050, -0.0161, -0.0173],
        [-0.0099,  0.0115,  0.0207,  ...,  0.0175,  0.0524, -0.0081],
        ...,
        [-0.0207, -0.0113,  0.0338,  ...,  0.0201,  0.0233, -0.0123],
        [ 0.0470, -0.0094, -0.0106,  ...,  0.0123,  0.0257,  0.0346],
        [ 0.0019, -0.0194,  0.0185,  ..., -0.0005,  0.0013,  0.0052]],
       requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.0.attention.output.dense.bias
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.0.attention.output.LayerNorm.weight
Parameter containing:
tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.0.attention.output.LayerNorm.bias
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.0.intermediate.dense.weight
Parameter containing:
tensor([[ 1.9853e-02,  3.0361e-03, -1.4978e-02,  ..., -5.0515e-03,
          3.6380e-02,  2.6738e-02],
        [-9.1690e-03,  2.6236e-02,  1.1670e-03,  ..., -1.8201e-02,
          1.3608e-02,  6.1861e-03],
        [-3.7524e-03, -3.9127e-02, -8.0967e-03,  ...,  3.8852e-05,
         -7.0032e-03,  5.9576e-03],
        ...,
        [ 9.3508e-03, -1.2043e-02,  6.4300e-03,  ...,  2.9475e-02,
          4.1614e-03,  3.8958e-02],
        [ 3.4667e-02, -1.1099e-03, -1.7965e-02,  ...,  3.2097e-02,
         -6.9227e-03, -1.1979e-03],
        [ 1.9147e-02,  2.7226e-02, -2.4653e-02,  ..., -1.3463e-02,
         -3.5072e-02,  9.6269e-03]], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.0.intermediate.dense.bias
Parameter containing:
tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.0.output.dense.weight
Parameter containing:
tensor([[ 0.0203, -0.0015, -0.0079,  ..., -0.0006,  0.0014, -0.0079],
        [ 0.0218, -0.0113, -0.0214,  ...,  0.0456, -0.0276,  0.0160],
        [ 0.0167,  0.0017, -0.0181,  ...,  0.0238,  0.0097, -0.0168],
        ...,
        [-0.0044,  0.0081, -0.0075,  ..., -0.0235, -0.0041, -0.0036],
        [-0.0496, -0.0554, -0.0272,  ..., -0.0151, -0.0031, -0.0120],
        [ 0.0533,  0.0228,  0.0130,  ..., -0.0032, -0.0013,  0.0094]],
       requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.0.output.dense.bias
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.0.output.LayerNorm.weight
Parameter containing:
tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.0.output.LayerNorm.bias
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.1.attention.self.query.weight
Parameter containing:
tensor([[ 0.0224, -0.0069, -0.0016,  ...,  0.0171, -0.0220,  0.0051],
        [ 0.0203, -0.0138,  0.0347,  ...,  0.0079, -0.0058, -0.0042],
        [-0.0133,  0.0199,  0.0076,  ..., -0.0150,  0.0134,  0.0050],
        ...,
        [-0.0153,  0.0105,  0.0169,  ..., -0.0419,  0.0069,  0.0073],
        [ 0.0063,  0.0041, -0.0082,  ..., -0.0094, -0.0028,  0.0190],
        [ 0.0152, -0.0025,  0.0020,  ...,  0.0156, -0.0173, -0.0453]],
       requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.1.attention.self.query.bias
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.1.attention.self.key.weight
Parameter containing:
tensor([[ 0.0016, -0.0191, -0.0016,  ..., -0.0105,  0.0219,  0.0317],
        [-0.0050,  0.0171, -0.0246,  ...,  0.0151,  0.0023,  0.0046],
        [ 0.0537,  0.0539, -0.0245,  ..., -0.0257,  0.0217,  0.0153],
        ...,
        [-0.0035,  0.0049, -0.0066,  ...,  0.0132,  0.0255,  0.0154],
        [-0.0031,  0.0165,  0.0239,  ...,  0.0101,  0.0436,  0.0256],
        [-0.0171,  0.0058,  0.0024,  ..., -0.0488,  0.0283,  0.0273]],
       requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.1.attention.self.key.bias
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.1.attention.self.value.weight
Parameter containing:
tensor([[-0.0226,  0.0159,  0.0143,  ..., -0.0098, -0.0104, -0.0193],
        [ 0.0334,  0.0106, -0.0308,  ...,  0.0252,  0.0113,  0.0187],
        [ 0.0068,  0.0007, -0.0106,  ...,  0.0083, -0.0047,  0.0194],
        ...,
        [-0.0062,  0.0043,  0.0039,  ..., -0.0142, -0.0075, -0.0068],
        [ 0.0140, -0.0555,  0.0332,  ...,  0.0109, -0.0170, -0.0041],
        [-0.0201, -0.0441,  0.0499,  ..., -0.0239,  0.0023, -0.0191]],
       requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.1.attention.self.value.bias
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.1.attention.output.dense.weight
Parameter containing:
tensor([[-0.0004, -0.0094,  0.0103,  ...,  0.0552, -0.0109, -0.0422],
        [ 0.0008,  0.0353, -0.0347,  ...,  0.0028, -0.0105,  0.0170],
        [-0.0231, -0.0358, -0.0125,  ...,  0.0018, -0.0235, -0.0239],
        ...,
        [ 0.0163, -0.0127,  0.0019,  ...,  0.0251, -0.0227,  0.0010],
        [ 0.0300, -0.0035,  0.0386,  ...,  0.0199,  0.0079, -0.0021],
        [-0.0238,  0.0274, -0.0185,  ...,  0.0166,  0.0056,  0.0048]],
       requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.1.attention.output.dense.bias
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.1.attention.output.LayerNorm.weight
Parameter containing:
tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.1.attention.output.LayerNorm.bias
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.1.intermediate.dense.weight
Parameter containing:
tensor([[ 0.0021,  0.0224,  0.0100,  ...,  0.0255, -0.0173,  0.0175],
        [-0.0228,  0.0236, -0.0238,  ...,  0.0012, -0.0049, -0.0114],
        [ 0.0287,  0.0028, -0.0122,  ...,  0.0153,  0.0190,  0.0024],
        ...,
        [ 0.0258, -0.0129,  0.0182,  ...,  0.0014,  0.0423,  0.0029],
        [-0.0257,  0.0065,  0.0286,  ..., -0.0414,  0.0425,  0.0025],
        [ 0.0094,  0.0188,  0.0079,  ..., -0.0081,  0.0302, -0.0229]],
       requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.1.intermediate.dense.bias
Parameter containing:
tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.1.output.dense.weight
Parameter containing:
tensor([[-0.0020, -0.0121,  0.0132,  ...,  0.0494,  0.0017, -0.0216],
        [ 0.0002, -0.0234, -0.0157,  ..., -0.0179, -0.0242, -0.0357],
        [ 0.0041,  0.0215,  0.0321,  ..., -0.0165, -0.0037, -0.0015],
        ...,
        [-0.0110,  0.0361,  0.0123,  ..., -0.0192,  0.0191,  0.0012],
        [-0.0206,  0.0104,  0.0145,  ...,  0.0289, -0.0037, -0.0312],
        [ 0.0045, -0.0149,  0.0314,  ...,  0.0043, -0.0099,  0.0055]],
       requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.1.output.dense.bias
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.1.output.LayerNorm.weight
Parameter containing:
tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.1.output.LayerNorm.bias
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.2.attention.self.query.weight
Parameter containing:
tensor([[-0.0137,  0.0086, -0.0026,  ...,  0.0177, -0.0476, -0.0169],
        [-0.0314, -0.0049, -0.0068,  ..., -0.0312,  0.0137,  0.0056],
        [-0.0057, -0.0074,  0.0085,  ...,  0.0019,  0.0126,  0.0003],
        ...,
        [-0.0384, -0.0174,  0.0079,  ...,  0.0105, -0.0019, -0.0324],
        [-0.0012,  0.0277,  0.0149,  ..., -0.0144, -0.0094, -0.0053],
        [-0.0135, -0.0001, -0.0240,  ..., -0.0216, -0.0012, -0.0270]],
       requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.2.attention.self.query.bias
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.2.attention.self.key.weight
Parameter containing:
tensor([[ 3.6793e-03, -1.8933e-03, -2.0601e-02,  ...,  3.2366e-02,
         -1.2469e-05, -9.2692e-03],
        [ 1.0071e-02,  3.9168e-02,  9.2864e-03,  ..., -1.1657e-02,
          1.2091e-02, -2.1224e-02],
        [-3.5579e-02,  3.3689e-02,  4.3894e-02,  ..., -1.4294e-02,
          4.3189e-03,  1.6389e-02],
        ...,
        [ 1.3745e-02, -1.9069e-02, -1.2759e-02,  ..., -4.4130e-03,
         -1.0042e-02, -2.6257e-02],
        [ 2.2460e-02,  1.1127e-02, -3.4604e-02,  ..., -1.0216e-02,
         -1.1865e-02,  3.0191e-02],
        [ 1.4562e-02,  2.1203e-02,  1.1093e-02,  ...,  6.4836e-03,
         -6.6327e-03,  2.9300e-02]], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.2.attention.self.key.bias
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.2.attention.self.value.weight
Parameter containing:
tensor([[-0.0035, -0.0173, -0.0038,  ..., -0.0335,  0.0012,  0.0185],
        [-0.0266,  0.0180, -0.0087,  ..., -0.0176,  0.0156,  0.0060],
        [ 0.0227, -0.0008,  0.0126,  ...,  0.0080, -0.0113, -0.0263],
        ...,
        [ 0.0210,  0.0295, -0.0088,  ..., -0.0072,  0.0219, -0.0246],
        [-0.0394,  0.0069,  0.0313,  ..., -0.0307, -0.0082, -0.0180],
        [ 0.0112, -0.0092,  0.0066,  ...,  0.0005, -0.0203, -0.0315]],
       requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.2.attention.self.value.bias
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.2.attention.output.dense.weight
Parameter containing:
tensor([[ 0.0060,  0.0091, -0.0332,  ..., -0.0055, -0.0023, -0.0047],
        [-0.0173, -0.0114, -0.0336,  ..., -0.0003, -0.0102,  0.0293],
        [ 0.0049,  0.0197, -0.0150,  ..., -0.0017, -0.0328,  0.0302],
        ...,
        [-0.0067, -0.0055,  0.0098,  ..., -0.0157,  0.0132, -0.0240],
        [ 0.0149,  0.0082, -0.0330,  ...,  0.0006,  0.0186, -0.0031],
        [ 0.0209, -0.0084, -0.0436,  ...,  0.0043, -0.0237,  0.0148]],
       requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.2.attention.output.dense.bias
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.2.attention.output.LayerNorm.weight
Parameter containing:
tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.2.attention.output.LayerNorm.bias
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.2.intermediate.dense.weight
Parameter containing:
tensor([[-0.0123, -0.0203,  0.0172,  ..., -0.0152, -0.0256, -0.0014],
        [ 0.0033, -0.0181, -0.0160,  ...,  0.0364, -0.0027,  0.0229],
        [-0.0001,  0.0096, -0.0478,  ..., -0.0008, -0.0201,  0.0383],
        ...,
        [ 0.0298, -0.0151, -0.0276,  ..., -0.0138, -0.0444, -0.0207],
        [ 0.0122,  0.0027,  0.0313,  ..., -0.0150, -0.0063,  0.0053],
        [ 0.0212,  0.0221,  0.0046,  ...,  0.0096, -0.0182, -0.0034]],
       requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.2.intermediate.dense.bias
Parameter containing:
tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.2.output.dense.weight
Parameter containing:
tensor([[-0.0001, -0.0286,  0.0067,  ..., -0.0006,  0.0264,  0.0239],
        [-0.0010, -0.0120,  0.0220,  ..., -0.0088,  0.0068, -0.0120],
        [-0.0055, -0.0168, -0.0038,  ..., -0.0212,  0.0074,  0.0204],
        ...,
        [ 0.0143, -0.0313,  0.0118,  ...,  0.0266, -0.0381, -0.0038],
        [-0.0007,  0.0283, -0.0028,  ..., -0.0179,  0.0223,  0.0117],
        [ 0.0186, -0.0016,  0.0196,  ...,  0.0161,  0.0108, -0.0263]],
       requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.2.output.dense.bias
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.2.output.LayerNorm.weight
Parameter containing:
tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.2.output.LayerNorm.bias
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.3.attention.self.query.weight
Parameter containing:
tensor([[ 0.0155, -0.0002,  0.0129,  ..., -0.0069,  0.0011, -0.0187],
        [ 0.0110,  0.0082,  0.0052,  ...,  0.0231, -0.0020, -0.0097],
        [-0.0158,  0.0087, -0.0048,  ...,  0.0259,  0.0182,  0.0086],
        ...,
        [ 0.0068, -0.0153,  0.0330,  ..., -0.0069, -0.0341, -0.0285],
        [ 0.0198,  0.0099,  0.0307,  ...,  0.0202, -0.0331,  0.0220],
        [ 0.0051,  0.0151, -0.0320,  ..., -0.0171, -0.0190, -0.0073]],
       requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.3.attention.self.query.bias
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.3.attention.self.key.weight
Parameter containing:
tensor([[ 0.0183,  0.0346, -0.0459,  ...,  0.0080,  0.0047,  0.0375],
        [ 0.0004,  0.0073,  0.0036,  ...,  0.0290, -0.0135, -0.0048],
        [-0.0113,  0.0010,  0.0133,  ...,  0.0165,  0.0312, -0.0219],
        ...,
        [ 0.0118, -0.0056, -0.0471,  ..., -0.0034, -0.0161,  0.0148],
        [-0.0369,  0.0002,  0.0069,  ...,  0.0158,  0.0062, -0.0070],
        [ 0.0120,  0.0165, -0.0341,  ...,  0.0337, -0.0117, -0.0004]],
       requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.3.attention.self.key.bias
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.3.attention.self.value.weight
Parameter containing:
tensor([[ 0.0055, -0.0040, -0.0220,  ...,  0.0111,  0.0273, -0.0012],
        [-0.0215,  0.0142,  0.0167,  ...,  0.0239, -0.0027,  0.0054],
        [ 0.0149,  0.0173, -0.0055,  ...,  0.0210,  0.0174,  0.0063],
        ...,
        [ 0.0170,  0.0182,  0.0142,  ...,  0.0248,  0.0042, -0.0096],
        [-0.0226,  0.0284,  0.0073,  ..., -0.0204,  0.0177, -0.0314],
        [ 0.0077,  0.0228, -0.0271,  ...,  0.0219,  0.0241, -0.0094]],
       requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.3.attention.self.value.bias
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.3.attention.output.dense.weight
Parameter containing:
tensor([[ 0.0059,  0.0101, -0.0105,  ..., -0.0051, -0.0117,  0.0135],
        [-0.0228,  0.0294, -0.0219,  ..., -0.0068,  0.0531, -0.0214],
        [ 0.0047,  0.0280,  0.0151,  ...,  0.0090,  0.0134,  0.0224],
        ...,
        [ 0.0047,  0.0089,  0.0491,  ..., -0.0019, -0.0106,  0.0135],
        [-0.0067,  0.0263, -0.0556,  ...,  0.0134, -0.0321,  0.0223],
        [-0.0011, -0.0207,  0.0017,  ..., -0.0149, -0.0076, -0.0125]],
       requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.3.attention.output.dense.bias
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.3.attention.output.LayerNorm.weight
Parameter containing:
tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.3.attention.output.LayerNorm.bias
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.3.intermediate.dense.weight
Parameter containing:
tensor([[ 0.0200,  0.0166, -0.0082,  ...,  0.0304,  0.0198, -0.0032],
        [-0.0075,  0.0670,  0.0099,  ..., -0.0293,  0.0135, -0.0048],
        [ 0.0181,  0.0160, -0.0084,  ...,  0.0023, -0.0298, -0.0068],
        ...,
        [-0.0171,  0.0181, -0.0063,  ..., -0.0145,  0.0249, -0.0011],
        [-0.0458, -0.0081, -0.0248,  ...,  0.0018, -0.0082,  0.0049],
        [ 0.0295, -0.0159,  0.0113,  ...,  0.0032, -0.0440,  0.0053]],
       requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.3.intermediate.dense.bias
Parameter containing:
tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.3.output.dense.weight
Parameter containing:
tensor([[-0.0147, -0.0007,  0.0051,  ...,  0.0031, -0.0008, -0.0066],
        [ 0.0065,  0.0245, -0.0105,  ...,  0.0241,  0.0295, -0.0150],
        [-0.0207,  0.0082, -0.0027,  ...,  0.0090,  0.0098,  0.0068],
        ...,
        [-0.0207,  0.0068, -0.0040,  ..., -0.0389,  0.0271, -0.0314],
        [ 0.0022, -0.0250, -0.0053,  ..., -0.0183, -0.0028,  0.0180],
        [ 0.0142, -0.0240, -0.0010,  ..., -0.0186,  0.0179,  0.0080]],
       requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.3.output.dense.bias
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.3.output.LayerNorm.weight
Parameter containing:
tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.3.output.LayerNorm.bias
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.4.attention.self.query.weight
Parameter containing:
tensor([[ 0.0080, -0.0626, -0.0115,  ..., -0.0032, -0.0009, -0.0012],
        [-0.0028,  0.0084,  0.0265,  ...,  0.0245, -0.0082,  0.0024],
        [-0.0073, -0.0049,  0.0100,  ..., -0.0228, -0.0163, -0.0485],
        ...,
        [-0.0104,  0.0151,  0.0017,  ..., -0.0009, -0.0094,  0.0253],
        [ 0.0151,  0.0012, -0.0125,  ..., -0.0101, -0.0099,  0.0042],
        [ 0.0013,  0.0162, -0.0381,  ..., -0.0006,  0.0345, -0.0253]],
       requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.4.attention.self.query.bias
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.4.attention.self.key.weight
Parameter containing:
tensor([[ 0.0282, -0.0148,  0.0289,  ..., -0.0034,  0.0107, -0.0096],
        [ 0.0057, -0.0401, -0.0240,  ..., -0.0001,  0.0215,  0.0203],
        [ 0.0104, -0.0142,  0.0031,  ...,  0.0115,  0.0172, -0.0126],
        ...,
        [ 0.0258,  0.0061,  0.0079,  ..., -0.0045, -0.0448,  0.0045],
        [ 0.0235,  0.0218, -0.0044,  ...,  0.0128, -0.0132,  0.0167],
        [ 0.0103,  0.0003,  0.0185,  ...,  0.0112, -0.0011, -0.0021]],
       requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.4.attention.self.key.bias
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.4.attention.self.value.weight
Parameter containing:
tensor([[-0.0243, -0.0040, -0.0054,  ..., -0.0049,  0.0320, -0.0157],
        [ 0.0355, -0.0104,  0.0081,  ...,  0.0301,  0.0146, -0.0083],
        [-0.0303, -0.0013,  0.0206,  ...,  0.0207,  0.0102, -0.0026],
        ...,
        [ 0.0184,  0.0010, -0.0078,  ...,  0.0081, -0.0147, -0.0084],
        [-0.0340, -0.0046, -0.0137,  ...,  0.0188,  0.0394, -0.0066],
        [-0.0112,  0.0004,  0.0128,  ..., -0.0454, -0.0125, -0.0169]],
       requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.4.attention.self.value.bias
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.4.attention.output.dense.weight
Parameter containing:
tensor([[ 0.0261, -0.0151,  0.0056,  ...,  0.0235,  0.0098,  0.0009],
        [-0.0152,  0.0035,  0.0119,  ..., -0.0133,  0.0113, -0.0110],
        [ 0.0081,  0.0253, -0.0168,  ..., -0.0055, -0.0044,  0.0299],
        ...,
        [-0.0055, -0.0051,  0.0540,  ..., -0.0155, -0.0095, -0.0327],
        [-0.0185,  0.0137,  0.0337,  ..., -0.0201, -0.0065,  0.0203],
        [-0.0107, -0.0041, -0.0062,  ..., -0.0116, -0.0126, -0.0039]],
       requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.4.attention.output.dense.bias
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.4.attention.output.LayerNorm.weight
Parameter containing:
tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.4.attention.output.LayerNorm.bias
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.4.intermediate.dense.weight
Parameter containing:
tensor([[ 0.0030, -0.0527, -0.0185,  ...,  0.0170, -0.0067,  0.0304],
        [-0.0594, -0.0098, -0.0341,  ...,  0.0272, -0.0017,  0.0088],
        [-0.0086,  0.0150,  0.0205,  ...,  0.0484,  0.0057,  0.0129],
        ...,
        [ 0.0150, -0.0192,  0.0386,  ..., -0.0009, -0.0190,  0.0235],
        [ 0.0322, -0.0105,  0.0044,  ...,  0.0012, -0.0035,  0.0162],
        [ 0.0057, -0.0176,  0.0048,  ..., -0.0348,  0.0284,  0.0183]],
       requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.4.intermediate.dense.bias
Parameter containing:
tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.4.output.dense.weight
Parameter containing:
tensor([[ 0.0434,  0.0106,  0.0158,  ...,  0.0256,  0.0121,  0.0136],
        [-0.0352, -0.0466,  0.0347,  ...,  0.0121,  0.0415,  0.0044],
        [-0.0308, -0.0158,  0.0098,  ...,  0.0510,  0.0096, -0.0024],
        ...,
        [ 0.0009,  0.0075,  0.0196,  ..., -0.0093,  0.0134, -0.0002],
        [ 0.0073, -0.0054,  0.0386,  ..., -0.0192, -0.0397, -0.0292],
        [ 0.0116, -0.0091,  0.0126,  ..., -0.0159,  0.0194, -0.0115]],
       requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.4.output.dense.bias
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.4.output.LayerNorm.weight
Parameter containing:
tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.4.output.LayerNorm.bias
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.5.attention.self.query.weight
Parameter containing:
tensor([[-0.0288, -0.0021,  0.0101,  ..., -0.0105,  0.0144,  0.0092],
        [-0.0219, -0.0156, -0.0239,  ..., -0.0094, -0.0116, -0.0001],
        [ 0.0224, -0.0040, -0.0125,  ..., -0.0129, -0.0010,  0.0086],
        ...,
        [ 0.0003,  0.0080,  0.0012,  ..., -0.0068, -0.0306,  0.0085],
        [ 0.0056,  0.0361, -0.0245,  ...,  0.0048,  0.0201,  0.0068],
        [ 0.0236,  0.0228, -0.0099,  ...,  0.0015, -0.0141, -0.0142]],
       requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.5.attention.self.query.bias
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.5.attention.self.key.weight
Parameter containing:
tensor([[-0.0094, -0.0140, -0.0071,  ..., -0.0141, -0.0176, -0.0220],
        [ 0.0186,  0.0073,  0.0029,  ..., -0.0205,  0.0128,  0.0073],
        [ 0.0141, -0.0080, -0.0213,  ...,  0.0051, -0.0086, -0.0181],
        ...,
        [ 0.0356, -0.0253, -0.0307,  ...,  0.0024,  0.0201, -0.0008],
        [ 0.0302, -0.0281, -0.0448,  ..., -0.0125,  0.0040,  0.0156],
        [-0.0190, -0.0096,  0.0512,  ...,  0.0019,  0.0295, -0.0281]],
       requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.5.attention.self.key.bias
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.5.attention.self.value.weight
Parameter containing:
tensor([[-0.0051, -0.0204,  0.0210,  ...,  0.0270, -0.0164, -0.0122],
        [-0.0230, -0.0100,  0.0155,  ...,  0.0029, -0.0028,  0.0073],
        [-0.0211,  0.0204,  0.0002,  ..., -0.0212,  0.0025,  0.0099],
        ...,
        [ 0.0047, -0.0172,  0.0074,  ..., -0.0188, -0.0051,  0.0062],
        [-0.0209,  0.0186,  0.0085,  ...,  0.0014, -0.0090, -0.0153],
        [ 0.0029, -0.0254,  0.0354,  ...,  0.0162,  0.0355, -0.0115]],
       requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.5.attention.self.value.bias
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.5.attention.output.dense.weight
Parameter containing:
tensor([[ 5.6283e-03,  8.3458e-03,  1.2824e-02,  ..., -2.1874e-02,
          2.1041e-02,  1.0748e-02],
        [-3.7899e-03,  3.2215e-02, -5.8244e-03,  ..., -2.7889e-02,
         -1.0916e-02, -6.4945e-03],
        [ 3.0172e-03,  2.5224e-02,  2.6976e-02,  ..., -3.5250e-02,
          1.1822e-02,  5.4415e-03],
        ...,
        [-3.8001e-03, -8.7370e-05, -2.9498e-02,  ..., -1.7601e-02,
         -2.6850e-03, -2.6734e-02],
        [ 1.8936e-02,  1.1875e-02,  1.0427e-02,  ...,  3.0757e-02,
          4.0974e-03,  2.8124e-03],
        [-3.6535e-02,  4.1728e-03, -3.2062e-02,  ..., -5.9497e-03,
         -1.7814e-02, -7.3276e-03]], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.5.attention.output.dense.bias
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.5.attention.output.LayerNorm.weight
Parameter containing:
tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.5.attention.output.LayerNorm.bias
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.5.intermediate.dense.weight
Parameter containing:
tensor([[-0.0031, -0.0265, -0.0175,  ...,  0.0045, -0.0115, -0.0199],
        [-0.0067, -0.0223,  0.0043,  ..., -0.0237, -0.0055, -0.0055],
        [-0.0246,  0.0055, -0.0166,  ...,  0.0061,  0.0158, -0.0258],
        ...,
        [-0.0014, -0.0081,  0.0189,  ..., -0.0109,  0.0021, -0.0016],
        [-0.0123,  0.0240,  0.0075,  ..., -0.0259, -0.0130, -0.0130],
        [ 0.0123,  0.0086, -0.0180,  ..., -0.0107, -0.0087,  0.0033]],
       requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.5.intermediate.dense.bias
Parameter containing:
tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.5.output.dense.weight
Parameter containing:
tensor([[-0.0009, -0.0200,  0.0198,  ...,  0.0146,  0.0062, -0.0024],
        [ 0.0254,  0.0225, -0.0504,  ...,  0.0060, -0.0036,  0.0104],
        [ 0.0210,  0.0276, -0.0195,  ..., -0.0052, -0.0009, -0.0294],
        ...,
        [-0.0266,  0.0203,  0.0073,  ...,  0.0055,  0.0132, -0.0143],
        [-0.0004, -0.0002, -0.0359,  ...,  0.0242, -0.0080,  0.0027],
        [ 0.0200, -0.0249, -0.0004,  ..., -0.0221, -0.0257,  0.0242]],
       requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.5.output.dense.bias
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.5.output.LayerNorm.weight
Parameter containing:
tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.5.output.LayerNorm.bias
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.6.attention.self.query.weight
Parameter containing:
tensor([[-0.0354, -0.0032, -0.0150,  ...,  0.0041, -0.0366,  0.0304],
        [ 0.0036, -0.0037,  0.0306,  ..., -0.0007, -0.0301,  0.0087],
        [ 0.0170, -0.0083, -0.0058,  ..., -0.0029,  0.0040,  0.0009],
        ...,
        [-0.0172, -0.0088, -0.0078,  ..., -0.0157,  0.0016, -0.0069],
        [ 0.0087, -0.0466, -0.0135,  ..., -0.0005,  0.0028, -0.0012],
        [-0.0051,  0.0190, -0.0018,  ..., -0.0104, -0.0359, -0.0382]],
       requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.6.attention.self.query.bias
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.6.attention.self.key.weight
Parameter containing:
tensor([[-0.0059,  0.0451, -0.0292,  ..., -0.0273,  0.0121,  0.0017],
        [ 0.0146,  0.0047,  0.0057,  ..., -0.0143,  0.0293,  0.0032],
        [-0.0158, -0.0227, -0.0007,  ..., -0.0077, -0.0275,  0.0157],
        ...,
        [-0.0044,  0.0090,  0.0075,  ..., -0.0153,  0.0450,  0.0171],
        [-0.0075,  0.0073,  0.0057,  ...,  0.0032, -0.0168, -0.0347],
        [-0.0114, -0.0123,  0.0355,  ...,  0.0028, -0.0110,  0.0168]],
       requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.6.attention.self.key.bias
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.6.attention.self.value.weight
Parameter containing:
tensor([[-0.0229, -0.0206, -0.0021,  ...,  0.0124,  0.0214,  0.0118],
        [ 0.0206, -0.0133,  0.0289,  ..., -0.0101, -0.0012, -0.0119],
        [-0.0444,  0.0241, -0.0008,  ..., -0.0068,  0.0073, -0.0100],
        ...,
        [ 0.0156,  0.0007, -0.0341,  ...,  0.0138, -0.0125, -0.0150],
        [-0.0093, -0.0347, -0.0191,  ..., -0.0041, -0.0227, -0.0194],
        [-0.0047,  0.0129, -0.0237,  ...,  0.0043,  0.0034,  0.0351]],
       requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.6.attention.self.value.bias
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.6.attention.output.dense.weight
Parameter containing:
tensor([[-0.0217,  0.0189,  0.0164,  ...,  0.0281,  0.0119,  0.0369],
        [-0.0137,  0.0295,  0.0043,  ..., -0.0134,  0.0095,  0.0374],
        [ 0.0088,  0.0199, -0.0325,  ...,  0.0178,  0.0207, -0.0130],
        ...,
        [-0.0080,  0.0260, -0.0014,  ..., -0.0171, -0.0044,  0.0131],
        [-0.0152, -0.0213, -0.0154,  ..., -0.0337,  0.0142,  0.0084],
        [-0.0033, -0.0241, -0.0131,  ...,  0.0025,  0.0098,  0.0037]],
       requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.6.attention.output.dense.bias
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.6.attention.output.LayerNorm.weight
Parameter containing:
tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.6.attention.output.LayerNorm.bias
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.6.intermediate.dense.weight
Parameter containing:
tensor([[ 2.2215e-02, -6.1664e-03, -1.6078e-02,  ...,  2.5839e-03,
          9.4709e-03, -2.3556e-02],
        [ 7.8045e-03,  1.9027e-02, -2.0520e-02,  ..., -2.9476e-02,
          2.7276e-02, -8.1329e-03],
        [ 6.7797e-03, -1.2403e-02,  4.0934e-02,  ...,  3.7441e-02,
          3.9223e-02, -1.8617e-02],
        ...,
        [ 3.9690e-03, -6.3816e-03,  1.4835e-02,  ..., -2.2694e-03,
          7.1880e-03,  1.9380e-02],
        [-6.3749e-03, -2.3054e-02, -2.3087e-03,  ..., -2.0380e-05,
         -3.5466e-02,  3.3780e-02],
        [-1.5922e-02, -1.0717e-02,  2.5226e-02,  ..., -1.1825e-02,
         -8.4450e-04, -1.6808e-03]], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.6.intermediate.dense.bias
Parameter containing:
tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.6.output.dense.weight
Parameter containing:
tensor([[-0.0001,  0.0087,  0.0126,  ...,  0.0247, -0.0029, -0.0069],
        [-0.0026,  0.0307, -0.0268,  ...,  0.0128, -0.0005,  0.0440],
        [-0.0472,  0.0539, -0.0019,  ...,  0.0339, -0.0110,  0.0345],
        ...,
        [-0.0160, -0.0165, -0.0120,  ...,  0.0002, -0.0151, -0.0410],
        [-0.0104, -0.0275, -0.0246,  ...,  0.0024, -0.0243, -0.0195],
        [-0.0122, -0.0108,  0.0083,  ..., -0.0195,  0.0022, -0.0150]],
       requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.6.output.dense.bias
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.6.output.LayerNorm.weight
Parameter containing:
tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.6.output.LayerNorm.bias
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.7.attention.self.query.weight
Parameter containing:
tensor([[ 0.0314,  0.0442, -0.0085,  ...,  0.0056, -0.0227,  0.0537],
        [-0.0044,  0.0177,  0.0135,  ...,  0.0063, -0.0168,  0.0347],
        [ 0.0164, -0.0065,  0.0154,  ..., -0.0056, -0.0075, -0.0223],
        ...,
        [ 0.0208,  0.0183, -0.0206,  ...,  0.0009,  0.0358, -0.0062],
        [ 0.0184, -0.0044, -0.0097,  ...,  0.0104, -0.0137, -0.0220],
        [-0.0038,  0.0098, -0.0148,  ..., -0.0160,  0.0072, -0.0006]],
       requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.7.attention.self.query.bias
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.7.attention.self.key.weight
Parameter containing:
tensor([[-0.0016,  0.0212, -0.0316,  ..., -0.0262,  0.0026, -0.0188],
        [ 0.0012, -0.0088,  0.0083,  ...,  0.0068,  0.0202,  0.0227],
        [-0.0317, -0.0067,  0.0544,  ..., -0.0055,  0.0197,  0.0092],
        ...,
        [-0.0065, -0.0230,  0.0045,  ..., -0.0151, -0.0453, -0.0089],
        [ 0.0225,  0.0443, -0.0190,  ..., -0.0033, -0.0106,  0.0316],
        [-0.0145,  0.0332, -0.0170,  ..., -0.0266, -0.0207,  0.0001]],
       requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.7.attention.self.key.bias
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.7.attention.self.value.weight
Parameter containing:
tensor([[-0.0210, -0.0103, -0.0062,  ..., -0.0170,  0.0017,  0.0044],
        [-0.0296, -0.0228,  0.0290,  ...,  0.0199, -0.0008, -0.0101],
        [ 0.0020, -0.0089, -0.0051,  ...,  0.0083, -0.0331,  0.0018],
        ...,
        [-0.0008,  0.0124, -0.0059,  ..., -0.0053, -0.0243,  0.0010],
        [-0.0009,  0.0131, -0.0105,  ...,  0.0118, -0.0006,  0.0282],
        [ 0.0106,  0.0086,  0.0105,  ..., -0.0053, -0.0219, -0.0207]],
       requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.7.attention.self.value.bias
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.7.attention.output.dense.weight
Parameter containing:
tensor([[ 0.0395,  0.0049,  0.0176,  ...,  0.0080, -0.0203, -0.0165],
        [-0.0194, -0.0044, -0.0096,  ...,  0.0151,  0.0171,  0.0022],
        [ 0.0035,  0.0149, -0.0303,  ...,  0.0206, -0.0126, -0.0287],
        ...,
        [-0.0296,  0.0013,  0.0178,  ...,  0.0091, -0.0267,  0.0040],
        [ 0.0260,  0.0307,  0.0109,  ...,  0.0087, -0.0051, -0.0234],
        [-0.0002,  0.0097,  0.0152,  ...,  0.0066, -0.0005, -0.0082]],
       requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.7.attention.output.dense.bias
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.7.attention.output.LayerNorm.weight
Parameter containing:
tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.7.attention.output.LayerNorm.bias
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.7.intermediate.dense.weight
Parameter containing:
tensor([[ 0.0493,  0.0097, -0.0113,  ...,  0.0165, -0.0062,  0.0106],
        [-0.0216, -0.0179, -0.0085,  ...,  0.0176, -0.0218,  0.0286],
        [-0.0109,  0.0234, -0.0173,  ...,  0.0008, -0.0101, -0.0228],
        ...,
        [ 0.0184,  0.0195, -0.0200,  ...,  0.0279, -0.0217, -0.0103],
        [-0.0041, -0.0240, -0.0128,  ..., -0.0011, -0.0113,  0.0047],
        [-0.0013, -0.0353, -0.0199,  ..., -0.0032,  0.0304, -0.0081]],
       requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.7.intermediate.dense.bias
Parameter containing:
tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.7.output.dense.weight
Parameter containing:
tensor([[-3.0756e-02,  2.1763e-02,  1.9728e-03,  ...,  2.9442e-02,
          2.4510e-02, -1.9684e-02],
        [ 2.7042e-02,  2.6159e-02, -1.5640e-02,  ...,  2.2299e-02,
          8.0943e-03, -4.0150e-03],
        [ 3.5696e-02, -2.4142e-03,  1.5220e-02,  ..., -2.1856e-02,
          1.7067e-02,  1.5351e-02],
        ...,
        [-9.7817e-03,  2.1798e-02, -1.8652e-02,  ..., -1.0926e-05,
          1.9875e-02, -1.6324e-02],
        [ 2.1157e-02, -2.4501e-02,  3.3899e-02,  ...,  1.7612e-02,
         -8.1067e-03,  3.0692e-03],
        [-1.3236e-02,  2.0185e-02,  1.7353e-02,  ..., -1.5446e-02,
         -8.0510e-03, -1.1449e-02]], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.7.output.dense.bias
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.7.output.LayerNorm.weight
Parameter containing:
tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)
Model parameters:
transformer.bert.encoder.layer.7.output.LayerNorm.bias
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], requires_grad=True)
Model parameters:
transformer.bert.pooler.dense.weight
Parameter containing:
tensor([[-0.0209, -0.0141, -0.0226,  ...,  0.0135, -0.0191,  0.0008],
        [ 0.0153,  0.0190, -0.0276,  ...,  0.0212,  0.0257,  0.0064],
        [ 0.0028,  0.0272, -0.0140,  ...,  0.0020, -0.0170, -0.0011],
        ...,
        [-0.0193, -0.0016, -0.0419,  ...,  0.0034, -0.0159,  0.0213],
        [ 0.0046,  0.0043, -0.0066,  ...,  0.0174,  0.0380,  0.0325],
        [-0.0339,  0.0185,  0.0115,  ...,  0.0102,  0.0110, -0.0143]],
       requires_grad=True)
Model parameters:
transformer.bert.pooler.dense.bias
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], requires_grad=True)
Model parameters:
transformer.cls_embedding.0.weight
Parameter containing:
tensor([[-0.0172, -0.0562,  0.0068,  ...,  0.0479,  0.0066, -0.0561],
        [-0.0143, -0.0203,  0.0245,  ..., -0.0281,  0.0138,  0.0021],
        [-0.0140, -0.0549, -0.0038,  ...,  0.0545,  0.0089,  0.0227],
        ...,
        [ 0.0196,  0.0479,  0.0436,  ...,  0.0313,  0.0067, -0.0532],
        [-0.0374,  0.0444,  0.0342,  ...,  0.0497,  0.0502, -0.0484],
        [-0.0478, -0.0274,  0.0502,  ...,  0.0233, -0.0481, -0.0246]],
       requires_grad=True)
Model parameters:
transformer.cls_embedding.0.bias
Parameter containing:
tensor([-0.0093,  0.0173, -0.0501, -0.0305, -0.0014, -0.0427, -0.0326, -0.0263,
        -0.0545,  0.0253, -0.0336,  0.0323,  0.0064,  0.0251, -0.0175, -0.0521,
         0.0562, -0.0435, -0.0308, -0.0210, -0.0284,  0.0183,  0.0005,  0.0471,
        -0.0188,  0.0322, -0.0420, -0.0157, -0.0142, -0.0070,  0.0225, -0.0284,
         0.0555, -0.0403, -0.0406,  0.0236, -0.0369, -0.0326, -0.0238, -0.0109,
        -0.0308, -0.0551, -0.0060, -0.0214, -0.0089, -0.0519, -0.0121, -0.0287,
        -0.0054,  0.0562,  0.0026, -0.0181,  0.0350, -0.0165,  0.0507,  0.0002,
         0.0100, -0.0396, -0.0467, -0.0447, -0.0520, -0.0449, -0.0545, -0.0031,
         0.0111, -0.0354, -0.0493,  0.0226,  0.0351, -0.0092, -0.0518, -0.0407,
        -0.0547,  0.0074, -0.0234,  0.0189, -0.0297,  0.0076, -0.0451, -0.0321,
        -0.0102,  0.0323, -0.0452, -0.0020,  0.0497,  0.0514,  0.0198, -0.0383,
         0.0050, -0.0450, -0.0152, -0.0087, -0.0360,  0.0378,  0.0172,  0.0281,
        -0.0113,  0.0050, -0.0134, -0.0058, -0.0131,  0.0173, -0.0066,  0.0029,
        -0.0213, -0.0216,  0.0282,  0.0100,  0.0133, -0.0105, -0.0537, -0.0477,
         0.0340, -0.0181, -0.0095,  0.0321, -0.0156,  0.0432,  0.0382, -0.0288,
        -0.0009,  0.0411, -0.0300, -0.0197,  0.0121,  0.0074,  0.0328, -0.0032,
        -0.0208, -0.0466,  0.0395, -0.0558,  0.0475,  0.0031,  0.0497, -0.0257,
        -0.0440, -0.0260,  0.0495, -0.0126, -0.0279, -0.0215, -0.0140, -0.0543,
        -0.0330, -0.0448, -0.0516, -0.0221, -0.0497,  0.0010,  0.0360,  0.0430,
         0.0256,  0.0231,  0.0006, -0.0353, -0.0180,  0.0099, -0.0053, -0.0031,
         0.0327, -0.0180,  0.0208, -0.0341, -0.0079, -0.0127, -0.0481,  0.0197,
         0.0271, -0.0453, -0.0364, -0.0375,  0.0034, -0.0039,  0.0464, -0.0342,
        -0.0084,  0.0506, -0.0036,  0.0515, -0.0221,  0.0174,  0.0417, -0.0023,
        -0.0300, -0.0015,  0.0503,  0.0262,  0.0353, -0.0481,  0.0304,  0.0248,
         0.0104,  0.0236,  0.0030, -0.0229, -0.0122,  0.0033,  0.0091,  0.0126,
        -0.0443, -0.0488,  0.0361,  0.0305, -0.0117, -0.0405, -0.0078, -0.0266,
        -0.0416,  0.0110, -0.0459, -0.0486,  0.0530, -0.0125, -0.0316, -0.0032,
         0.0376, -0.0040,  0.0202,  0.0302, -0.0131, -0.0269, -0.0449, -0.0266,
        -0.0146,  0.0358,  0.0049,  0.0441, -0.0179, -0.0495,  0.0086, -0.0535,
         0.0272,  0.0363, -0.0307,  0.0005,  0.0478, -0.0146,  0.0150, -0.0472,
         0.0114, -0.0301,  0.0024,  0.0352, -0.0535, -0.0209,  0.0191,  0.0311,
        -0.0209, -0.0511,  0.0495,  0.0417,  0.0378, -0.0333,  0.0506, -0.0395,
        -0.0200,  0.0208,  0.0440,  0.0397, -0.0333,  0.0081,  0.0447,  0.0349,
        -0.0099,  0.0123,  0.0209,  0.0341,  0.0025,  0.0138,  0.0556,  0.0453,
        -0.0260,  0.0028,  0.0103,  0.0052,  0.0155,  0.0317, -0.0539,  0.0473,
        -0.0129,  0.0275,  0.0020, -0.0180, -0.0146,  0.0402, -0.0124, -0.0499,
         0.0302, -0.0504, -0.0524,  0.0462,  0.0353,  0.0136,  0.0491, -0.0191,
        -0.0093, -0.0432, -0.0083,  0.0497,  0.0281,  0.0377,  0.0542, -0.0008,
        -0.0117,  0.0388,  0.0509, -0.0294,  0.0301, -0.0286, -0.0078, -0.0328,
        -0.0072,  0.0486, -0.0266,  0.0456], requires_grad=True)
Model parameters:
high_spatial_attention.qkv.weight
Parameter containing:
tensor([[-0.0907,  0.0650, -0.0058,  ...,  0.0173,  0.0243, -0.0240],
        [ 0.0240,  0.0794, -0.0764,  ..., -0.0213,  0.0900,  0.0289],
        [ 0.0761, -0.0975,  0.0508,  ...,  0.0621,  0.0911,  0.0551],
        ...,
        [ 0.0087,  0.0534,  0.0330,  ..., -0.0966, -0.0360,  0.0909],
        [-0.0922, -0.0600,  0.0141,  ...,  0.0279, -0.0252, -0.0030],
        [-0.0556,  0.0188,  0.0723,  ...,  0.0570, -0.0570, -0.0027]],
       requires_grad=True)
Model parameters:
low_spatial_attention.qkv.weight
Parameter containing:
tensor([[-0.1031,  0.0421,  0.0660,  ..., -0.0767,  0.0100,  0.0223],
        [ 0.0254, -0.0665, -0.0904,  ...,  0.0744, -0.0265,  0.0651],
        [ 0.0575, -0.0510,  0.0888,  ..., -0.0575, -0.0921,  0.0338],
        ...,
        [ 0.0838, -0.0860,  0.0743,  ...,  0.0689, -0.0626,  0.0260],
        [ 0.0484,  0.0816,  0.0520,  ..., -0.0293,  0.0121, -0.0367],
        [ 0.0433, -0.0103,  0.0882,  ...,  0.0273,  0.0289,  0.0750]],
       requires_grad=True)
Model parameters:
ultralow_spatial_attention.qkv.weight
Parameter containing:
tensor([[ 0.0590, -0.0447, -0.0219,  ...,  0.0516, -0.0256,  0.0457],
        [-0.0810,  0.0656, -0.0206,  ...,  0.0843,  0.0103, -0.0403],
        [-0.0748, -0.0208,  0.0701,  ...,  0.0981, -0.0361, -0.0934],
        ...,
        [-0.0083,  0.0191,  0.0486,  ...,  0.0266,  0.0336,  0.0757],
        [-0.0169, -0.0289, -0.0396,  ..., -0.0053, -0.0288,  0.0171],
        [-0.0832,  0.0193,  0.0565,  ...,  0.0367,  0.0747,  0.0768]],
       requires_grad=True)
Model parameters:
regression_head.linear.weight
Parameter containing:
tensor([[ 0.0113,  0.0260,  0.0435,  0.0308,  0.0562, -0.0498,  0.0133, -0.0277,
         -0.0051, -0.0130,  0.0341,  0.0519, -0.0095,  0.0027,  0.0384, -0.0560,
         -0.0498, -0.0182, -0.0396, -0.0441, -0.0479,  0.0320, -0.0317, -0.0182,
         -0.0272,  0.0203,  0.0015, -0.0130,  0.0392,  0.0464, -0.0250, -0.0193,
          0.0531, -0.0162,  0.0393,  0.0153,  0.0219,  0.0143,  0.0431, -0.0292,
          0.0383,  0.0092,  0.0517, -0.0366,  0.0512, -0.0331, -0.0038,  0.0177,
         -0.0420, -0.0338,  0.0242,  0.0381,  0.0133, -0.0407, -0.0060,  0.0305,
         -0.0172, -0.0189, -0.0369, -0.0463,  0.0248,  0.0553,  0.0034,  0.0402,
          0.0171,  0.0126, -0.0251,  0.0183,  0.0396,  0.0009, -0.0256,  0.0368,
          0.0098,  0.0346, -0.0015,  0.0531,  0.0535, -0.0524, -0.0096,  0.0385,
         -0.0346, -0.0379,  0.0422,  0.0522,  0.0246, -0.0005, -0.0010,  0.0197,
         -0.0348,  0.0036,  0.0202, -0.0426, -0.0532, -0.0535, -0.0024,  0.0201,
         -0.0450, -0.0440,  0.0351,  0.0516,  0.0477, -0.0250, -0.0488, -0.0377,
         -0.0050, -0.0111, -0.0202,  0.0525,  0.0470, -0.0281,  0.0342, -0.0096,
         -0.0090,  0.0255, -0.0244,  0.0353,  0.0357,  0.0101,  0.0088,  0.0396,
          0.0521, -0.0411, -0.0028,  0.0076, -0.0384, -0.0373,  0.0297, -0.0040,
         -0.0310,  0.0215,  0.0142, -0.0423,  0.0368,  0.0381,  0.0276, -0.0060,
         -0.0100,  0.0532,  0.0091,  0.0023,  0.0542, -0.0436, -0.0423,  0.0152,
         -0.0329, -0.0300, -0.0173,  0.0107,  0.0386, -0.0458, -0.0082,  0.0413,
          0.0476,  0.0064, -0.0311,  0.0419,  0.0146, -0.0083, -0.0114,  0.0331,
         -0.0375,  0.0347,  0.0136,  0.0374, -0.0495, -0.0167,  0.0558,  0.0150,
          0.0060,  0.0146,  0.0113,  0.0304,  0.0519,  0.0530,  0.0023, -0.0464,
          0.0257,  0.0020,  0.0468,  0.0108, -0.0076, -0.0094, -0.0445,  0.0028,
         -0.0232, -0.0384, -0.0132, -0.0179,  0.0084, -0.0389,  0.0076, -0.0138,
         -0.0118,  0.0499,  0.0420, -0.0466, -0.0515, -0.0474,  0.0552,  0.0081,
          0.0217,  0.0073, -0.0090, -0.0167, -0.0122,  0.0354, -0.0285, -0.0454,
         -0.0020, -0.0527, -0.0398,  0.0089,  0.0044,  0.0159, -0.0302,  0.0547,
          0.0416,  0.0221,  0.0213,  0.0144,  0.0440, -0.0255,  0.0346,  0.0354,
         -0.0051, -0.0034, -0.0260,  0.0425,  0.0197,  0.0087,  0.0490,  0.0157,
          0.0055,  0.0126,  0.0471, -0.0281, -0.0360,  0.0232, -0.0239,  0.0186,
          0.0132, -0.0049, -0.0510, -0.0315, -0.0537,  0.0121,  0.0250, -0.0542,
         -0.0363, -0.0307,  0.0550,  0.0288,  0.0373, -0.0257, -0.0361,  0.0009,
         -0.0507, -0.0515,  0.0348, -0.0538,  0.0279, -0.0300,  0.0447, -0.0435,
          0.0328, -0.0371, -0.0403,  0.0303, -0.0045,  0.0382, -0.0329,  0.0326,
         -0.0399, -0.0223,  0.0302,  0.0552, -0.0366,  0.0274,  0.0317, -0.0372,
          0.0436,  0.0262,  0.0124, -0.0227, -0.0078, -0.0548,  0.0268,  0.0205,
          0.0041,  0.0407,  0.0546,  0.0338, -0.0549,  0.0279,  0.0357,  0.0024,
         -0.0522,  0.0428,  0.0060,  0.0562,  0.0205,  0.0267, -0.0175,  0.0522,
          0.0014, -0.0492,  0.0463,  0.0145, -0.0538,  0.0252, -0.0289,  0.0382,
          0.0168, -0.0040, -0.0124, -0.0162]], requires_grad=True)
Model parameters:
regression_head.linear.bias
Parameter containing:
tensor([0.0362], requires_grad=True)
Model parameters:
regression_head.norm.weight
Parameter containing:
tensor([1.], requires_grad=True)
Model parameters:
regression_head.norm.bias
Parameter containing:
tensor([0.], requires_grad=True)
Model parameters:
<generator object Module.parameters at 0x7f6adb2bc0b0>
Model layers info:
Model layer:

Transformer_Finetune_Three_Channels(
  (transformer): Transformer_Block(
    (bert): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(1, 316, padding_idx=0)
        (position_embeddings): Embedding(93, 316)
        (token_type_embeddings): Embedding(2, 316)
        (LayerNorm): LayerNorm((316,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.3, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0-7): 8 x BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=316, out_features=316, bias=True)
                (key): Linear(in_features=316, out_features=316, bias=True)
                (value): Linear(in_features=316, out_features=316, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=316, out_features=316, bias=True)
                (LayerNorm): LayerNorm((316,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.3, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=316, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=316, bias=True)
              (LayerNorm): LayerNorm((316,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.3, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=316, out_features=316, bias=True)
        (activation): Tanh()
      )
    )
    (cls_embedding): Sequential(
      (0): Linear(in_features=316, out_features=316, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
    )
  )
  (high_spatial_attention): Attention(
    (qkv): Linear(in_features=92, out_features=276, bias=False)
    (attn_drop): Dropout(p=0.0, inplace=False)
  )
  (low_spatial_attention): Attention(
    (qkv): Linear(in_features=92, out_features=276, bias=False)
    (attn_drop): Dropout(p=0.0, inplace=False)
  )
  (ultralow_spatial_attention): Attention(
    (qkv): Linear(in_features=92, out_features=276, bias=False)
    (attn_drop): Dropout(p=0.0, inplace=False)
  )
  (regression_head): Classifier(
    (linear): Linear(in_features=316, out_features=1, bias=True)
    (dropout): Dropout(p=0.6, inplace=False)
    (norm): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
)
Model layer:
transformer
Transformer_Block(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(1, 316, padding_idx=0)
      (position_embeddings): Embedding(93, 316)
      (token_type_embeddings): Embedding(2, 316)
      (LayerNorm): LayerNorm((316,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.3, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-7): 8 x BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=316, out_features=316, bias=True)
              (key): Linear(in_features=316, out_features=316, bias=True)
              (value): Linear(in_features=316, out_features=316, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=316, out_features=316, bias=True)
              (LayerNorm): LayerNorm((316,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.3, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=316, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=316, bias=True)
            (LayerNorm): LayerNorm((316,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.3, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=316, out_features=316, bias=True)
      (activation): Tanh()
    )
  )
  (cls_embedding): Sequential(
    (0): Linear(in_features=316, out_features=316, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
  )
)
Model layer:
transformer.bert
BertModel(
  (embeddings): BertEmbeddings(
    (word_embeddings): Embedding(1, 316, padding_idx=0)
    (position_embeddings): Embedding(93, 316)
    (token_type_embeddings): Embedding(2, 316)
    (LayerNorm): LayerNorm((316,), eps=1e-12, elementwise_affine=True)
    (dropout): Dropout(p=0.3, inplace=False)
  )
  (encoder): BertEncoder(
    (layer): ModuleList(
      (0-7): 8 x BertLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=316, out_features=316, bias=True)
            (key): Linear(in_features=316, out_features=316, bias=True)
            (value): Linear(in_features=316, out_features=316, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=316, out_features=316, bias=True)
            (LayerNorm): LayerNorm((316,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.3, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=316, out_features=3072, bias=True)
          (intermediate_act_fn): GELUActivation()
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=316, bias=True)
          (LayerNorm): LayerNorm((316,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.3, inplace=False)
        )
      )
    )
  )
  (pooler): BertPooler(
    (dense): Linear(in_features=316, out_features=316, bias=True)
    (activation): Tanh()
  )
)
Model layer:
transformer.bert.embeddings
BertEmbeddings(
  (word_embeddings): Embedding(1, 316, padding_idx=0)
  (position_embeddings): Embedding(93, 316)
  (token_type_embeddings): Embedding(2, 316)
  (LayerNorm): LayerNorm((316,), eps=1e-12, elementwise_affine=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
Model layer:
transformer.bert.embeddings.word_embeddings
Embedding(1, 316, padding_idx=0)
Model layer:
transformer.bert.embeddings.position_embeddings
Embedding(93, 316)
Model layer:
transformer.bert.embeddings.token_type_embeddings
Embedding(2, 316)
Model layer:
transformer.bert.embeddings.LayerNorm
LayerNorm((316,), eps=1e-12, elementwise_affine=True)
Model layer:
transformer.bert.embeddings.dropout
Dropout(p=0.3, inplace=False)
Model layer:
transformer.bert.encoder
BertEncoder(
  (layer): ModuleList(
    (0-7): 8 x BertLayer(
      (attention): BertAttention(
        (self): BertSelfAttention(
          (query): Linear(in_features=316, out_features=316, bias=True)
          (key): Linear(in_features=316, out_features=316, bias=True)
          (value): Linear(in_features=316, out_features=316, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (output): BertSelfOutput(
          (dense): Linear(in_features=316, out_features=316, bias=True)
          (LayerNorm): LayerNorm((316,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.3, inplace=False)
        )
      )
      (intermediate): BertIntermediate(
        (dense): Linear(in_features=316, out_features=3072, bias=True)
        (intermediate_act_fn): GELUActivation()
      )
      (output): BertOutput(
        (dense): Linear(in_features=3072, out_features=316, bias=True)
        (LayerNorm): LayerNorm((316,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.3, inplace=False)
      )
    )
  )
)
Model layer:
transformer.bert.encoder.layer
ModuleList(
  (0-7): 8 x BertLayer(
    (attention): BertAttention(
      (self): BertSelfAttention(
        (query): Linear(in_features=316, out_features=316, bias=True)
        (key): Linear(in_features=316, out_features=316, bias=True)
        (value): Linear(in_features=316, out_features=316, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (output): BertSelfOutput(
        (dense): Linear(in_features=316, out_features=316, bias=True)
        (LayerNorm): LayerNorm((316,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.3, inplace=False)
      )
    )
    (intermediate): BertIntermediate(
      (dense): Linear(in_features=316, out_features=3072, bias=True)
      (intermediate_act_fn): GELUActivation()
    )
    (output): BertOutput(
      (dense): Linear(in_features=3072, out_features=316, bias=True)
      (LayerNorm): LayerNorm((316,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.3, inplace=False)
    )
  )
)
Model layer:
transformer.bert.encoder.layer.0
BertLayer(
  (attention): BertAttention(
    (self): BertSelfAttention(
      (query): Linear(in_features=316, out_features=316, bias=True)
      (key): Linear(in_features=316, out_features=316, bias=True)
      (value): Linear(in_features=316, out_features=316, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (output): BertSelfOutput(
      (dense): Linear(in_features=316, out_features=316, bias=True)
      (LayerNorm): LayerNorm((316,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.3, inplace=False)
    )
  )
  (intermediate): BertIntermediate(
    (dense): Linear(in_features=316, out_features=3072, bias=True)
    (intermediate_act_fn): GELUActivation()
  )
  (output): BertOutput(
    (dense): Linear(in_features=3072, out_features=316, bias=True)
    (LayerNorm): LayerNorm((316,), eps=1e-12, elementwise_affine=True)
    (dropout): Dropout(p=0.3, inplace=False)
  )
)
Model layer:
transformer.bert.encoder.layer.0.attention
BertAttention(
  (self): BertSelfAttention(
    (query): Linear(in_features=316, out_features=316, bias=True)
    (key): Linear(in_features=316, out_features=316, bias=True)
    (value): Linear(in_features=316, out_features=316, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (output): BertSelfOutput(
    (dense): Linear(in_features=316, out_features=316, bias=True)
    (LayerNorm): LayerNorm((316,), eps=1e-12, elementwise_affine=True)
    (dropout): Dropout(p=0.3, inplace=False)
  )
)
Model layer:
transformer.bert.encoder.layer.0.attention.self
BertSelfAttention(
  (query): Linear(in_features=316, out_features=316, bias=True)
  (key): Linear(in_features=316, out_features=316, bias=True)
  (value): Linear(in_features=316, out_features=316, bias=True)
  (dropout): Dropout(p=0.1, inplace=False)
)
Model layer:
transformer.bert.encoder.layer.0.attention.self.query
Linear(in_features=316, out_features=316, bias=True)
Model layer:
transformer.bert.encoder.layer.0.attention.self.key
Linear(in_features=316, out_features=316, bias=True)
Model layer:
transformer.bert.encoder.layer.0.attention.self.value
Linear(in_features=316, out_features=316, bias=True)
Model layer:
transformer.bert.encoder.layer.0.attention.self.dropout
Dropout(p=0.1, inplace=False)
Model layer:
transformer.bert.encoder.layer.0.attention.output
BertSelfOutput(
  (dense): Linear(in_features=316, out_features=316, bias=True)
  (LayerNorm): LayerNorm((316,), eps=1e-12, elementwise_affine=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
Model layer:
transformer.bert.encoder.layer.0.attention.output.dense
Linear(in_features=316, out_features=316, bias=True)
Model layer:
transformer.bert.encoder.layer.0.attention.output.LayerNorm
LayerNorm((316,), eps=1e-12, elementwise_affine=True)
Model layer:
transformer.bert.encoder.layer.0.attention.output.dropout
Dropout(p=0.3, inplace=False)
Model layer:
transformer.bert.encoder.layer.0.intermediate
BertIntermediate(
  (dense): Linear(in_features=316, out_features=3072, bias=True)
  (intermediate_act_fn): GELUActivation()
)
Model layer:
transformer.bert.encoder.layer.0.intermediate.dense
Linear(in_features=316, out_features=3072, bias=True)
Model layer:
transformer.bert.encoder.layer.0.intermediate.intermediate_act_fn
GELUActivation()
Model layer:
transformer.bert.encoder.layer.0.output
BertOutput(
  (dense): Linear(in_features=3072, out_features=316, bias=True)
  (LayerNorm): LayerNorm((316,), eps=1e-12, elementwise_affine=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
Model layer:
transformer.bert.encoder.layer.0.output.dense
Linear(in_features=3072, out_features=316, bias=True)
Model layer:
transformer.bert.encoder.layer.0.output.LayerNorm
LayerNorm((316,), eps=1e-12, elementwise_affine=True)
Model layer:
transformer.bert.encoder.layer.0.output.dropout
Dropout(p=0.3, inplace=False)
Model layer:
transformer.bert.encoder.layer.1
BertLayer(
  (attention): BertAttention(
    (self): BertSelfAttention(
      (query): Linear(in_features=316, out_features=316, bias=True)
      (key): Linear(in_features=316, out_features=316, bias=True)
      (value): Linear(in_features=316, out_features=316, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (output): BertSelfOutput(
      (dense): Linear(in_features=316, out_features=316, bias=True)
      (LayerNorm): LayerNorm((316,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.3, inplace=False)
    )
  )
  (intermediate): BertIntermediate(
    (dense): Linear(in_features=316, out_features=3072, bias=True)
    (intermediate_act_fn): GELUActivation()
  )
  (output): BertOutput(
    (dense): Linear(in_features=3072, out_features=316, bias=True)
    (LayerNorm): LayerNorm((316,), eps=1e-12, elementwise_affine=True)
    (dropout): Dropout(p=0.3, inplace=False)
  )
)
Model layer:
transformer.bert.encoder.layer.1.attention
BertAttention(
  (self): BertSelfAttention(
    (query): Linear(in_features=316, out_features=316, bias=True)
    (key): Linear(in_features=316, out_features=316, bias=True)
    (value): Linear(in_features=316, out_features=316, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (output): BertSelfOutput(
    (dense): Linear(in_features=316, out_features=316, bias=True)
    (LayerNorm): LayerNorm((316,), eps=1e-12, elementwise_affine=True)
    (dropout): Dropout(p=0.3, inplace=False)
  )
)
Model layer:
transformer.bert.encoder.layer.1.attention.self
BertSelfAttention(
  (query): Linear(in_features=316, out_features=316, bias=True)
  (key): Linear(in_features=316, out_features=316, bias=True)
  (value): Linear(in_features=316, out_features=316, bias=True)
  (dropout): Dropout(p=0.1, inplace=False)
)
Model layer:
transformer.bert.encoder.layer.1.attention.self.query
Linear(in_features=316, out_features=316, bias=True)
Model layer:
transformer.bert.encoder.layer.1.attention.self.key
Linear(in_features=316, out_features=316, bias=True)
Model layer:
transformer.bert.encoder.layer.1.attention.self.value
Linear(in_features=316, out_features=316, bias=True)
Model layer:
transformer.bert.encoder.layer.1.attention.self.dropout
Dropout(p=0.1, inplace=False)
Model layer:
transformer.bert.encoder.layer.1.attention.output
BertSelfOutput(
  (dense): Linear(in_features=316, out_features=316, bias=True)
  (LayerNorm): LayerNorm((316,), eps=1e-12, elementwise_affine=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
Model layer:
transformer.bert.encoder.layer.1.attention.output.dense
Linear(in_features=316, out_features=316, bias=True)
Model layer:
transformer.bert.encoder.layer.1.attention.output.LayerNorm
LayerNorm((316,), eps=1e-12, elementwise_affine=True)
Model layer:
transformer.bert.encoder.layer.1.attention.output.dropout
Dropout(p=0.3, inplace=False)
Model layer:
transformer.bert.encoder.layer.1.intermediate
BertIntermediate(
  (dense): Linear(in_features=316, out_features=3072, bias=True)
  (intermediate_act_fn): GELUActivation()
)
Model layer:
transformer.bert.encoder.layer.1.intermediate.dense
Linear(in_features=316, out_features=3072, bias=True)
Model layer:
transformer.bert.encoder.layer.1.intermediate.intermediate_act_fn
GELUActivation()
Model layer:
transformer.bert.encoder.layer.1.output
BertOutput(
  (dense): Linear(in_features=3072, out_features=316, bias=True)
  (LayerNorm): LayerNorm((316,), eps=1e-12, elementwise_affine=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
Model layer:
transformer.bert.encoder.layer.1.output.dense
Linear(in_features=3072, out_features=316, bias=True)
Model layer:
transformer.bert.encoder.layer.1.output.LayerNorm
LayerNorm((316,), eps=1e-12, elementwise_affine=True)
Model layer:
transformer.bert.encoder.layer.1.output.dropout
Dropout(p=0.3, inplace=False)
Model layer:
transformer.bert.encoder.layer.2
BertLayer(
  (attention): BertAttention(
    (self): BertSelfAttention(
      (query): Linear(in_features=316, out_features=316, bias=True)
      (key): Linear(in_features=316, out_features=316, bias=True)
      (value): Linear(in_features=316, out_features=316, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (output): BertSelfOutput(
      (dense): Linear(in_features=316, out_features=316, bias=True)
      (LayerNorm): LayerNorm((316,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.3, inplace=False)
    )
  )
  (intermediate): BertIntermediate(
    (dense): Linear(in_features=316, out_features=3072, bias=True)
    (intermediate_act_fn): GELUActivation()
  )
  (output): BertOutput(
    (dense): Linear(in_features=3072, out_features=316, bias=True)
    (LayerNorm): LayerNorm((316,), eps=1e-12, elementwise_affine=True)
    (dropout): Dropout(p=0.3, inplace=False)
  )
)
Model layer:
transformer.bert.encoder.layer.2.attention
BertAttention(
  (self): BertSelfAttention(
    (query): Linear(in_features=316, out_features=316, bias=True)
    (key): Linear(in_features=316, out_features=316, bias=True)
    (value): Linear(in_features=316, out_features=316, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (output): BertSelfOutput(
    (dense): Linear(in_features=316, out_features=316, bias=True)
    (LayerNorm): LayerNorm((316,), eps=1e-12, elementwise_affine=True)
    (dropout): Dropout(p=0.3, inplace=False)
  )
)
Model layer:
transformer.bert.encoder.layer.2.attention.self
BertSelfAttention(
  (query): Linear(in_features=316, out_features=316, bias=True)
  (key): Linear(in_features=316, out_features=316, bias=True)
  (value): Linear(in_features=316, out_features=316, bias=True)
  (dropout): Dropout(p=0.1, inplace=False)
)
Model layer:
transformer.bert.encoder.layer.2.attention.self.query
Linear(in_features=316, out_features=316, bias=True)
Model layer:
transformer.bert.encoder.layer.2.attention.self.key
Linear(in_features=316, out_features=316, bias=True)
Model layer:
transformer.bert.encoder.layer.2.attention.self.value
Linear(in_features=316, out_features=316, bias=True)
Model layer:
transformer.bert.encoder.layer.2.attention.self.dropout
Dropout(p=0.1, inplace=False)
Model layer:
transformer.bert.encoder.layer.2.attention.output
BertSelfOutput(
  (dense): Linear(in_features=316, out_features=316, bias=True)
  (LayerNorm): LayerNorm((316,), eps=1e-12, elementwise_affine=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
Model layer:
transformer.bert.encoder.layer.2.attention.output.dense
Linear(in_features=316, out_features=316, bias=True)
Model layer:
transformer.bert.encoder.layer.2.attention.output.LayerNorm
LayerNorm((316,), eps=1e-12, elementwise_affine=True)
Model layer:
transformer.bert.encoder.layer.2.attention.output.dropout
Dropout(p=0.3, inplace=False)
Model layer:
transformer.bert.encoder.layer.2.intermediate
BertIntermediate(
  (dense): Linear(in_features=316, out_features=3072, bias=True)
  (intermediate_act_fn): GELUActivation()
)
Model layer:
transformer.bert.encoder.layer.2.intermediate.dense
Linear(in_features=316, out_features=3072, bias=True)
Model layer:
transformer.bert.encoder.layer.2.intermediate.intermediate_act_fn
GELUActivation()
Model layer:
transformer.bert.encoder.layer.2.output
BertOutput(
  (dense): Linear(in_features=3072, out_features=316, bias=True)
  (LayerNorm): LayerNorm((316,), eps=1e-12, elementwise_affine=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
Model layer:
transformer.bert.encoder.layer.2.output.dense
Linear(in_features=3072, out_features=316, bias=True)
Model layer:
transformer.bert.encoder.layer.2.output.LayerNorm
LayerNorm((316,), eps=1e-12, elementwise_affine=True)
Model layer:
transformer.bert.encoder.layer.2.output.dropout
Dropout(p=0.3, inplace=False)
Model layer:
transformer.bert.encoder.layer.3
BertLayer(
  (attention): BertAttention(
    (self): BertSelfAttention(
      (query): Linear(in_features=316, out_features=316, bias=True)
      (key): Linear(in_features=316, out_features=316, bias=True)
      (value): Linear(in_features=316, out_features=316, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (output): BertSelfOutput(
      (dense): Linear(in_features=316, out_features=316, bias=True)
      (LayerNorm): LayerNorm((316,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.3, inplace=False)
    )
  )
  (intermediate): BertIntermediate(
    (dense): Linear(in_features=316, out_features=3072, bias=True)
    (intermediate_act_fn): GELUActivation()
  )
  (output): BertOutput(
    (dense): Linear(in_features=3072, out_features=316, bias=True)
    (LayerNorm): LayerNorm((316,), eps=1e-12, elementwise_affine=True)
    (dropout): Dropout(p=0.3, inplace=False)
  )
)
Model layer:
transformer.bert.encoder.layer.3.attention
BertAttention(
  (self): BertSelfAttention(
    (query): Linear(in_features=316, out_features=316, bias=True)
    (key): Linear(in_features=316, out_features=316, bias=True)
    (value): Linear(in_features=316, out_features=316, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (output): BertSelfOutput(
    (dense): Linear(in_features=316, out_features=316, bias=True)
    (LayerNorm): LayerNorm((316,), eps=1e-12, elementwise_affine=True)
    (dropout): Dropout(p=0.3, inplace=False)
  )
)
Model layer:
transformer.bert.encoder.layer.3.attention.self
BertSelfAttention(
  (query): Linear(in_features=316, out_features=316, bias=True)
  (key): Linear(in_features=316, out_features=316, bias=True)
  (value): Linear(in_features=316, out_features=316, bias=True)
  (dropout): Dropout(p=0.1, inplace=False)
)
Model layer:
transformer.bert.encoder.layer.3.attention.self.query
Linear(in_features=316, out_features=316, bias=True)
Model layer:
transformer.bert.encoder.layer.3.attention.self.key
Linear(in_features=316, out_features=316, bias=True)
Model layer:
transformer.bert.encoder.layer.3.attention.self.value
Linear(in_features=316, out_features=316, bias=True)
Model layer:
transformer.bert.encoder.layer.3.attention.self.dropout
Dropout(p=0.1, inplace=False)
Model layer:
transformer.bert.encoder.layer.3.attention.output
BertSelfOutput(
  (dense): Linear(in_features=316, out_features=316, bias=True)
  (LayerNorm): LayerNorm((316,), eps=1e-12, elementwise_affine=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
Model layer:
transformer.bert.encoder.layer.3.attention.output.dense
Linear(in_features=316, out_features=316, bias=True)
Model layer:
transformer.bert.encoder.layer.3.attention.output.LayerNorm
LayerNorm((316,), eps=1e-12, elementwise_affine=True)
Model layer:
transformer.bert.encoder.layer.3.attention.output.dropout
Dropout(p=0.3, inplace=False)
Model layer:
transformer.bert.encoder.layer.3.intermediate
BertIntermediate(
  (dense): Linear(in_features=316, out_features=3072, bias=True)
  (intermediate_act_fn): GELUActivation()
)
Model layer:
transformer.bert.encoder.layer.3.intermediate.dense
Linear(in_features=316, out_features=3072, bias=True)
Model layer:
transformer.bert.encoder.layer.3.intermediate.intermediate_act_fn
GELUActivation()
Model layer:
transformer.bert.encoder.layer.3.output
BertOutput(
  (dense): Linear(in_features=3072, out_features=316, bias=True)
  (LayerNorm): LayerNorm((316,), eps=1e-12, elementwise_affine=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
Model layer:
transformer.bert.encoder.layer.3.output.dense
Linear(in_features=3072, out_features=316, bias=True)
Model layer:
transformer.bert.encoder.layer.3.output.LayerNorm
LayerNorm((316,), eps=1e-12, elementwise_affine=True)
Model layer:
transformer.bert.encoder.layer.3.output.dropout
Dropout(p=0.3, inplace=False)
Model layer:
transformer.bert.encoder.layer.4
BertLayer(
  (attention): BertAttention(
    (self): BertSelfAttention(
      (query): Linear(in_features=316, out_features=316, bias=True)
      (key): Linear(in_features=316, out_features=316, bias=True)
      (value): Linear(in_features=316, out_features=316, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (output): BertSelfOutput(
      (dense): Linear(in_features=316, out_features=316, bias=True)
      (LayerNorm): LayerNorm((316,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.3, inplace=False)
    )
  )
  (intermediate): BertIntermediate(
    (dense): Linear(in_features=316, out_features=3072, bias=True)
    (intermediate_act_fn): GELUActivation()
  )
  (output): BertOutput(
    (dense): Linear(in_features=3072, out_features=316, bias=True)
    (LayerNorm): LayerNorm((316,), eps=1e-12, elementwise_affine=True)
    (dropout): Dropout(p=0.3, inplace=False)
  )
)
Model layer:
transformer.bert.encoder.layer.4.attention
BertAttention(
  (self): BertSelfAttention(
    (query): Linear(in_features=316, out_features=316, bias=True)
    (key): Linear(in_features=316, out_features=316, bias=True)
    (value): Linear(in_features=316, out_features=316, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (output): BertSelfOutput(
    (dense): Linear(in_features=316, out_features=316, bias=True)
    (LayerNorm): LayerNorm((316,), eps=1e-12, elementwise_affine=True)
    (dropout): Dropout(p=0.3, inplace=False)
  )
)
Model layer:
transformer.bert.encoder.layer.4.attention.self
BertSelfAttention(
  (query): Linear(in_features=316, out_features=316, bias=True)
  (key): Linear(in_features=316, out_features=316, bias=True)
  (value): Linear(in_features=316, out_features=316, bias=True)
  (dropout): Dropout(p=0.1, inplace=False)
)
Model layer:
transformer.bert.encoder.layer.4.attention.self.query
Linear(in_features=316, out_features=316, bias=True)
Model layer:
transformer.bert.encoder.layer.4.attention.self.key
Linear(in_features=316, out_features=316, bias=True)
Model layer:
transformer.bert.encoder.layer.4.attention.self.value
Linear(in_features=316, out_features=316, bias=True)
Model layer:
transformer.bert.encoder.layer.4.attention.self.dropout
Dropout(p=0.1, inplace=False)
Model layer:
transformer.bert.encoder.layer.4.attention.output
BertSelfOutput(
  (dense): Linear(in_features=316, out_features=316, bias=True)
  (LayerNorm): LayerNorm((316,), eps=1e-12, elementwise_affine=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
Model layer:
transformer.bert.encoder.layer.4.attention.output.dense
Linear(in_features=316, out_features=316, bias=True)
Model layer:
transformer.bert.encoder.layer.4.attention.output.LayerNorm
LayerNorm((316,), eps=1e-12, elementwise_affine=True)
Model layer:
transformer.bert.encoder.layer.4.attention.output.dropout
Dropout(p=0.3, inplace=False)
Model layer:
transformer.bert.encoder.layer.4.intermediate
BertIntermediate(
  (dense): Linear(in_features=316, out_features=3072, bias=True)
  (intermediate_act_fn): GELUActivation()
)
Model layer:
transformer.bert.encoder.layer.4.intermediate.dense
Linear(in_features=316, out_features=3072, bias=True)
Model layer:
transformer.bert.encoder.layer.4.intermediate.intermediate_act_fn
GELUActivation()
Model layer:
transformer.bert.encoder.layer.4.output
BertOutput(
  (dense): Linear(in_features=3072, out_features=316, bias=True)
  (LayerNorm): LayerNorm((316,), eps=1e-12, elementwise_affine=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
Model layer:
transformer.bert.encoder.layer.4.output.dense
Linear(in_features=3072, out_features=316, bias=True)
Model layer:
transformer.bert.encoder.layer.4.output.LayerNorm
LayerNorm((316,), eps=1e-12, elementwise_affine=True)
Model layer:
transformer.bert.encoder.layer.4.output.dropout
Dropout(p=0.3, inplace=False)
Model layer:
transformer.bert.encoder.layer.5
BertLayer(
  (attention): BertAttention(
    (self): BertSelfAttention(
      (query): Linear(in_features=316, out_features=316, bias=True)
      (key): Linear(in_features=316, out_features=316, bias=True)
      (value): Linear(in_features=316, out_features=316, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (output): BertSelfOutput(
      (dense): Linear(in_features=316, out_features=316, bias=True)
      (LayerNorm): LayerNorm((316,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.3, inplace=False)
    )
  )
  (intermediate): BertIntermediate(
    (dense): Linear(in_features=316, out_features=3072, bias=True)
    (intermediate_act_fn): GELUActivation()
  )
  (output): BertOutput(
    (dense): Linear(in_features=3072, out_features=316, bias=True)
    (LayerNorm): LayerNorm((316,), eps=1e-12, elementwise_affine=True)
    (dropout): Dropout(p=0.3, inplace=False)
  )
)
Model layer:
transformer.bert.encoder.layer.5.attention
BertAttention(
  (self): BertSelfAttention(
    (query): Linear(in_features=316, out_features=316, bias=True)
    (key): Linear(in_features=316, out_features=316, bias=True)
    (value): Linear(in_features=316, out_features=316, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (output): BertSelfOutput(
    (dense): Linear(in_features=316, out_features=316, bias=True)
    (LayerNorm): LayerNorm((316,), eps=1e-12, elementwise_affine=True)
    (dropout): Dropout(p=0.3, inplace=False)
  )
)
Model layer:
transformer.bert.encoder.layer.5.attention.self
BertSelfAttention(
  (query): Linear(in_features=316, out_features=316, bias=True)
  (key): Linear(in_features=316, out_features=316, bias=True)
  (value): Linear(in_features=316, out_features=316, bias=True)
  (dropout): Dropout(p=0.1, inplace=False)
)
Model layer:
transformer.bert.encoder.layer.5.attention.self.query
Linear(in_features=316, out_features=316, bias=True)
Model layer:
transformer.bert.encoder.layer.5.attention.self.key
Linear(in_features=316, out_features=316, bias=True)
Model layer:
transformer.bert.encoder.layer.5.attention.self.value
Linear(in_features=316, out_features=316, bias=True)
Model layer:
transformer.bert.encoder.layer.5.attention.self.dropout
Dropout(p=0.1, inplace=False)
Model layer:
transformer.bert.encoder.layer.5.attention.output
BertSelfOutput(
  (dense): Linear(in_features=316, out_features=316, bias=True)
  (LayerNorm): LayerNorm((316,), eps=1e-12, elementwise_affine=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
Model layer:
transformer.bert.encoder.layer.5.attention.output.dense
Linear(in_features=316, out_features=316, bias=True)
Model layer:
transformer.bert.encoder.layer.5.attention.output.LayerNorm
LayerNorm((316,), eps=1e-12, elementwise_affine=True)
Model layer:
transformer.bert.encoder.layer.5.attention.output.dropout
Dropout(p=0.3, inplace=False)
Model layer:
transformer.bert.encoder.layer.5.intermediate
BertIntermediate(
  (dense): Linear(in_features=316, out_features=3072, bias=True)
  (intermediate_act_fn): GELUActivation()
)
Model layer:
transformer.bert.encoder.layer.5.intermediate.dense
Linear(in_features=316, out_features=3072, bias=True)
Model layer:
transformer.bert.encoder.layer.5.intermediate.intermediate_act_fn
GELUActivation()
Model layer:
transformer.bert.encoder.layer.5.output
BertOutput(
  (dense): Linear(in_features=3072, out_features=316, bias=True)
  (LayerNorm): LayerNorm((316,), eps=1e-12, elementwise_affine=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
Model layer:
transformer.bert.encoder.layer.5.output.dense
Linear(in_features=3072, out_features=316, bias=True)
Model layer:
transformer.bert.encoder.layer.5.output.LayerNorm
LayerNorm((316,), eps=1e-12, elementwise_affine=True)
Model layer:
transformer.bert.encoder.layer.5.output.dropout
Dropout(p=0.3, inplace=False)
Model layer:
transformer.bert.encoder.layer.6
BertLayer(
  (attention): BertAttention(
    (self): BertSelfAttention(
      (query): Linear(in_features=316, out_features=316, bias=True)
      (key): Linear(in_features=316, out_features=316, bias=True)
      (value): Linear(in_features=316, out_features=316, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (output): BertSelfOutput(
      (dense): Linear(in_features=316, out_features=316, bias=True)
      (LayerNorm): LayerNorm((316,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.3, inplace=False)
    )
  )
  (intermediate): BertIntermediate(
    (dense): Linear(in_features=316, out_features=3072, bias=True)
    (intermediate_act_fn): GELUActivation()
  )
  (output): BertOutput(
    (dense): Linear(in_features=3072, out_features=316, bias=True)
    (LayerNorm): LayerNorm((316,), eps=1e-12, elementwise_affine=True)
    (dropout): Dropout(p=0.3, inplace=False)
  )
)
Model layer:
transformer.bert.encoder.layer.6.attention
BertAttention(
  (self): BertSelfAttention(
    (query): Linear(in_features=316, out_features=316, bias=True)
    (key): Linear(in_features=316, out_features=316, bias=True)
    (value): Linear(in_features=316, out_features=316, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (output): BertSelfOutput(
    (dense): Linear(in_features=316, out_features=316, bias=True)
    (LayerNorm): LayerNorm((316,), eps=1e-12, elementwise_affine=True)
    (dropout): Dropout(p=0.3, inplace=False)
  )
)
Model layer:
transformer.bert.encoder.layer.6.attention.self
BertSelfAttention(
  (query): Linear(in_features=316, out_features=316, bias=True)
  (key): Linear(in_features=316, out_features=316, bias=True)
  (value): Linear(in_features=316, out_features=316, bias=True)
  (dropout): Dropout(p=0.1, inplace=False)
)
Model layer:
transformer.bert.encoder.layer.6.attention.self.query
Linear(in_features=316, out_features=316, bias=True)
Model layer:
transformer.bert.encoder.layer.6.attention.self.key
Linear(in_features=316, out_features=316, bias=True)
Model layer:
transformer.bert.encoder.layer.6.attention.self.value
Linear(in_features=316, out_features=316, bias=True)
Model layer:
transformer.bert.encoder.layer.6.attention.self.dropout
Dropout(p=0.1, inplace=False)
Model layer:
transformer.bert.encoder.layer.6.attention.output
BertSelfOutput(
  (dense): Linear(in_features=316, out_features=316, bias=True)
  (LayerNorm): LayerNorm((316,), eps=1e-12, elementwise_affine=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
Model layer:
transformer.bert.encoder.layer.6.attention.output.dense
Linear(in_features=316, out_features=316, bias=True)
Model layer:
transformer.bert.encoder.layer.6.attention.output.LayerNorm
LayerNorm((316,), eps=1e-12, elementwise_affine=True)
Model layer:
transformer.bert.encoder.layer.6.attention.output.dropout
Dropout(p=0.3, inplace=False)
Model layer:
transformer.bert.encoder.layer.6.intermediate
BertIntermediate(
  (dense): Linear(in_features=316, out_features=3072, bias=True)
  (intermediate_act_fn): GELUActivation()
)
Model layer:
transformer.bert.encoder.layer.6.intermediate.dense
Linear(in_features=316, out_features=3072, bias=True)
Model layer:
transformer.bert.encoder.layer.6.intermediate.intermediate_act_fn
GELUActivation()
Model layer:
transformer.bert.encoder.layer.6.output
BertOutput(
  (dense): Linear(in_features=3072, out_features=316, bias=True)
  (LayerNorm): LayerNorm((316,), eps=1e-12, elementwise_affine=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
Model layer:
transformer.bert.encoder.layer.6.output.dense
Linear(in_features=3072, out_features=316, bias=True)
Model layer:
transformer.bert.encoder.layer.6.output.LayerNorm
LayerNorm((316,), eps=1e-12, elementwise_affine=True)
Model layer:
transformer.bert.encoder.layer.6.output.dropout
Dropout(p=0.3, inplace=False)
Model layer:
transformer.bert.encoder.layer.7
BertLayer(
  (attention): BertAttention(
    (self): BertSelfAttention(
      (query): Linear(in_features=316, out_features=316, bias=True)
      (key): Linear(in_features=316, out_features=316, bias=True)
      (value): Linear(in_features=316, out_features=316, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (output): BertSelfOutput(
      (dense): Linear(in_features=316, out_features=316, bias=True)
      (LayerNorm): LayerNorm((316,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.3, inplace=False)
    )
  )
  (intermediate): BertIntermediate(
    (dense): Linear(in_features=316, out_features=3072, bias=True)
    (intermediate_act_fn): GELUActivation()
  )
  (output): BertOutput(
    (dense): Linear(in_features=3072, out_features=316, bias=True)
    (LayerNorm): LayerNorm((316,), eps=1e-12, elementwise_affine=True)
    (dropout): Dropout(p=0.3, inplace=False)
  )
)
Model layer:
transformer.bert.encoder.layer.7.attention
BertAttention(
  (self): BertSelfAttention(
    (query): Linear(in_features=316, out_features=316, bias=True)
    (key): Linear(in_features=316, out_features=316, bias=True)
    (value): Linear(in_features=316, out_features=316, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (output): BertSelfOutput(
    (dense): Linear(in_features=316, out_features=316, bias=True)
    (LayerNorm): LayerNorm((316,), eps=1e-12, elementwise_affine=True)
    (dropout): Dropout(p=0.3, inplace=False)
  )
)
Model layer:
transformer.bert.encoder.layer.7.attention.self
BertSelfAttention(
  (query): Linear(in_features=316, out_features=316, bias=True)
  (key): Linear(in_features=316, out_features=316, bias=True)
  (value): Linear(in_features=316, out_features=316, bias=True)
  (dropout): Dropout(p=0.1, inplace=False)
)
Model layer:
transformer.bert.encoder.layer.7.attention.self.query
Linear(in_features=316, out_features=316, bias=True)
Model layer:
transformer.bert.encoder.layer.7.attention.self.key
Linear(in_features=316, out_features=316, bias=True)
Model layer:
transformer.bert.encoder.layer.7.attention.self.value
Linear(in_features=316, out_features=316, bias=True)
Model layer:
transformer.bert.encoder.layer.7.attention.self.dropout
Dropout(p=0.1, inplace=False)
Model layer:
transformer.bert.encoder.layer.7.attention.output
BertSelfOutput(
  (dense): Linear(in_features=316, out_features=316, bias=True)
  (LayerNorm): LayerNorm((316,), eps=1e-12, elementwise_affine=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
Model layer:
transformer.bert.encoder.layer.7.attention.output.dense
Linear(in_features=316, out_features=316, bias=True)
Model layer:
transformer.bert.encoder.layer.7.attention.output.LayerNorm
LayerNorm((316,), eps=1e-12, elementwise_affine=True)
Model layer:
transformer.bert.encoder.layer.7.attention.output.dropout
Dropout(p=0.3, inplace=False)
Model layer:
transformer.bert.encoder.layer.7.intermediate
BertIntermediate(
  (dense): Linear(in_features=316, out_features=3072, bias=True)
  (intermediate_act_fn): GELUActivation()
)
Model layer:
transformer.bert.encoder.layer.7.intermediate.dense
Linear(in_features=316, out_features=3072, bias=True)
Model layer:
transformer.bert.encoder.layer.7.intermediate.intermediate_act_fn
GELUActivation()
Model layer:
transformer.bert.encoder.layer.7.output
BertOutput(
  (dense): Linear(in_features=3072, out_features=316, bias=True)
  (LayerNorm): LayerNorm((316,), eps=1e-12, elementwise_affine=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
Model layer:
transformer.bert.encoder.layer.7.output.dense
Linear(in_features=3072, out_features=316, bias=True)
Model layer:
transformer.bert.encoder.layer.7.output.LayerNorm
LayerNorm((316,), eps=1e-12, elementwise_affine=True)
Model layer:
transformer.bert.encoder.layer.7.output.dropout
Dropout(p=0.3, inplace=False)
Model layer:
transformer.bert.pooler
BertPooler(
  (dense): Linear(in_features=316, out_features=316, bias=True)
  (activation): Tanh()
)
Model layer:
transformer.bert.pooler.dense
Linear(in_features=316, out_features=316, bias=True)
Model layer:
transformer.bert.pooler.activation
Tanh()
Model layer:
transformer.cls_embedding
Sequential(
  (0): Linear(in_features=316, out_features=316, bias=True)
  (1): LeakyReLU(negative_slope=0.01)
)
Model layer:
transformer.cls_embedding.0
Linear(in_features=316, out_features=316, bias=True)
Model layer:
transformer.cls_embedding.1
LeakyReLU(negative_slope=0.01)
Model layer:
high_spatial_attention
Attention(
  (qkv): Linear(in_features=92, out_features=276, bias=False)
  (attn_drop): Dropout(p=0.0, inplace=False)
)
Model layer:
high_spatial_attention.qkv
Linear(in_features=92, out_features=276, bias=False)
Model layer:
high_spatial_attention.attn_drop
Dropout(p=0.0, inplace=False)
Model layer:
low_spatial_attention
Attention(
  (qkv): Linear(in_features=92, out_features=276, bias=False)
  (attn_drop): Dropout(p=0.0, inplace=False)
)
Model layer:
low_spatial_attention.qkv
Linear(in_features=92, out_features=276, bias=False)
Model layer:
low_spatial_attention.attn_drop
Dropout(p=0.0, inplace=False)
Model layer:
ultralow_spatial_attention
Attention(
  (qkv): Linear(in_features=92, out_features=276, bias=False)
  (attn_drop): Dropout(p=0.0, inplace=False)
)
Model layer:
ultralow_spatial_attention.qkv
Linear(in_features=92, out_features=276, bias=False)
Model layer:
ultralow_spatial_attention.attn_drop
Dropout(p=0.0, inplace=False)
Model layer:
regression_head
Classifier(
  (linear): Linear(in_features=316, out_features=1, bias=True)
  (dropout): Dropout(p=0.6, inplace=False)
  (norm): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Model layer:
regression_head.linear
Linear(in_features=316, out_features=1, bias=True)
Model layer:
regression_head.dropout
Dropout(p=0.6, inplace=False)
Model layer:
regression_head.norm
BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Model summary:
=====================================================================================
Layer (type:depth-idx)                                       Param #
=====================================================================================
Transformer_Finetune_Three_Channels                          --
‚îú‚îÄTransformer_Block: 1-1                                     --
‚îÇ    ‚îî‚îÄBertModel: 2-1                                        --
‚îÇ    ‚îÇ    ‚îî‚îÄBertEmbeddings: 3-1                              30,968
‚îÇ    ‚îÇ    ‚îî‚îÄBertEncoder: 3-2                                 18,774,752
‚îÇ    ‚îÇ    ‚îî‚îÄBertPooler: 3-3                                  100,172
‚îÇ    ‚îî‚îÄSequential: 2-2                                       --
‚îÇ    ‚îÇ    ‚îî‚îÄLinear: 3-4                                      100,172
‚îÇ    ‚îÇ    ‚îî‚îÄLeakyReLU: 3-5                                   --
‚îú‚îÄAttention: 1-2                                             --
‚îÇ    ‚îî‚îÄLinear: 2-3                                           25,392
‚îÇ    ‚îî‚îÄDropout: 2-4                                          --
‚îú‚îÄAttention: 1-3                                             --
‚îÇ    ‚îî‚îÄLinear: 2-5                                           25,392
‚îÇ    ‚îî‚îÄDropout: 2-6                                          --
‚îú‚îÄAttention: 1-4                                             --
‚îÇ    ‚îî‚îÄLinear: 2-7                                           25,392
‚îÇ    ‚îî‚îÄDropout: 2-8                                          --
‚îú‚îÄClassifier: 1-5                                            --
‚îÇ    ‚îî‚îÄLinear: 2-9                                           317
‚îÇ    ‚îî‚îÄDropout: 2-10                                         --
‚îÇ    ‚îî‚îÄBatchNorm1d: 2-11                                     2
=====================================================================================
Total params: 19,082,559
Trainable params: 19,082,559
Non-trainable params: 0
=====================================================================================
Model graph:
{'binary_classification': tensor([[-1.5076],
        [ 0.1258],
        [ 0.0000],
        [ 0.0000],
        [ 0.0000],
        [ 0.5493],
        [ 0.3661],
        [-0.0214]], grad_fn=<DivBackward0>), 'high_spatial_attention': tensor([[[[0.0030, 0.0024, 0.0038,  ..., 0.0030, 0.0033, 0.0037],
          [0.0040, 0.0037, 0.0029,  ..., 0.0028, 0.0037, 0.0036],
          [0.0040, 0.0039, 0.0032,  ..., 0.0027, 0.0029, 0.0035],
          ...,
          [0.0045, 0.0010, 0.0019,  ..., 0.0017, 0.0035, 0.0048],
          [0.0030, 0.0037, 0.0038,  ..., 0.0037, 0.0039, 0.0018],
          [0.0022, 0.0037, 0.0046,  ..., 0.0037, 0.0020, 0.0022]],

         [[0.0041, 0.0028, 0.0041,  ..., 0.0021, 0.0028, 0.0021],
          [0.0032, 0.0046, 0.0034,  ..., 0.0036, 0.0017, 0.0024],
          [0.0031, 0.0025, 0.0022,  ..., 0.0038, 0.0034, 0.0032],
          ...,
          [0.0013, 0.0014, 0.0051,  ..., 0.0026, 0.0040, 0.0021],
          [0.0037, 0.0054, 0.0024,  ..., 0.0038, 0.0014, 0.0033],
          [0.0013, 0.0018, 0.0037,  ..., 0.0025, 0.0057, 0.0030]],

         [[0.0027, 0.0023, 0.0039,  ..., 0.0015, 0.0032, 0.0026],
          [0.0031, 0.0024, 0.0029,  ..., 0.0021, 0.0023, 0.0020],
          [0.0024, 0.0048, 0.0051,  ..., 0.0023, 0.0074, 0.0031],
          ...,
          [0.0035, 0.0022, 0.0012,  ..., 0.0039, 0.0027, 0.0038],
          [0.0028, 0.0024, 0.0035,  ..., 0.0024, 0.0027, 0.0028],
          [0.0029, 0.0019, 0.0036,  ..., 0.0022, 0.0017, 0.0020]],

         [[0.0025, 0.0018, 0.0050,  ..., 0.0022, 0.0033, 0.0042],
          [0.0023, 0.0025, 0.0042,  ..., 0.0018, 0.0023, 0.0013],
          [0.0036, 0.0028, 0.0037,  ..., 0.0043, 0.0024, 0.0026],
          ...,
          [0.0024, 0.0043, 0.0029,  ..., 0.0019, 0.0037, 0.0018],
          [0.0039, 0.0026, 0.0019,  ..., 0.0027, 0.0040, 0.0023],
          [0.0026, 0.0024, 0.0019,  ..., 0.0034, 0.0032, 0.0020]]],


        [[[0.0026, 0.0030, 0.0029,  ..., 0.0024, 0.0037, 0.0021],
          [0.0018, 0.0029, 0.0019,  ..., 0.0013, 0.0024, 0.0024],
          [0.0022, 0.0034, 0.0023,  ..., 0.0030, 0.0025, 0.0025],
          ...,
          [0.0037, 0.0037, 0.0036,  ..., 0.0039, 0.0025, 0.0016],
          [0.0033, 0.0039, 0.0019,  ..., 0.0028, 0.0026, 0.0032],
          [0.0024, 0.0035, 0.0028,  ..., 0.0029, 0.0039, 0.0030]],

         [[0.0065, 0.0039, 0.0055,  ..., 0.0021, 0.0023, 0.0020],
          [0.0032, 0.0035, 0.0024,  ..., 0.0035, 0.0028, 0.0014],
          [0.0042, 0.0036, 0.0052,  ..., 0.0026, 0.0025, 0.0064],
          ...,
          [0.0021, 0.0034, 0.0045,  ..., 0.0026, 0.0024, 0.0026],
          [0.0036, 0.0030, 0.0040,  ..., 0.0032, 0.0037, 0.0064],
          [0.0028, 0.0024, 0.0048,  ..., 0.0034, 0.0037, 0.0072]],

         [[0.0025, 0.0046, 0.0047,  ..., 0.0026, 0.0029, 0.0034],
          [0.0015, 0.0050, 0.0033,  ..., 0.0020, 0.0025, 0.0043],
          [0.0019, 0.0019, 0.0036,  ..., 0.0036, 0.0035, 0.0031],
          ...,
          [0.0023, 0.0018, 0.0027,  ..., 0.0027, 0.0036, 0.0038],
          [0.0065, 0.0044, 0.0036,  ..., 0.0036, 0.0027, 0.0016],
          [0.0030, 0.0022, 0.0031,  ..., 0.0027, 0.0034, 0.0031]],

         [[0.0032, 0.0031, 0.0031,  ..., 0.0039, 0.0046, 0.0061],
          [0.0023, 0.0039, 0.0035,  ..., 0.0043, 0.0034, 0.0035],
          [0.0035, 0.0023, 0.0031,  ..., 0.0035, 0.0026, 0.0032],
          ...,
          [0.0040, 0.0029, 0.0024,  ..., 0.0020, 0.0026, 0.0023],
          [0.0039, 0.0022, 0.0021,  ..., 0.0018, 0.0024, 0.0023],
          [0.0027, 0.0042, 0.0051,  ..., 0.0026, 0.0023, 0.0034]]],


        [[[0.0016, 0.0070, 0.0034,  ..., 0.0026, 0.0028, 0.0026],
          [0.0048, 0.0017, 0.0014,  ..., 0.0030, 0.0032, 0.0057],
          [0.0037, 0.0032, 0.0022,  ..., 0.0040, 0.0023, 0.0028],
          ...,
          [0.0027, 0.0041, 0.0022,  ..., 0.0022, 0.0028, 0.0032],
          [0.0036, 0.0030, 0.0021,  ..., 0.0037, 0.0031, 0.0034],
          [0.0019, 0.0052, 0.0033,  ..., 0.0022, 0.0043, 0.0021]],

         [[0.0030, 0.0038, 0.0038,  ..., 0.0034, 0.0022, 0.0041],
          [0.0007, 0.0022, 0.0022,  ..., 0.0023, 0.0026, 0.0021],
          [0.0045, 0.0025, 0.0027,  ..., 0.0023, 0.0025, 0.0030],
          ...,
          [0.0039, 0.0027, 0.0035,  ..., 0.0027, 0.0027, 0.0020],
          [0.0023, 0.0031, 0.0040,  ..., 0.0036, 0.0032, 0.0019],
          [0.0033, 0.0037, 0.0024,  ..., 0.0028, 0.0029, 0.0033]],

         [[0.0038, 0.0032, 0.0039,  ..., 0.0031, 0.0025, 0.0032],
          [0.0026, 0.0052, 0.0045,  ..., 0.0023, 0.0043, 0.0033],
          [0.0041, 0.0021, 0.0032,  ..., 0.0023, 0.0023, 0.0037],
          ...,
          [0.0036, 0.0040, 0.0026,  ..., 0.0029, 0.0035, 0.0023],
          [0.0024, 0.0023, 0.0028,  ..., 0.0030, 0.0029, 0.0021],
          [0.0022, 0.0033, 0.0054,  ..., 0.0031, 0.0021, 0.0021]],

         [[0.0035, 0.0019, 0.0024,  ..., 0.0020, 0.0016, 0.0020],
          [0.0028, 0.0044, 0.0022,  ..., 0.0031, 0.0051, 0.0033],
          [0.0038, 0.0046, 0.0035,  ..., 0.0022, 0.0026, 0.0029],
          ...,
          [0.0035, 0.0043, 0.0046,  ..., 0.0038, 0.0021, 0.0020],
          [0.0019, 0.0031, 0.0037,  ..., 0.0032, 0.0024, 0.0030],
          [0.0061, 0.0018, 0.0018,  ..., 0.0011, 0.0016, 0.0015]]],


        ...,


        [[[0.0029, 0.0042, 0.0027,  ..., 0.0039, 0.0050, 0.0023],
          [0.0017, 0.0042, 0.0025,  ..., 0.0028, 0.0026, 0.0034],
          [0.0058, 0.0031, 0.0038,  ..., 0.0048, 0.0028, 0.0025],
          ...,
          [0.0022, 0.0032, 0.0028,  ..., 0.0034, 0.0024, 0.0026],
          [0.0030, 0.0023, 0.0052,  ..., 0.0027, 0.0036, 0.0027],
          [0.0025, 0.0046, 0.0051,  ..., 0.0037, 0.0039, 0.0018]],

         [[0.0030, 0.0035, 0.0021,  ..., 0.0025, 0.0028, 0.0058],
          [0.0037, 0.0029, 0.0032,  ..., 0.0028, 0.0043, 0.0031],
          [0.0024, 0.0031, 0.0027,  ..., 0.0038, 0.0018, 0.0032],
          ...,
          [0.0035, 0.0025, 0.0036,  ..., 0.0025, 0.0022, 0.0019],
          [0.0018, 0.0029, 0.0017,  ..., 0.0014, 0.0028, 0.0031],
          [0.0019, 0.0034, 0.0017,  ..., 0.0019, 0.0035, 0.0040]],

         [[0.0025, 0.0021, 0.0027,  ..., 0.0019, 0.0039, 0.0039],
          [0.0044, 0.0018, 0.0037,  ..., 0.0020, 0.0034, 0.0044],
          [0.0025, 0.0039, 0.0026,  ..., 0.0038, 0.0021, 0.0023],
          ...,
          [0.0032, 0.0025, 0.0023,  ..., 0.0030, 0.0025, 0.0017],
          [0.0045, 0.0040, 0.0015,  ..., 0.0020, 0.0026, 0.0019],
          [0.0042, 0.0031, 0.0033,  ..., 0.0032, 0.0019, 0.0036]],

         [[0.0031, 0.0037, 0.0041,  ..., 0.0055, 0.0024, 0.0050],
          [0.0026, 0.0035, 0.0028,  ..., 0.0045, 0.0030, 0.0049],
          [0.0039, 0.0030, 0.0036,  ..., 0.0030, 0.0026, 0.0021],
          ...,
          [0.0024, 0.0033, 0.0035,  ..., 0.0045, 0.0019, 0.0022],
          [0.0032, 0.0017, 0.0036,  ..., 0.0019, 0.0027, 0.0041],
          [0.0030, 0.0027, 0.0018,  ..., 0.0030, 0.0028, 0.0024]]],


        [[[0.0037, 0.0036, 0.0023,  ..., 0.0037, 0.0036, 0.0035],
          [0.0033, 0.0019, 0.0025,  ..., 0.0032, 0.0019, 0.0033],
          [0.0037, 0.0026, 0.0027,  ..., 0.0020, 0.0040, 0.0020],
          ...,
          [0.0018, 0.0044, 0.0033,  ..., 0.0026, 0.0017, 0.0026],
          [0.0027, 0.0025, 0.0065,  ..., 0.0041, 0.0026, 0.0023],
          [0.0042, 0.0032, 0.0031,  ..., 0.0037, 0.0025, 0.0030]],

         [[0.0030, 0.0038, 0.0033,  ..., 0.0025, 0.0021, 0.0032],
          [0.0032, 0.0030, 0.0037,  ..., 0.0031, 0.0035, 0.0041],
          [0.0028, 0.0029, 0.0032,  ..., 0.0035, 0.0051, 0.0044],
          ...,
          [0.0029, 0.0027, 0.0023,  ..., 0.0020, 0.0027, 0.0025],
          [0.0038, 0.0029, 0.0049,  ..., 0.0038, 0.0031, 0.0015],
          [0.0047, 0.0025, 0.0023,  ..., 0.0023, 0.0029, 0.0039]],

         [[0.0030, 0.0036, 0.0065,  ..., 0.0024, 0.0028, 0.0025],
          [0.0054, 0.0022, 0.0021,  ..., 0.0032, 0.0040, 0.0035],
          [0.0031, 0.0038, 0.0034,  ..., 0.0025, 0.0028, 0.0034],
          ...,
          [0.0028, 0.0029, 0.0046,  ..., 0.0044, 0.0028, 0.0024],
          [0.0023, 0.0031, 0.0030,  ..., 0.0033, 0.0024, 0.0051],
          [0.0029, 0.0038, 0.0031,  ..., 0.0030, 0.0032, 0.0038]],

         [[0.0043, 0.0021, 0.0028,  ..., 0.0024, 0.0029, 0.0026],
          [0.0038, 0.0042, 0.0028,  ..., 0.0044, 0.0017, 0.0018],
          [0.0031, 0.0043, 0.0039,  ..., 0.0021, 0.0036, 0.0021],
          ...,
          [0.0030, 0.0016, 0.0021,  ..., 0.0030, 0.0055, 0.0026],
          [0.0031, 0.0032, 0.0023,  ..., 0.0039, 0.0021, 0.0042],
          [0.0025, 0.0042, 0.0030,  ..., 0.0026, 0.0012, 0.0034]]],


        [[[0.0020, 0.0082, 0.0043,  ..., 0.0016, 0.0044, 0.0030],
          [0.0020, 0.0058, 0.0027,  ..., 0.0016, 0.0012, 0.0062],
          [0.0019, 0.0020, 0.0025,  ..., 0.0042, 0.0016, 0.0035],
          ...,
          [0.0042, 0.0031, 0.0032,  ..., 0.0031, 0.0034, 0.0024],
          [0.0037, 0.0029, 0.0036,  ..., 0.0036, 0.0019, 0.0055],
          [0.0019, 0.0061, 0.0050,  ..., 0.0022, 0.0039, 0.0063]],

         [[0.0032, 0.0062, 0.0024,  ..., 0.0036, 0.0023, 0.0040],
          [0.0024, 0.0038, 0.0036,  ..., 0.0031, 0.0019, 0.0038],
          [0.0029, 0.0060, 0.0026,  ..., 0.0027, 0.0026, 0.0038],
          ...,
          [0.0028, 0.0045, 0.0029,  ..., 0.0057, 0.0078, 0.0023],
          [0.0046, 0.0015, 0.0031,  ..., 0.0035, 0.0081, 0.0029],
          [0.0046, 0.0034, 0.0047,  ..., 0.0038, 0.0032, 0.0014]],

         [[0.0032, 0.0031, 0.0019,  ..., 0.0014, 0.0018, 0.0024],
          [0.0017, 0.0047, 0.0027,  ..., 0.0028, 0.0032, 0.0030],
          [0.0032, 0.0030, 0.0043,  ..., 0.0028, 0.0029, 0.0021],
          ...,
          [0.0041, 0.0039, 0.0025,  ..., 0.0037, 0.0044, 0.0018],
          [0.0075, 0.0023, 0.0023,  ..., 0.0069, 0.0037, 0.0037],
          [0.0030, 0.0037, 0.0029,  ..., 0.0047, 0.0039, 0.0035]],

         [[0.0030, 0.0024, 0.0029,  ..., 0.0028, 0.0022, 0.0028],
          [0.0027, 0.0027, 0.0016,  ..., 0.0027, 0.0053, 0.0033],
          [0.0021, 0.0029, 0.0028,  ..., 0.0035, 0.0032, 0.0019],
          ...,
          [0.0019, 0.0046, 0.0030,  ..., 0.0027, 0.0050, 0.0032],
          [0.0035, 0.0044, 0.0020,  ..., 0.0021, 0.0033, 0.0037],
          [0.0028, 0.0022, 0.0050,  ..., 0.0037, 0.0028, 0.0021]]]],
       grad_fn=<SoftmaxBackward0>), 'low_spatial_attention': tensor([[[[0.0028, 0.0042, 0.0030,  ..., 0.0032, 0.0023, 0.0027],
          [0.0024, 0.0039, 0.0040,  ..., 0.0025, 0.0021, 0.0023],
          [0.0034, 0.0020, 0.0036,  ..., 0.0025, 0.0034, 0.0026],
          ...,
          [0.0026, 0.0030, 0.0019,  ..., 0.0034, 0.0033, 0.0031],
          [0.0019, 0.0032, 0.0059,  ..., 0.0020, 0.0021, 0.0021],
          [0.0028, 0.0024, 0.0029,  ..., 0.0024, 0.0038, 0.0031]],

         [[0.0024, 0.0024, 0.0025,  ..., 0.0049, 0.0031, 0.0038],
          [0.0031, 0.0037, 0.0038,  ..., 0.0022, 0.0024, 0.0028],
          [0.0027, 0.0019, 0.0025,  ..., 0.0043, 0.0037, 0.0034],
          ...,
          [0.0033, 0.0027, 0.0045,  ..., 0.0018, 0.0030, 0.0031],
          [0.0038, 0.0042, 0.0023,  ..., 0.0024, 0.0027, 0.0040],
          [0.0025, 0.0033, 0.0030,  ..., 0.0048, 0.0029, 0.0042]],

         [[0.0022, 0.0033, 0.0032,  ..., 0.0041, 0.0025, 0.0040],
          [0.0037, 0.0024, 0.0040,  ..., 0.0026, 0.0042, 0.0032],
          [0.0011, 0.0016, 0.0029,  ..., 0.0020, 0.0016, 0.0033],
          ...,
          [0.0068, 0.0028, 0.0033,  ..., 0.0094, 0.0034, 0.0023],
          [0.0025, 0.0033, 0.0029,  ..., 0.0026, 0.0034, 0.0029],
          [0.0026, 0.0032, 0.0029,  ..., 0.0051, 0.0025, 0.0049]],

         [[0.0027, 0.0033, 0.0015,  ..., 0.0034, 0.0019, 0.0032],
          [0.0020, 0.0050, 0.0023,  ..., 0.0027, 0.0028, 0.0030],
          [0.0019, 0.0056, 0.0014,  ..., 0.0046, 0.0031, 0.0021],
          ...,
          [0.0022, 0.0040, 0.0022,  ..., 0.0017, 0.0029, 0.0020],
          [0.0036, 0.0030, 0.0028,  ..., 0.0019, 0.0046, 0.0027],
          [0.0033, 0.0037, 0.0038,  ..., 0.0030, 0.0037, 0.0029]]],


        [[[0.0036, 0.0036, 0.0036,  ..., 0.0028, 0.0029, 0.0012],
          [0.0060, 0.0021, 0.0036,  ..., 0.0027, 0.0029, 0.0046],
          [0.0023, 0.0028, 0.0036,  ..., 0.0026, 0.0038, 0.0056],
          ...,
          [0.0024, 0.0032, 0.0032,  ..., 0.0038, 0.0021, 0.0042],
          [0.0020, 0.0022, 0.0027,  ..., 0.0031, 0.0024, 0.0027],
          [0.0017, 0.0027, 0.0022,  ..., 0.0035, 0.0025, 0.0027]],

         [[0.0022, 0.0055, 0.0027,  ..., 0.0026, 0.0024, 0.0025],
          [0.0033, 0.0028, 0.0045,  ..., 0.0026, 0.0047, 0.0028],
          [0.0024, 0.0035, 0.0030,  ..., 0.0029, 0.0039, 0.0025],
          ...,
          [0.0040, 0.0039, 0.0040,  ..., 0.0043, 0.0028, 0.0030],
          [0.0105, 0.0034, 0.0020,  ..., 0.0018, 0.0021, 0.0022],
          [0.0058, 0.0043, 0.0027,  ..., 0.0017, 0.0031, 0.0069]],

         [[0.0015, 0.0032, 0.0030,  ..., 0.0050, 0.0024, 0.0033],
          [0.0029, 0.0029, 0.0029,  ..., 0.0038, 0.0048, 0.0031],
          [0.0031, 0.0040, 0.0045,  ..., 0.0024, 0.0046, 0.0037],
          ...,
          [0.0030, 0.0061, 0.0026,  ..., 0.0021, 0.0040, 0.0026],
          [0.0017, 0.0023, 0.0044,  ..., 0.0029, 0.0042, 0.0045],
          [0.0029, 0.0030, 0.0020,  ..., 0.0027, 0.0023, 0.0033]],

         [[0.0019, 0.0029, 0.0061,  ..., 0.0027, 0.0041, 0.0016],
          [0.0036, 0.0046, 0.0034,  ..., 0.0042, 0.0021, 0.0048],
          [0.0032, 0.0026, 0.0022,  ..., 0.0026, 0.0019, 0.0031],
          ...,
          [0.0028, 0.0035, 0.0019,  ..., 0.0024, 0.0036, 0.0040],
          [0.0046, 0.0032, 0.0050,  ..., 0.0029, 0.0032, 0.0036],
          [0.0063, 0.0014, 0.0031,  ..., 0.0022, 0.0030, 0.0025]]],


        [[[0.0046, 0.0019, 0.0026,  ..., 0.0025, 0.0039, 0.0021],
          [0.0028, 0.0042, 0.0029,  ..., 0.0031, 0.0032, 0.0024],
          [0.0035, 0.0029, 0.0033,  ..., 0.0028, 0.0038, 0.0020],
          ...,
          [0.0010, 0.0059, 0.0039,  ..., 0.0030, 0.0031, 0.0030],
          [0.0023, 0.0027, 0.0046,  ..., 0.0042, 0.0036, 0.0033],
          [0.0029, 0.0016, 0.0028,  ..., 0.0042, 0.0048, 0.0040]],

         [[0.0051, 0.0026, 0.0036,  ..., 0.0028, 0.0035, 0.0018],
          [0.0042, 0.0031, 0.0037,  ..., 0.0020, 0.0033, 0.0027],
          [0.0026, 0.0030, 0.0020,  ..., 0.0029, 0.0097, 0.0029],
          ...,
          [0.0038, 0.0020, 0.0049,  ..., 0.0018, 0.0022, 0.0039],
          [0.0012, 0.0080, 0.0016,  ..., 0.0050, 0.0015, 0.0044],
          [0.0028, 0.0038, 0.0020,  ..., 0.0023, 0.0043, 0.0039]],

         [[0.0029, 0.0031, 0.0030,  ..., 0.0037, 0.0032, 0.0021],
          [0.0023, 0.0038, 0.0034,  ..., 0.0023, 0.0030, 0.0029],
          [0.0048, 0.0036, 0.0027,  ..., 0.0020, 0.0029, 0.0032],
          ...,
          [0.0039, 0.0031, 0.0024,  ..., 0.0018, 0.0029, 0.0051],
          [0.0019, 0.0029, 0.0042,  ..., 0.0025, 0.0033, 0.0011],
          [0.0027, 0.0034, 0.0026,  ..., 0.0030, 0.0028, 0.0028]],

         [[0.0039, 0.0034, 0.0022,  ..., 0.0036, 0.0032, 0.0028],
          [0.0026, 0.0027, 0.0030,  ..., 0.0030, 0.0031, 0.0029],
          [0.0028, 0.0030, 0.0039,  ..., 0.0018, 0.0034, 0.0026],
          ...,
          [0.0014, 0.0027, 0.0030,  ..., 0.0024, 0.0044, 0.0025],
          [0.0051, 0.0017, 0.0042,  ..., 0.0021, 0.0022, 0.0026],
          [0.0034, 0.0026, 0.0029,  ..., 0.0034, 0.0030, 0.0030]]],


        ...,


        [[[0.0028, 0.0026, 0.0022,  ..., 0.0042, 0.0036, 0.0031],
          [0.0031, 0.0024, 0.0020,  ..., 0.0023, 0.0050, 0.0043],
          [0.0034, 0.0022, 0.0033,  ..., 0.0032, 0.0020, 0.0024],
          ...,
          [0.0049, 0.0024, 0.0021,  ..., 0.0025, 0.0048, 0.0034],
          [0.0053, 0.0025, 0.0027,  ..., 0.0032, 0.0038, 0.0051],
          [0.0044, 0.0047, 0.0019,  ..., 0.0048, 0.0026, 0.0032]],

         [[0.0053, 0.0047, 0.0034,  ..., 0.0042, 0.0034, 0.0060],
          [0.0061, 0.0037, 0.0043,  ..., 0.0032, 0.0029, 0.0043],
          [0.0027, 0.0023, 0.0063,  ..., 0.0047, 0.0027, 0.0012],
          ...,
          [0.0021, 0.0017, 0.0040,  ..., 0.0043, 0.0034, 0.0032],
          [0.0023, 0.0027, 0.0032,  ..., 0.0025, 0.0035, 0.0034],
          [0.0020, 0.0027, 0.0042,  ..., 0.0029, 0.0023, 0.0025]],

         [[0.0013, 0.0033, 0.0042,  ..., 0.0024, 0.0021, 0.0029],
          [0.0029, 0.0014, 0.0041,  ..., 0.0035, 0.0017, 0.0016],
          [0.0029, 0.0042, 0.0043,  ..., 0.0019, 0.0039, 0.0027],
          ...,
          [0.0037, 0.0026, 0.0027,  ..., 0.0022, 0.0038, 0.0022],
          [0.0022, 0.0028, 0.0047,  ..., 0.0038, 0.0039, 0.0028],
          [0.0019, 0.0028, 0.0025,  ..., 0.0038, 0.0020, 0.0026]],

         [[0.0030, 0.0033, 0.0032,  ..., 0.0029, 0.0031, 0.0041],
          [0.0038, 0.0016, 0.0032,  ..., 0.0028, 0.0022, 0.0048],
          [0.0039, 0.0026, 0.0029,  ..., 0.0029, 0.0037, 0.0028],
          ...,
          [0.0025, 0.0043, 0.0033,  ..., 0.0035, 0.0027, 0.0025],
          [0.0025, 0.0025, 0.0019,  ..., 0.0031, 0.0031, 0.0025],
          [0.0022, 0.0034, 0.0023,  ..., 0.0034, 0.0042, 0.0024]]],


        [[[0.0024, 0.0022, 0.0016,  ..., 0.0036, 0.0034, 0.0024],
          [0.0015, 0.0021, 0.0041,  ..., 0.0025, 0.0026, 0.0035],
          [0.0036, 0.0031, 0.0041,  ..., 0.0038, 0.0032, 0.0026],
          ...,
          [0.0038, 0.0042, 0.0021,  ..., 0.0046, 0.0027, 0.0028],
          [0.0051, 0.0034, 0.0018,  ..., 0.0033, 0.0031, 0.0040],
          [0.0016, 0.0027, 0.0026,  ..., 0.0021, 0.0026, 0.0038]],

         [[0.0054, 0.0023, 0.0036,  ..., 0.0022, 0.0037, 0.0027],
          [0.0023, 0.0034, 0.0020,  ..., 0.0031, 0.0047, 0.0028],
          [0.0021, 0.0049, 0.0040,  ..., 0.0027, 0.0026, 0.0034],
          ...,
          [0.0025, 0.0058, 0.0027,  ..., 0.0024, 0.0024, 0.0026],
          [0.0035, 0.0031, 0.0027,  ..., 0.0033, 0.0041, 0.0039],
          [0.0055, 0.0040, 0.0022,  ..., 0.0040, 0.0043, 0.0021]],

         [[0.0044, 0.0032, 0.0025,  ..., 0.0039, 0.0026, 0.0022],
          [0.0028, 0.0024, 0.0017,  ..., 0.0027, 0.0024, 0.0027],
          [0.0035, 0.0032, 0.0047,  ..., 0.0024, 0.0036, 0.0034],
          ...,
          [0.0039, 0.0038, 0.0022,  ..., 0.0024, 0.0022, 0.0021],
          [0.0023, 0.0024, 0.0020,  ..., 0.0046, 0.0022, 0.0029],
          [0.0021, 0.0029, 0.0030,  ..., 0.0032, 0.0039, 0.0028]],

         [[0.0016, 0.0028, 0.0040,  ..., 0.0052, 0.0031, 0.0038],
          [0.0032, 0.0022, 0.0037,  ..., 0.0017, 0.0033, 0.0034],
          [0.0025, 0.0047, 0.0019,  ..., 0.0018, 0.0023, 0.0021],
          ...,
          [0.0050, 0.0021, 0.0029,  ..., 0.0023, 0.0028, 0.0035],
          [0.0057, 0.0017, 0.0044,  ..., 0.0047, 0.0042, 0.0023],
          [0.0023, 0.0039, 0.0025,  ..., 0.0027, 0.0024, 0.0021]]],


        [[[0.0027, 0.0026, 0.0018,  ..., 0.0016, 0.0040, 0.0022],
          [0.0039, 0.0021, 0.0024,  ..., 0.0047, 0.0037, 0.0042],
          [0.0042, 0.0024, 0.0026,  ..., 0.0035, 0.0033, 0.0046],
          ...,
          [0.0042, 0.0021, 0.0023,  ..., 0.0043, 0.0032, 0.0029],
          [0.0024, 0.0025, 0.0031,  ..., 0.0039, 0.0042, 0.0064],
          [0.0029, 0.0035, 0.0025,  ..., 0.0039, 0.0041, 0.0059]],

         [[0.0044, 0.0022, 0.0035,  ..., 0.0026, 0.0025, 0.0017],
          [0.0026, 0.0025, 0.0035,  ..., 0.0045, 0.0024, 0.0027],
          [0.0057, 0.0037, 0.0027,  ..., 0.0027, 0.0026, 0.0037],
          ...,
          [0.0020, 0.0021, 0.0047,  ..., 0.0026, 0.0025, 0.0046],
          [0.0051, 0.0028, 0.0039,  ..., 0.0027, 0.0030, 0.0034],
          [0.0030, 0.0034, 0.0024,  ..., 0.0020, 0.0014, 0.0044]],

         [[0.0037, 0.0032, 0.0029,  ..., 0.0034, 0.0028, 0.0035],
          [0.0021, 0.0030, 0.0034,  ..., 0.0025, 0.0029, 0.0064],
          [0.0033, 0.0042, 0.0017,  ..., 0.0038, 0.0033, 0.0023],
          ...,
          [0.0035, 0.0035, 0.0013,  ..., 0.0044, 0.0035, 0.0016],
          [0.0040, 0.0041, 0.0061,  ..., 0.0018, 0.0025, 0.0053],
          [0.0038, 0.0025, 0.0035,  ..., 0.0021, 0.0037, 0.0031]],

         [[0.0027, 0.0033, 0.0043,  ..., 0.0046, 0.0026, 0.0033],
          [0.0033, 0.0039, 0.0039,  ..., 0.0026, 0.0035, 0.0047],
          [0.0026, 0.0022, 0.0031,  ..., 0.0020, 0.0030, 0.0043],
          ...,
          [0.0042, 0.0039, 0.0016,  ..., 0.0031, 0.0023, 0.0022],
          [0.0028, 0.0031, 0.0036,  ..., 0.0031, 0.0028, 0.0023],
          [0.0026, 0.0028, 0.0062,  ..., 0.0038, 0.0024, 0.0018]]]],
       grad_fn=<SoftmaxBackward0>), 'ultralow_spatial_attention': tensor([[[[0.0042, 0.0045, 0.0025,  ..., 0.0035, 0.0018, 0.0018],
          [0.0019, 0.0026, 0.0035,  ..., 0.0019, 0.0016, 0.0042],
          [0.0034, 0.0016, 0.0032,  ..., 0.0023, 0.0032, 0.0030],
          ...,
          [0.0020, 0.0021, 0.0024,  ..., 0.0026, 0.0022, 0.0032],
          [0.0031, 0.0020, 0.0048,  ..., 0.0030, 0.0064, 0.0046],
          [0.0022, 0.0039, 0.0042,  ..., 0.0023, 0.0015, 0.0028]],

         [[0.0043, 0.0020, 0.0020,  ..., 0.0021, 0.0058, 0.0041],
          [0.0043, 0.0030, 0.0026,  ..., 0.0030, 0.0048, 0.0022],
          [0.0064, 0.0029, 0.0026,  ..., 0.0025, 0.0066, 0.0029],
          ...,
          [0.0024, 0.0050, 0.0044,  ..., 0.0040, 0.0044, 0.0033],
          [0.0037, 0.0027, 0.0015,  ..., 0.0034, 0.0017, 0.0035],
          [0.0032, 0.0024, 0.0023,  ..., 0.0020, 0.0067, 0.0040]],

         [[0.0022, 0.0021, 0.0024,  ..., 0.0038, 0.0029, 0.0046],
          [0.0028, 0.0036, 0.0043,  ..., 0.0028, 0.0021, 0.0031],
          [0.0038, 0.0029, 0.0031,  ..., 0.0039, 0.0030, 0.0030],
          ...,
          [0.0030, 0.0029, 0.0050,  ..., 0.0034, 0.0022, 0.0028],
          [0.0022, 0.0022, 0.0022,  ..., 0.0015, 0.0028, 0.0028],
          [0.0027, 0.0025, 0.0018,  ..., 0.0016, 0.0022, 0.0026]],

         [[0.0036, 0.0047, 0.0026,  ..., 0.0030, 0.0019, 0.0037],
          [0.0039, 0.0031, 0.0026,  ..., 0.0018, 0.0035, 0.0040],
          [0.0028, 0.0022, 0.0031,  ..., 0.0029, 0.0048, 0.0051],
          ...,
          [0.0032, 0.0024, 0.0016,  ..., 0.0037, 0.0041, 0.0015],
          [0.0021, 0.0021, 0.0024,  ..., 0.0033, 0.0015, 0.0071],
          [0.0016, 0.0050, 0.0029,  ..., 0.0037, 0.0030, 0.0028]]],


        [[[0.0030, 0.0056, 0.0028,  ..., 0.0027, 0.0035, 0.0029],
          [0.0036, 0.0034, 0.0038,  ..., 0.0030, 0.0026, 0.0035],
          [0.0037, 0.0041, 0.0021,  ..., 0.0036, 0.0024, 0.0034],
          ...,
          [0.0048, 0.0018, 0.0026,  ..., 0.0031, 0.0020, 0.0035],
          [0.0022, 0.0026, 0.0034,  ..., 0.0025, 0.0041, 0.0028],
          [0.0039, 0.0022, 0.0026,  ..., 0.0037, 0.0024, 0.0032]],

         [[0.0026, 0.0032, 0.0032,  ..., 0.0032, 0.0013, 0.0021],
          [0.0043, 0.0017, 0.0026,  ..., 0.0022, 0.0062, 0.0022],
          [0.0017, 0.0055, 0.0072,  ..., 0.0032, 0.0019, 0.0051],
          ...,
          [0.0037, 0.0023, 0.0030,  ..., 0.0030, 0.0032, 0.0023],
          [0.0015, 0.0051, 0.0042,  ..., 0.0037, 0.0020, 0.0060],
          [0.0059, 0.0020, 0.0023,  ..., 0.0035, 0.0030, 0.0034]],

         [[0.0028, 0.0025, 0.0030,  ..., 0.0026, 0.0032, 0.0036],
          [0.0028, 0.0025, 0.0052,  ..., 0.0020, 0.0039, 0.0043],
          [0.0037, 0.0022, 0.0021,  ..., 0.0025, 0.0032, 0.0016],
          ...,
          [0.0024, 0.0019, 0.0049,  ..., 0.0022, 0.0037, 0.0039],
          [0.0047, 0.0031, 0.0014,  ..., 0.0045, 0.0019, 0.0025],
          [0.0034, 0.0030, 0.0033,  ..., 0.0020, 0.0032, 0.0040]],

         [[0.0021, 0.0037, 0.0024,  ..., 0.0037, 0.0033, 0.0027],
          [0.0030, 0.0075, 0.0032,  ..., 0.0044, 0.0018, 0.0024],
          [0.0051, 0.0074, 0.0043,  ..., 0.0021, 0.0025, 0.0029],
          ...,
          [0.0025, 0.0036, 0.0033,  ..., 0.0027, 0.0036, 0.0058],
          [0.0016, 0.0015, 0.0030,  ..., 0.0032, 0.0049, 0.0016],
          [0.0062, 0.0024, 0.0028,  ..., 0.0028, 0.0022, 0.0044]]],


        [[[0.0045, 0.0031, 0.0044,  ..., 0.0031, 0.0104, 0.0039],
          [0.0022, 0.0042, 0.0029,  ..., 0.0017, 0.0041, 0.0040],
          [0.0023, 0.0026, 0.0047,  ..., 0.0018, 0.0027, 0.0029],
          ...,
          [0.0026, 0.0023, 0.0032,  ..., 0.0024, 0.0021, 0.0025],
          [0.0030, 0.0052, 0.0033,  ..., 0.0032, 0.0028, 0.0027],
          [0.0026, 0.0043, 0.0022,  ..., 0.0030, 0.0035, 0.0040]],

         [[0.0046, 0.0026, 0.0034,  ..., 0.0045, 0.0021, 0.0029],
          [0.0033, 0.0029, 0.0057,  ..., 0.0027, 0.0024, 0.0034],
          [0.0021, 0.0060, 0.0022,  ..., 0.0032, 0.0042, 0.0026],
          ...,
          [0.0027, 0.0029, 0.0038,  ..., 0.0030, 0.0027, 0.0026],
          [0.0016, 0.0037, 0.0028,  ..., 0.0030, 0.0044, 0.0032],
          [0.0026, 0.0033, 0.0036,  ..., 0.0038, 0.0038, 0.0029]],

         [[0.0018, 0.0028, 0.0054,  ..., 0.0026, 0.0036, 0.0023],
          [0.0049, 0.0037, 0.0014,  ..., 0.0032, 0.0036, 0.0028],
          [0.0029, 0.0026, 0.0032,  ..., 0.0028, 0.0025, 0.0022],
          ...,
          [0.0029, 0.0048, 0.0028,  ..., 0.0031, 0.0033, 0.0030],
          [0.0047, 0.0023, 0.0013,  ..., 0.0027, 0.0017, 0.0033],
          [0.0041, 0.0031, 0.0028,  ..., 0.0029, 0.0035, 0.0030]],

         [[0.0021, 0.0032, 0.0041,  ..., 0.0025, 0.0037, 0.0030],
          [0.0038, 0.0030, 0.0042,  ..., 0.0034, 0.0042, 0.0046],
          [0.0049, 0.0023, 0.0047,  ..., 0.0022, 0.0024, 0.0015],
          ...,
          [0.0029, 0.0030, 0.0025,  ..., 0.0028, 0.0027, 0.0031],
          [0.0030, 0.0029, 0.0052,  ..., 0.0067, 0.0043, 0.0028],
          [0.0014, 0.0048, 0.0034,  ..., 0.0034, 0.0056, 0.0029]]],


        ...,


        [[[0.0041, 0.0024, 0.0015,  ..., 0.0023, 0.0024, 0.0036],
          [0.0031, 0.0047, 0.0037,  ..., 0.0025, 0.0014, 0.0025],
          [0.0037, 0.0022, 0.0019,  ..., 0.0040, 0.0031, 0.0036],
          ...,
          [0.0049, 0.0023, 0.0025,  ..., 0.0038, 0.0041, 0.0037],
          [0.0040, 0.0023, 0.0022,  ..., 0.0032, 0.0037, 0.0039],
          [0.0034, 0.0042, 0.0026,  ..., 0.0026, 0.0026, 0.0035]],

         [[0.0028, 0.0034, 0.0035,  ..., 0.0019, 0.0018, 0.0038],
          [0.0024, 0.0043, 0.0027,  ..., 0.0023, 0.0025, 0.0025],
          [0.0030, 0.0042, 0.0037,  ..., 0.0022, 0.0027, 0.0023],
          ...,
          [0.0024, 0.0033, 0.0028,  ..., 0.0030, 0.0022, 0.0023],
          [0.0034, 0.0054, 0.0046,  ..., 0.0040, 0.0041, 0.0025],
          [0.0033, 0.0034, 0.0035,  ..., 0.0038, 0.0032, 0.0026]],

         [[0.0024, 0.0028, 0.0047,  ..., 0.0041, 0.0041, 0.0030],
          [0.0041, 0.0042, 0.0042,  ..., 0.0031, 0.0028, 0.0030],
          [0.0033, 0.0031, 0.0029,  ..., 0.0021, 0.0026, 0.0033],
          ...,
          [0.0046, 0.0041, 0.0041,  ..., 0.0035, 0.0041, 0.0031],
          [0.0032, 0.0031, 0.0021,  ..., 0.0026, 0.0032, 0.0028],
          [0.0027, 0.0030, 0.0046,  ..., 0.0041, 0.0046, 0.0046]],

         [[0.0024, 0.0037, 0.0024,  ..., 0.0024, 0.0032, 0.0020],
          [0.0032, 0.0022, 0.0028,  ..., 0.0039, 0.0029, 0.0034],
          [0.0024, 0.0023, 0.0039,  ..., 0.0034, 0.0060, 0.0026],
          ...,
          [0.0022, 0.0040, 0.0030,  ..., 0.0032, 0.0032, 0.0022],
          [0.0018, 0.0041, 0.0036,  ..., 0.0031, 0.0024, 0.0017],
          [0.0018, 0.0031, 0.0033,  ..., 0.0028, 0.0024, 0.0029]]],


        [[[0.0051, 0.0018, 0.0055,  ..., 0.0023, 0.0042, 0.0039],
          [0.0044, 0.0028, 0.0024,  ..., 0.0036, 0.0032, 0.0034],
          [0.0035, 0.0039, 0.0028,  ..., 0.0025, 0.0035, 0.0024],
          ...,
          [0.0031, 0.0028, 0.0024,  ..., 0.0034, 0.0034, 0.0043],
          [0.0030, 0.0022, 0.0038,  ..., 0.0027, 0.0037, 0.0034],
          [0.0025, 0.0048, 0.0033,  ..., 0.0021, 0.0032, 0.0035]],

         [[0.0039, 0.0025, 0.0018,  ..., 0.0056, 0.0037, 0.0034],
          [0.0026, 0.0037, 0.0024,  ..., 0.0031, 0.0020, 0.0026],
          [0.0052, 0.0039, 0.0017,  ..., 0.0055, 0.0040, 0.0021],
          ...,
          [0.0025, 0.0023, 0.0027,  ..., 0.0036, 0.0042, 0.0050],
          [0.0024, 0.0021, 0.0046,  ..., 0.0030, 0.0019, 0.0036],
          [0.0020, 0.0022, 0.0050,  ..., 0.0029, 0.0022, 0.0036]],

         [[0.0026, 0.0024, 0.0017,  ..., 0.0026, 0.0039, 0.0031],
          [0.0030, 0.0047, 0.0048,  ..., 0.0028, 0.0040, 0.0033],
          [0.0029, 0.0017, 0.0036,  ..., 0.0050, 0.0030, 0.0020],
          ...,
          [0.0048, 0.0028, 0.0023,  ..., 0.0040, 0.0034, 0.0037],
          [0.0030, 0.0016, 0.0029,  ..., 0.0021, 0.0024, 0.0018],
          [0.0026, 0.0022, 0.0029,  ..., 0.0036, 0.0032, 0.0023]],

         [[0.0023, 0.0036, 0.0021,  ..., 0.0049, 0.0017, 0.0033],
          [0.0027, 0.0029, 0.0034,  ..., 0.0026, 0.0018, 0.0027],
          [0.0024, 0.0026, 0.0033,  ..., 0.0039, 0.0037, 0.0032],
          ...,
          [0.0025, 0.0018, 0.0050,  ..., 0.0027, 0.0030, 0.0025],
          [0.0025, 0.0042, 0.0033,  ..., 0.0035, 0.0022, 0.0023],
          [0.0024, 0.0033, 0.0026,  ..., 0.0032, 0.0032, 0.0041]]],


        [[[0.0039, 0.0041, 0.0028,  ..., 0.0032, 0.0021, 0.0017],
          [0.0025, 0.0034, 0.0033,  ..., 0.0020, 0.0037, 0.0040],
          [0.0026, 0.0045, 0.0036,  ..., 0.0035, 0.0027, 0.0027],
          ...,
          [0.0031, 0.0038, 0.0026,  ..., 0.0024, 0.0025, 0.0035],
          [0.0027, 0.0012, 0.0020,  ..., 0.0038, 0.0049, 0.0031],
          [0.0023, 0.0024, 0.0033,  ..., 0.0031, 0.0023, 0.0036]],

         [[0.0017, 0.0039, 0.0033,  ..., 0.0020, 0.0037, 0.0042],
          [0.0038, 0.0047, 0.0023,  ..., 0.0047, 0.0035, 0.0027],
          [0.0026, 0.0060, 0.0030,  ..., 0.0050, 0.0020, 0.0029],
          ...,
          [0.0041, 0.0013, 0.0019,  ..., 0.0022, 0.0053, 0.0037],
          [0.0046, 0.0035, 0.0021,  ..., 0.0035, 0.0032, 0.0022],
          [0.0016, 0.0068, 0.0040,  ..., 0.0041, 0.0032, 0.0028]],

         [[0.0039, 0.0028, 0.0034,  ..., 0.0026, 0.0050, 0.0033],
          [0.0036, 0.0037, 0.0025,  ..., 0.0035, 0.0033, 0.0023],
          [0.0028, 0.0025, 0.0030,  ..., 0.0036, 0.0033, 0.0027],
          ...,
          [0.0016, 0.0034, 0.0029,  ..., 0.0022, 0.0035, 0.0046],
          [0.0037, 0.0014, 0.0019,  ..., 0.0031, 0.0018, 0.0028],
          [0.0038, 0.0028, 0.0044,  ..., 0.0033, 0.0038, 0.0035]],

         [[0.0029, 0.0021, 0.0035,  ..., 0.0047, 0.0025, 0.0052],
          [0.0029, 0.0065, 0.0034,  ..., 0.0016, 0.0025, 0.0021],
          [0.0024, 0.0026, 0.0028,  ..., 0.0029, 0.0025, 0.0049],
          ...,
          [0.0016, 0.0024, 0.0026,  ..., 0.0033, 0.0044, 0.0019],
          [0.0014, 0.0042, 0.0023,  ..., 0.0014, 0.0031, 0.0022],
          [0.0031, 0.0041, 0.0042,  ..., 0.0020, 0.0022, 0.0044]]]],
       grad_fn=<SoftmaxBackward0>)}
torch.Size([8, 1]) torch.Size([8, 4, 316, 316]) torch.Size([8, 4, 316, 316]) torch.Size([8, 4, 316, 316])
There are no checkpoints or weights from previous steps
Single GPU or CPU training
self.gpu: 0
self.device: cuda
moved model to: cuda:0
mbbn
using binary_classification loss
using spatial_difference loss
Starting epoch 1/1
max allocated memory: 72.84 MB
max cached memory: 94.00 MB
  0%|          | 0/183 [00:00<?, ?it/s]  1%|          | 1/183 [00:03<09:08,  3.01s/it]  1%|          | 2/183 [00:04<05:57,  1.97s/it]  2%|‚ñè         | 3/183 [00:05<04:54,  1.64s/it]  2%|‚ñè         | 4/183 [00:06<04:25,  1.48s/it]  3%|‚ñé         | 5/183 [00:08<04:12,  1.42s/it]  3%|‚ñé         | 6/183 [00:09<04:02,  1.37s/it]  4%|‚ñç         | 7/183 [00:10<03:54,  1.33s/it]  4%|‚ñç         | 8/183 [00:11<03:47,  1.30s/it]  5%|‚ñç         | 9/183 [00:13<03:42,  1.28s/it]  5%|‚ñå         | 10/183 [00:15<04:39,  1.61s/it]  6%|‚ñå         | 11/183 [00:16<04:21,  1.52s/it]  7%|‚ñã         | 12/183 [00:18<04:07,  1.45s/it]  7%|‚ñã         | 13/183 [00:19<03:59,  1.41s/it]  8%|‚ñä         | 14/183 [00:20<03:52,  1.38s/it]  8%|‚ñä         | 15/183 [00:21<03:44,  1.34s/it]  9%|‚ñä         | 16/183 [00:23<03:38,  1.31s/it]  9%|‚ñâ         | 17/183 [00:24<03:35,  1.30s/it] 10%|‚ñâ         | 18/183 [00:25<03:31,  1.28s/it] 10%|‚ñà         | 19/183 [00:26<03:32,  1.29s/it] 11%|‚ñà         | 20/183 [00:29<04:30,  1.66s/it] 11%|‚ñà‚ñè        | 21/183 [00:30<04:08,  1.54s/it] 12%|‚ñà‚ñè        | 22/183 [00:31<03:53,  1.45s/it] 13%|‚ñà‚ñé        | 23/183 [00:33<03:45,  1.41s/it] 13%|‚ñà‚ñé        | 24/183 [00:34<03:35,  1.36s/it] 14%|‚ñà‚ñé        | 25/183 [00:35<03:28,  1.32s/it] 14%|‚ñà‚ñç        | 26/183 [00:37<03:39,  1.40s/it] 15%|‚ñà‚ñç        | 27/183 [00:38<03:34,  1.38s/it] 15%|‚ñà‚ñå        | 28/183 [00:39<03:31,  1.37s/it] 16%|‚ñà‚ñå        | 29/183 [00:41<03:25,  1.33s/it] 16%|‚ñà‚ñã        | 30/183 [00:43<04:13,  1.66s/it] 17%|‚ñà‚ñã        | 31/183 [00:44<03:57,  1.56s/it] 17%|‚ñà‚ñã        | 32/183 [00:46<03:41,  1.47s/it] 17%|‚ñà‚ñã        | 32/183 [00:47<03:43,  1.48s/it]
NaN detected in loss spatial_difference for ['Seoul-SNU_sub-DNO23LSM', 'NYSPI-Columbia-Adults_sub-simpaocd307000', 'Barcelone-Bellvitge-ANTIGA-1.5T_sub-subIDIBELL15P38', 'Braga-UMinho-Braga-3T_sub-MRI202002211BART053', 'Barcelona-HCPB_sub-052', 'Bangalore-NIMHANS_sub-ODP048', 'Yale-Pittinger-Yale-2014_sub-Yale2014AdultOCDtr9746', 'Yale-Pittinger-Yale-2014_sub-Yale2014AdultOCDtb0221']
at spatial_difference loss
['Seoul-SNU_sub-DNO23LSM', 'NYSPI-Columbia-Adults_sub-simpaocd307000', 'Barcelone-Bellvitge-ANTIGA-1.5T_sub-subIDIBELL15P38', 'Braga-UMinho-Braga-3T_sub-MRI202002211BART053', 'Barcelona-HCPB_sub-052', 'Bangalore-NIMHANS_sub-ODP048', 'Yale-Pittinger-Yale-2014_sub-Yale2014AdultOCDtr9746', 'Yale-Pittinger-Yale-2014_sub-Yale2014AdultOCDtb0221']
current_nan_list: {'NYSPI-Columbia-Adults_sub-simpaocd307000', 'Barcelona-HCPB_sub-052', 'Seoul-SNU_sub-DNO23LSM', 'Bangalore-NIMHANS_sub-ODP048', 'Barcelone-Bellvitge-ANTIGA-1.5T_sub-subIDIBELL15P38', 'Yale-Pittinger-Yale-2014_sub-Yale2014AdultOCDtr9746', 'Braga-UMinho-Braga-3T_sub-MRI202002211BART053', 'Yale-Pittinger-Yale-2014_sub-Yale2014AdultOCDtb0221'}
NaN detected in loss spatial_difference for ['Braga-UMinho-Braga-3T_sub-MRI201906172OCDREP001', 'Seoul-SNU_sub-UMO27LCY', 'Barcelone-Bellvitge-ANTIGA-1.5T_sub-subIDIBELL15P18', 'Braga-UMinho-Braga-1.5T-act_sub-MRI201510071OCD096', 'Amsterdam-VUmc_sub-916092', 'Bangalore-NIMHANS_sub-C0071', 'Barcelone-Bellvitge-PROV-1.5T_sub-subIDIBELL15268', 'Brazil_sub-C001906']
at spatial_difference loss
['Braga-UMinho-Braga-3T_sub-MRI201906172OCDREP001', 'Seoul-SNU_sub-UMO27LCY', 'Barcelone-Bellvitge-ANTIGA-1.5T_sub-subIDIBELL15P18', 'Braga-UMinho-Braga-1.5T-act_sub-MRI201510071OCD096', 'Amsterdam-VUmc_sub-916092', 'Bangalore-NIMHANS_sub-C0071', 'Barcelone-Bellvitge-PROV-1.5T_sub-subIDIBELL15268', 'Brazil_sub-C001906']
current_nan_list: {'NYSPI-Columbia-Adults_sub-simpaocd307000', 'Barcelona-HCPB_sub-052', 'Braga-UMinho-Braga-1.5T-act_sub-MRI201510071OCD096', 'Brazil_sub-C001906', 'Seoul-SNU_sub-DNO23LSM', 'Bangalore-NIMHANS_sub-ODP048', 'Braga-UMinho-Braga-3T_sub-MRI201906172OCDREP001', 'Barcelone-Bellvitge-PROV-1.5T_sub-subIDIBELL15268', 'Barcelone-Bellvitge-ANTIGA-1.5T_sub-subIDIBELL15P38', 'Amsterdam-VUmc_sub-916092', 'Yale-Pittinger-Yale-2014_sub-Yale2014AdultOCDtr9746', 'Bangalore-NIMHANS_sub-C0071', 'Braga-UMinho-Braga-3T_sub-MRI202002211BART053', 'Seoul-SNU_sub-UMO27LCY', 'Yale-Pittinger-Yale-2014_sub-Yale2014AdultOCDtb0221', 'Barcelone-Bellvitge-ANTIGA-1.5T_sub-subIDIBELL15P18'}
NaN detected in loss spatial_difference for ['Shanghai-SMCH_sub-056', 'Bangalore-NIMHANS_sub-C0075', 'Yale-Pittinger-Yale-2014_sub-Yale2014AdultOCDtb0314', 'Seoul-SNU_sub-NOR66LDY', 'Dresden_sub-GEROME3028', 'Seoul-SNU_sub-UMO68LSJ', 'Barcelona-HCPB_sub-050', 'Bangalore-NIMHANS_sub-C0034']
at spatial_difference loss
['Shanghai-SMCH_sub-056', 'Bangalore-NIMHANS_sub-C0075', 'Yale-Pittinger-Yale-2014_sub-Yale2014AdultOCDtb0314', 'Seoul-SNU_sub-NOR66LDY', 'Dresden_sub-GEROME3028', 'Seoul-SNU_sub-UMO68LSJ', 'Barcelona-HCPB_sub-050', 'Bangalore-NIMHANS_sub-C0034']
current_nan_list: {'Yale-Pittinger-Yale-2014_sub-Yale2014AdultOCDtb0314', 'Barcelona-HCPB_sub-050', 'Bangalore-NIMHANS_sub-C0071', 'Braga-UMinho-Braga-3T_sub-MRI202002211BART053', 'Braga-UMinho-Braga-1.5T-act_sub-MRI201510071OCD096', 'Bangalore-NIMHANS_sub-C0034', 'Seoul-SNU_sub-DNO23LSM', 'Bangalore-NIMHANS_sub-ODP048', 'Seoul-SNU_sub-UMO68LSJ', 'Braga-UMinho-Braga-3T_sub-MRI201906172OCDREP001', 'Dresden_sub-GEROME3028', 'Brazil_sub-C001906', 'Bangalore-NIMHANS_sub-C0075', 'Barcelone-Bellvitge-ANTIGA-1.5T_sub-subIDIBELL15P18', 'Barcelone-Bellvitge-ANTIGA-1.5T_sub-subIDIBELL15P38', 'Barcelone-Bellvitge-PROV-1.5T_sub-subIDIBELL15268', 'Yale-Pittinger-Yale-2014_sub-Yale2014AdultOCDtb0221', 'NYSPI-Columbia-Adults_sub-simpaocd307000', 'Barcelona-HCPB_sub-052', 'Shanghai-SMCH_sub-056', 'Seoul-SNU_sub-NOR66LDY', 'Yale-Pittinger-Yale-2014_sub-Yale2014AdultOCDtr9746', 'Seoul-SNU_sub-UMO27LCY', 'Amsterdam-VUmc_sub-916092'}
NaN detected in loss spatial_difference for ['Seoul-SNU_sub-UMO61JCY', 'Bergen_sub-00047', 'Chiba-CHBSRPB_sub-OCD088', 'Barcelone-Bellvitge-PROV-1.5T_sub-subIDIBELL15C307', 'Chiba-CHB_sub-OCD099', 'Chiba-CHBSRPB_sub-MADHC059', 'Bergen_sub-00002', 'Brazil_sub-P00328420170524']
at spatial_difference loss
['Seoul-SNU_sub-UMO61JCY', 'Bergen_sub-00047', 'Chiba-CHBSRPB_sub-OCD088', 'Barcelone-Bellvitge-PROV-1.5T_sub-subIDIBELL15C307', 'Chiba-CHB_sub-OCD099', 'Chiba-CHBSRPB_sub-MADHC059', 'Bergen_sub-00002', 'Brazil_sub-P00328420170524']
current_nan_list: {'Yale-Pittinger-Yale-2014_sub-Yale2014AdultOCDtb0314', 'Barcelona-HCPB_sub-050', 'Bangalore-NIMHANS_sub-C0071', 'Bergen_sub-00047', 'Braga-UMinho-Braga-3T_sub-MRI202002211BART053', 'Braga-UMinho-Braga-1.5T-act_sub-MRI201510071OCD096', 'Bangalore-NIMHANS_sub-C0034', 'Seoul-SNU_sub-DNO23LSM', 'Bangalore-NIMHANS_sub-ODP048', 'Seoul-SNU_sub-UMO68LSJ', 'Bergen_sub-00002', 'Chiba-CHB_sub-OCD099', 'Chiba-CHBSRPB_sub-OCD088', 'Braga-UMinho-Braga-3T_sub-MRI201906172OCDREP001', 'Dresden_sub-GEROME3028', 'Brazil_sub-C001906', 'Brazil_sub-P00328420170524', 'Bangalore-NIMHANS_sub-C0075', 'Chiba-CHBSRPB_sub-MADHC059', 'Barcelone-Bellvitge-ANTIGA-1.5T_sub-subIDIBELL15P18', 'Barcelone-Bellvitge-ANTIGA-1.5T_sub-subIDIBELL15P38', 'Barcelone-Bellvitge-PROV-1.5T_sub-subIDIBELL15268', 'Yale-Pittinger-Yale-2014_sub-Yale2014AdultOCDtb0221', 'Barcelone-Bellvitge-PROV-1.5T_sub-subIDIBELL15C307', 'Seoul-SNU_sub-UMO61JCY', 'NYSPI-Columbia-Adults_sub-simpaocd307000', 'Barcelona-HCPB_sub-052', 'Shanghai-SMCH_sub-056', 'Seoul-SNU_sub-NOR66LDY', 'Yale-Pittinger-Yale-2014_sub-Yale2014AdultOCDtr9746', 'Seoul-SNU_sub-UMO27LCY', 'Amsterdam-VUmc_sub-916092'}
NaN detected in loss spatial_difference for ['Seoul-SNU_sub-DNO38BYU', 'NYSPI-Columbia-Adults_sub-simpaocd817000', 'Braga-UMinho-Braga-1.5T_sub-MRI201506082OCD079', 'Bangalore-NIMHANS_sub-C0055', 'Barcelona-HCPB_sub-C0111', 'Bangalore-NIMHANS_sub-ODP173', 'Bangalore-NIMHANS_sub-C0108', 'Shanghai-SMCH_sub-020']
at spatial_difference loss
['Seoul-SNU_sub-DNO38BYU', 'NYSPI-Columbia-Adults_sub-simpaocd817000', 'Braga-UMinho-Braga-1.5T_sub-MRI201506082OCD079', 'Bangalore-NIMHANS_sub-C0055', 'Barcelona-HCPB_sub-C0111', 'Bangalore-NIMHANS_sub-ODP173', 'Bangalore-NIMHANS_sub-C0108', 'Shanghai-SMCH_sub-020']
current_nan_list: {'Yale-Pittinger-Yale-2014_sub-Yale2014AdultOCDtb0314', 'Seoul-SNU_sub-DNO38BYU', 'NYSPI-Columbia-Adults_sub-simpaocd817000', 'Bangalore-NIMHANS_sub-ODP173', 'Barcelona-HCPB_sub-050', 'Bangalore-NIMHANS_sub-C0071', 'Bergen_sub-00047', 'Bangalore-NIMHANS_sub-C0055', 'Braga-UMinho-Braga-3T_sub-MRI202002211BART053', 'Braga-UMinho-Braga-1.5T-act_sub-MRI201510071OCD096', 'Bangalore-NIMHANS_sub-C0034', 'Seoul-SNU_sub-DNO23LSM', 'Bangalore-NIMHANS_sub-ODP048', 'Seoul-SNU_sub-UMO68LSJ', 'Bergen_sub-00002', 'Chiba-CHB_sub-OCD099', 'Chiba-CHBSRPB_sub-OCD088', 'Braga-UMinho-Braga-3T_sub-MRI201906172OCDREP001', 'Dresden_sub-GEROME3028', 'Barcelona-HCPB_sub-C0111', 'Brazil_sub-C001906', 'Brazil_sub-P00328420170524', 'Bangalore-NIMHANS_sub-C0075', 'Chiba-CHBSRPB_sub-MADHC059', 'Barcelone-Bellvitge-ANTIGA-1.5T_sub-subIDIBELL15P18', 'Braga-UMinho-Braga-1.5T_sub-MRI201506082OCD079', 'Barcelone-Bellvitge-ANTIGA-1.5T_sub-subIDIBELL15P38', 'Barcelone-Bellvitge-PROV-1.5T_sub-subIDIBELL15268', 'Yale-Pittinger-Yale-2014_sub-Yale2014AdultOCDtb0221', 'Barcelone-Bellvitge-PROV-1.5T_sub-subIDIBELL15C307', 'Seoul-SNU_sub-UMO61JCY', 'NYSPI-Columbia-Adults_sub-simpaocd307000', 'Barcelona-HCPB_sub-052', 'Shanghai-SMCH_sub-056', 'Bangalore-NIMHANS_sub-C0108', 'Shanghai-SMCH_sub-020', 'Seoul-SNU_sub-NOR66LDY', 'Yale-Pittinger-Yale-2014_sub-Yale2014AdultOCDtr9746', 'Seoul-SNU_sub-UMO27LCY', 'Amsterdam-VUmc_sub-916092'}
NaN detected in loss spatial_difference for ['Braga-UMinho-Braga-1.5T-act_sub-MRI201302132OCDREP002', 'Seoul-SNU_sub-NOR112YIS', 'Bergen_sub-00067', 'UCLA_sub-AOCD068', 'Shanghai-SMCH_sub-016', 'Braga-UMinho-Braga-3T_sub-MRI201911082BART042', 'Kyoto-KPU-Kyoto3T_sub-subKyoto3Tsubj0004', 'Vancouver-BCCHR_sub-059']
at spatial_difference loss
['Braga-UMinho-Braga-1.5T-act_sub-MRI201302132OCDREP002', 'Seoul-SNU_sub-NOR112YIS', 'Bergen_sub-00067', 'UCLA_sub-AOCD068', 'Shanghai-SMCH_sub-016', 'Braga-UMinho-Braga-3T_sub-MRI201911082BART042', 'Kyoto-KPU-Kyoto3T_sub-subKyoto3Tsubj0004', 'Vancouver-BCCHR_sub-059']
current_nan_list: {'Yale-Pittinger-Yale-2014_sub-Yale2014AdultOCDtb0314', 'Seoul-SNU_sub-DNO38BYU', 'NYSPI-Columbia-Adults_sub-simpaocd817000', 'Bangalore-NIMHANS_sub-ODP173', 'Barcelona-HCPB_sub-050', 'Bangalore-NIMHANS_sub-C0071', 'Bergen_sub-00047', 'Bangalore-NIMHANS_sub-C0055', 'Braga-UMinho-Braga-3T_sub-MRI202002211BART053', 'Bergen_sub-00067', 'Braga-UMinho-Braga-1.5T-act_sub-MRI201510071OCD096', 'Bangalore-NIMHANS_sub-C0034', 'Seoul-SNU_sub-DNO23LSM', 'Bangalore-NIMHANS_sub-ODP048', 'Seoul-SNU_sub-UMO68LSJ', 'Bergen_sub-00002', 'Shanghai-SMCH_sub-016', 'Seoul-SNU_sub-NOR112YIS', 'Chiba-CHB_sub-OCD099', 'Chiba-CHBSRPB_sub-OCD088', 'Braga-UMinho-Braga-3T_sub-MRI201906172OCDREP001', 'Dresden_sub-GEROME3028', 'Barcelona-HCPB_sub-C0111', 'Brazil_sub-C001906', 'Brazil_sub-P00328420170524', 'Bangalore-NIMHANS_sub-C0075', 'Chiba-CHBSRPB_sub-MADHC059', 'Vancouver-BCCHR_sub-059', 'Barcelone-Bellvitge-ANTIGA-1.5T_sub-subIDIBELL15P18', 'Braga-UMinho-Braga-3T_sub-MRI201911082BART042', 'Braga-UMinho-Braga-1.5T_sub-MRI201506082OCD079', 'Barcelone-Bellvitge-ANTIGA-1.5T_sub-subIDIBELL15P38', 'Kyoto-KPU-Kyoto3T_sub-subKyoto3Tsubj0004', 'Barcelone-Bellvitge-PROV-1.5T_sub-subIDIBELL15268', 'UCLA_sub-AOCD068', 'Yale-Pittinger-Yale-2014_sub-Yale2014AdultOCDtb0221', 'Barcelone-Bellvitge-PROV-1.5T_sub-subIDIBELL15C307', 'Seoul-SNU_sub-UMO61JCY', 'NYSPI-Columbia-Adults_sub-simpaocd307000', 'Barcelona-HCPB_sub-052', 'Braga-UMinho-Braga-1.5T-act_sub-MRI201302132OCDREP002', 'Shanghai-SMCH_sub-056', 'Bangalore-NIMHANS_sub-C0108', 'Shanghai-SMCH_sub-020', 'Seoul-SNU_sub-NOR66LDY', 'Yale-Pittinger-Yale-2014_sub-Yale2014AdultOCDtr9746', 'Seoul-SNU_sub-UMO27LCY', 'Amsterdam-VUmc_sub-916092'}
NaN detected in loss spatial_difference for ['Bangalore-NIMHANS_sub-C0230', 'Seoul-SNU_sub-DNO10YBI', 'Barcelona-HCPB_sub-019', 'Bangalore-NIMHANS_sub-C0222', 'Barcelone-Bellvitge-ANTIGA-1.5T_sub-subIDIBELL15P16', 'Dresden_sub-GEROME3008', 'Chiba-CHBC_sub-OCDC047', 'Barcelone-Bellvitge-PROV-1.5T_sub-subIDIBELL15C293']
at spatial_difference loss
['Bangalore-NIMHANS_sub-C0230', 'Seoul-SNU_sub-DNO10YBI', 'Barcelona-HCPB_sub-019', 'Bangalore-NIMHANS_sub-C0222', 'Barcelone-Bellvitge-ANTIGA-1.5T_sub-subIDIBELL15P16', 'Dresden_sub-GEROME3008', 'Chiba-CHBC_sub-OCDC047', 'Barcelone-Bellvitge-PROV-1.5T_sub-subIDIBELL15C293']
current_nan_list: {'Yale-Pittinger-Yale-2014_sub-Yale2014AdultOCDtb0314', 'Seoul-SNU_sub-DNO38BYU', 'NYSPI-Columbia-Adults_sub-simpaocd817000', 'Bangalore-NIMHANS_sub-ODP173', 'Barcelona-HCPB_sub-050', 'Barcelone-Bellvitge-PROV-1.5T_sub-subIDIBELL15C293', 'Dresden_sub-GEROME3008', 'Barcelone-Bellvitge-ANTIGA-1.5T_sub-subIDIBELL15P16', 'Bangalore-NIMHANS_sub-C0071', 'Bergen_sub-00047', 'Bangalore-NIMHANS_sub-C0055', 'Braga-UMinho-Braga-3T_sub-MRI202002211BART053', 'Bergen_sub-00067', 'Braga-UMinho-Braga-1.5T-act_sub-MRI201510071OCD096', 'Bangalore-NIMHANS_sub-C0034', 'Seoul-SNU_sub-DNO23LSM', 'Bangalore-NIMHANS_sub-ODP048', 'Seoul-SNU_sub-UMO68LSJ', 'Bergen_sub-00002', 'Shanghai-SMCH_sub-016', 'Seoul-SNU_sub-NOR112YIS', 'Seoul-SNU_sub-DNO10YBI', 'Chiba-CHB_sub-OCD099', 'Chiba-CHBSRPB_sub-OCD088', 'Braga-UMinho-Braga-3T_sub-MRI201906172OCDREP001', 'Dresden_sub-GEROME3028', 'Barcelona-HCPB_sub-C0111', 'Brazil_sub-C001906', 'Brazil_sub-P00328420170524', 'Bangalore-NIMHANS_sub-C0075', 'Chiba-CHBSRPB_sub-MADHC059', 'Vancouver-BCCHR_sub-059', 'Barcelone-Bellvitge-ANTIGA-1.5T_sub-subIDIBELL15P18', 'Braga-UMinho-Braga-3T_sub-MRI201911082BART042', 'Braga-UMinho-Braga-1.5T_sub-MRI201506082OCD079', 'Barcelone-Bellvitge-ANTIGA-1.5T_sub-subIDIBELL15P38', 'Kyoto-KPU-Kyoto3T_sub-subKyoto3Tsubj0004', 'Chiba-CHBC_sub-OCDC047', 'Bangalore-NIMHANS_sub-C0230', 'Bangalore-NIMHANS_sub-C0222', 'Barcelone-Bellvitge-PROV-1.5T_sub-subIDIBELL15268', 'UCLA_sub-AOCD068', 'Yale-Pittinger-Yale-2014_sub-Yale2014AdultOCDtb0221', 'Barcelone-Bellvitge-PROV-1.5T_sub-subIDIBELL15C307', 'Seoul-SNU_sub-UMO61JCY', 'NYSPI-Columbia-Adults_sub-simpaocd307000', 'Barcelona-HCPB_sub-052', 'Braga-UMinho-Braga-1.5T-act_sub-MRI201302132OCDREP002', 'Shanghai-SMCH_sub-056', 'Bangalore-NIMHANS_sub-C0108', 'Shanghai-SMCH_sub-020', 'Seoul-SNU_sub-NOR66LDY', 'Yale-Pittinger-Yale-2014_sub-Yale2014AdultOCDtr9746', 'Seoul-SNU_sub-UMO27LCY', 'Barcelona-HCPB_sub-019', 'Amsterdam-VUmc_sub-916092'}
NaN detected in loss spatial_difference for ['Bangalore-NIMHANS_sub-ODP054', 'Seoul-SNU_sub-UMO58PYA', 'Bangalore-NIMHANS_sub-ODP219', 'Vancouver-BCCHR_sub-064', 'Seoul-SNU_sub-NOR16KYJ', 'Chiba-CHBC_sub-HCC003', 'Kyoto-KPU-Kyoto1.5T_sub-subKyoto15Tsubj0124', 'Chiba-CHBC_sub-HCC030']
at spatial_difference loss
['Bangalore-NIMHANS_sub-ODP054', 'Seoul-SNU_sub-UMO58PYA', 'Bangalore-NIMHANS_sub-ODP219', 'Vancouver-BCCHR_sub-064', 'Seoul-SNU_sub-NOR16KYJ', 'Chiba-CHBC_sub-HCC003', 'Kyoto-KPU-Kyoto1.5T_sub-subKyoto15Tsubj0124', 'Chiba-CHBC_sub-HCC030']
current_nan_list: {'Yale-Pittinger-Yale-2014_sub-Yale2014AdultOCDtb0314', 'Seoul-SNU_sub-DNO38BYU', 'NYSPI-Columbia-Adults_sub-simpaocd817000', 'Bangalore-NIMHANS_sub-ODP173', 'Barcelona-HCPB_sub-050', 'Barcelone-Bellvitge-PROV-1.5T_sub-subIDIBELL15C293', 'Dresden_sub-GEROME3008', 'Barcelone-Bellvitge-ANTIGA-1.5T_sub-subIDIBELL15P16', 'Bangalore-NIMHANS_sub-ODP054', 'Bangalore-NIMHANS_sub-C0071', 'Bergen_sub-00047', 'Bangalore-NIMHANS_sub-C0055', 'Braga-UMinho-Braga-3T_sub-MRI202002211BART053', 'Bergen_sub-00067', 'Braga-UMinho-Braga-1.5T-act_sub-MRI201510071OCD096', 'Bangalore-NIMHANS_sub-C0034', 'Seoul-SNU_sub-DNO23LSM', 'Bangalore-NIMHANS_sub-ODP048', 'Seoul-SNU_sub-UMO68LSJ', 'Bergen_sub-00002', 'Shanghai-SMCH_sub-016', 'Seoul-SNU_sub-NOR112YIS', 'Seoul-SNU_sub-DNO10YBI', 'Seoul-SNU_sub-NOR16KYJ', 'Chiba-CHB_sub-OCD099', 'Chiba-CHBSRPB_sub-OCD088', 'Braga-UMinho-Braga-3T_sub-MRI201906172OCDREP001', 'Dresden_sub-GEROME3028', 'Kyoto-KPU-Kyoto1.5T_sub-subKyoto15Tsubj0124', 'Barcelona-HCPB_sub-C0111', 'Brazil_sub-C001906', 'Brazil_sub-P00328420170524', 'Bangalore-NIMHANS_sub-C0075', 'Chiba-CHBSRPB_sub-MADHC059', 'Vancouver-BCCHR_sub-059', 'Barcelone-Bellvitge-ANTIGA-1.5T_sub-subIDIBELL15P18', 'Braga-UMinho-Braga-3T_sub-MRI201911082BART042', 'Seoul-SNU_sub-UMO58PYA', 'Braga-UMinho-Braga-1.5T_sub-MRI201506082OCD079', 'Barcelone-Bellvitge-ANTIGA-1.5T_sub-subIDIBELL15P38', 'Kyoto-KPU-Kyoto3T_sub-subKyoto3Tsubj0004', 'Chiba-CHBC_sub-OCDC047', 'Bangalore-NIMHANS_sub-C0230', 'Bangalore-NIMHANS_sub-C0222', 'Barcelone-Bellvitge-PROV-1.5T_sub-subIDIBELL15268', 'UCLA_sub-AOCD068', 'Chiba-CHBC_sub-HCC030', 'Yale-Pittinger-Yale-2014_sub-Yale2014AdultOCDtb0221', 'Barcelone-Bellvitge-PROV-1.5T_sub-subIDIBELL15C307', 'Seoul-SNU_sub-UMO61JCY', 'NYSPI-Columbia-Adults_sub-simpaocd307000', 'Barcelona-HCPB_sub-052', 'Braga-UMinho-Braga-1.5T-act_sub-MRI201302132OCDREP002', 'Chiba-CHBC_sub-HCC003', 'Shanghai-SMCH_sub-056', 'Bangalore-NIMHANS_sub-C0108', 'Shanghai-SMCH_sub-020', 'Seoul-SNU_sub-NOR66LDY', 'Bangalore-NIMHANS_sub-ODP219', 'Yale-Pittinger-Yale-2014_sub-Yale2014AdultOCDtr9746', 'Seoul-SNU_sub-UMO27LCY', 'Barcelona-HCPB_sub-019', 'Amsterdam-VUmc_sub-916092', 'Vancouver-BCCHR_sub-064'}
NaN detected in loss spatial_difference for ['Barcelona-HCPB_sub-C0077', 'Amsterdam-VUmc_sub-916053', 'Amsterdam-VUmc_sub-916065', 'Bangalore-NIMHANS_sub-ODP064', 'Barcelona-HCPB_sub-C0056', 'Chiba-CHBSRPB_sub-MADHC067', 'Seoul-SNU_sub-NOR62MBS', 'Bangalore-NIMHANS_sub-ODP020']
at spatial_difference loss
['Barcelona-HCPB_sub-C0077', 'Amsterdam-VUmc_sub-916053', 'Amsterdam-VUmc_sub-916065', 'Bangalore-NIMHANS_sub-ODP064', 'Barcelona-HCPB_sub-C0056', 'Chiba-CHBSRPB_sub-MADHC067', 'Seoul-SNU_sub-NOR62MBS', 'Bangalore-NIMHANS_sub-ODP020']
current_nan_list: {'Amsterdam-VUmc_sub-916053', 'Yale-Pittinger-Yale-2014_sub-Yale2014AdultOCDtb0314', 'Seoul-SNU_sub-DNO38BYU', 'NYSPI-Columbia-Adults_sub-simpaocd817000', 'Seoul-SNU_sub-NOR62MBS', 'Barcelona-HCPB_sub-C0077', 'Bangalore-NIMHANS_sub-ODP173', 'Barcelona-HCPB_sub-050', 'Barcelone-Bellvitge-PROV-1.5T_sub-subIDIBELL15C293', 'Dresden_sub-GEROME3008', 'Barcelone-Bellvitge-ANTIGA-1.5T_sub-subIDIBELL15P16', 'Bangalore-NIMHANS_sub-ODP054', 'Bangalore-NIMHANS_sub-C0071', 'Bergen_sub-00047', 'Bangalore-NIMHANS_sub-C0055', 'Braga-UMinho-Braga-3T_sub-MRI202002211BART053', 'Bergen_sub-00067', 'Braga-UMinho-Braga-1.5T-act_sub-MRI201510071OCD096', 'Bangalore-NIMHANS_sub-C0034', 'Barcelona-HCPB_sub-C0056', 'Seoul-SNU_sub-DNO23LSM', 'Bangalore-NIMHANS_sub-ODP048', 'Seoul-SNU_sub-UMO68LSJ', 'Bergen_sub-00002', 'Shanghai-SMCH_sub-016', 'Seoul-SNU_sub-NOR112YIS', 'Seoul-SNU_sub-DNO10YBI', 'Seoul-SNU_sub-NOR16KYJ', 'Amsterdam-VUmc_sub-916065', 'Chiba-CHB_sub-OCD099', 'Chiba-CHBSRPB_sub-OCD088', 'Braga-UMinho-Braga-3T_sub-MRI201906172OCDREP001', 'Dresden_sub-GEROME3028', 'Kyoto-KPU-Kyoto1.5T_sub-subKyoto15Tsubj0124', 'Bangalore-NIMHANS_sub-ODP020', 'Barcelona-HCPB_sub-C0111', 'Brazil_sub-C001906', 'Brazil_sub-P00328420170524', 'Bangalore-NIMHANS_sub-C0075', 'Chiba-CHBSRPB_sub-MADHC059', 'Vancouver-BCCHR_sub-059', 'Barcelone-Bellvitge-ANTIGA-1.5T_sub-subIDIBELL15P18', 'Braga-UMinho-Braga-3T_sub-MRI201911082BART042', 'Seoul-SNU_sub-UMO58PYA', 'Braga-UMinho-Braga-1.5T_sub-MRI201506082OCD079', 'Barcelone-Bellvitge-ANTIGA-1.5T_sub-subIDIBELL15P38', 'Kyoto-KPU-Kyoto3T_sub-subKyoto3Tsubj0004', 'Chiba-CHBC_sub-OCDC047', 'Bangalore-NIMHANS_sub-C0230', 'Bangalore-NIMHANS_sub-C0222', 'Barcelone-Bellvitge-PROV-1.5T_sub-subIDIBELL15268', 'UCLA_sub-AOCD068', 'Bangalore-NIMHANS_sub-ODP064', 'Chiba-CHBC_sub-HCC030', 'Yale-Pittinger-Yale-2014_sub-Yale2014AdultOCDtb0221', 'Barcelone-Bellvitge-PROV-1.5T_sub-subIDIBELL15C307', 'Seoul-SNU_sub-UMO61JCY', 'NYSPI-Columbia-Adults_sub-simpaocd307000', 'Barcelona-HCPB_sub-052', 'Braga-UMinho-Braga-1.5T-act_sub-MRI201302132OCDREP002', 'Chiba-CHBC_sub-HCC003', 'Shanghai-SMCH_sub-056', 'Bangalore-NIMHANS_sub-C0108', 'Shanghai-SMCH_sub-020', 'Seoul-SNU_sub-NOR66LDY', 'Bangalore-NIMHANS_sub-ODP219', 'Chiba-CHBSRPB_sub-MADHC067', 'Yale-Pittinger-Yale-2014_sub-Yale2014AdultOCDtr9746', 'Seoul-SNU_sub-UMO27LCY', 'Barcelona-HCPB_sub-019', 'Amsterdam-VUmc_sub-916092', 'Vancouver-BCCHR_sub-064'}
NaN detected in loss spatial_difference for ['Seoul-SNU_sub-DNO11LSH', 'NYSPI-Columbia-Adults_sub-simpaocd847000', 'Kyoto-KPU-Kyoto3T_sub-subKyoto3Tsubj0015', 'Barcelone-Bellvitge-ANTIGA-1.5T_sub-subIDIBELL15C011', 'Braga-UMinho-Braga-3T_sub-MRI201909032BART029', 'Yale-Pittinger-HCP-Prisma_sub-YaleHCPPrismapb3546', 'Chiba-CHB_sub-OCD098', 'Braga-UMinho-Braga-1.5T-act_sub-MRI201205234OCD001']
at spatial_difference loss
['Seoul-SNU_sub-DNO11LSH', 'NYSPI-Columbia-Adults_sub-simpaocd847000', 'Kyoto-KPU-Kyoto3T_sub-subKyoto3Tsubj0015', 'Barcelone-Bellvitge-ANTIGA-1.5T_sub-subIDIBELL15C011', 'Braga-UMinho-Braga-3T_sub-MRI201909032BART029', 'Yale-Pittinger-HCP-Prisma_sub-YaleHCPPrismapb3546', 'Chiba-CHB_sub-OCD098', 'Braga-UMinho-Braga-1.5T-act_sub-MRI201205234OCD001']
current_nan_list: {'Bergen_sub-00047', 'Bergen_sub-00067', 'Chiba-CHB_sub-OCD098', 'Bergen_sub-00002', 'Chiba-CHB_sub-OCD099', 'Barcelona-HCPB_sub-C0111', 'Brazil_sub-P00328420170524', 'Bangalore-NIMHANS_sub-C0075', 'NYSPI-Columbia-Adults_sub-simpaocd847000', 'Yale-Pittinger-HCP-Prisma_sub-YaleHCPPrismapb3546', 'Chiba-CHBC_sub-OCDC047', 'Barcelone-Bellvitge-PROV-1.5T_sub-subIDIBELL15268', 'Yale-Pittinger-Yale-2014_sub-Yale2014AdultOCDtb0221', 'NYSPI-Columbia-Adults_sub-simpaocd307000', 'Barcelona-HCPB_sub-052', 'Shanghai-SMCH_sub-056', 'Bangalore-NIMHANS_sub-C0108', 'Kyoto-KPU-Kyoto3T_sub-subKyoto3Tsubj0015', 'Yale-Pittinger-Yale-2014_sub-Yale2014AdultOCDtr9746', 'Seoul-SNU_sub-DNO11LSH', 'Barcelone-Bellvitge-ANTIGA-1.5T_sub-subIDIBELL15P18', 'Vancouver-BCCHR_sub-064', 'Amsterdam-VUmc_sub-916053', 'Bangalore-NIMHANS_sub-ODP173', 'Bangalore-NIMHANS_sub-ODP054', 'Braga-UMinho-Braga-3T_sub-MRI202002211BART053', 'Bangalore-NIMHANS_sub-C0034', 'Barcelona-HCPB_sub-C0056', 'Seoul-SNU_sub-DNO23LSM', 'Bangalore-NIMHANS_sub-ODP048', 'Shanghai-SMCH_sub-016', 'Seoul-SNU_sub-DNO10YBI', 'Seoul-SNU_sub-NOR16KYJ', 'Bangalore-NIMHANS_sub-ODP020', 'Brazil_sub-C001906', 'Braga-UMinho-Braga-1.5T_sub-MRI201506082OCD079', 'Kyoto-KPU-Kyoto3T_sub-subKyoto3Tsubj0004', 'Bangalore-NIMHANS_sub-C0222', 'Seoul-SNU_sub-UMO61JCY', 'Braga-UMinho-Braga-1.5T-act_sub-MRI201205234OCD001', 'NYSPI-Columbia-Adults_sub-simpaocd817000', 'Barcelona-HCPB_sub-C0077', 'Bangalore-NIMHANS_sub-C0055', 'Seoul-SNU_sub-UMO68LSJ', 'Seoul-SNU_sub-NOR112YIS', 'Chiba-CHBSRPB_sub-MADHC059', 'Vancouver-BCCHR_sub-059', 'Braga-UMinho-Braga-3T_sub-MRI201911082BART042', 'Seoul-SNU_sub-UMO58PYA', 'Bangalore-NIMHANS_sub-ODP064', 'Bangalore-NIMHANS_sub-C0230', 'UCLA_sub-AOCD068', 'Chiba-CHBC_sub-HCC030', 'Barcelone-Bellvitge-PROV-1.5T_sub-subIDIBELL15C307', 'Chiba-CHBC_sub-HCC003', 'Shanghai-SMCH_sub-020', 'Seoul-SNU_sub-UMO27LCY', 'Kyoto-KPU-Kyoto1.5T_sub-subKyoto15Tsubj0124', 'Yale-Pittinger-Yale-2014_sub-Yale2014AdultOCDtb0314', 'Seoul-SNU_sub-DNO38BYU', 'Seoul-SNU_sub-NOR62MBS', 'Barcelona-HCPB_sub-050', 'Barcelone-Bellvitge-PROV-1.5T_sub-subIDIBELL15C293', 'Dresden_sub-GEROME3008', 'Barcelone-Bellvitge-ANTIGA-1.5T_sub-subIDIBELL15P16', 'Bangalore-NIMHANS_sub-C0071', 'Braga-UMinho-Braga-1.5T-act_sub-MRI201510071OCD096', 'Amsterdam-VUmc_sub-916065', 'Chiba-CHBSRPB_sub-OCD088', 'Braga-UMinho-Braga-3T_sub-MRI201906172OCDREP001', 'Dresden_sub-GEROME3028', 'Barcelone-Bellvitge-ANTIGA-1.5T_sub-subIDIBELL15C011', 'Barcelone-Bellvitge-ANTIGA-1.5T_sub-subIDIBELL15P38', 'Braga-UMinho-Braga-1.5T-act_sub-MRI201302132OCDREP002', 'Seoul-SNU_sub-NOR66LDY', 'Bangalore-NIMHANS_sub-ODP219', 'Braga-UMinho-Braga-3T_sub-MRI201909032BART029', 'Chiba-CHBSRPB_sub-MADHC067', 'Barcelona-HCPB_sub-019', 'Amsterdam-VUmc_sub-916092'}
NaN detected in loss spatial_difference for ['Barcelone-Bellvitge-RESP-CBT-3T_sub-RESPCBT35CTRPRE', 'New-York_sub-subSEQ1NKISENR52', 'Shanghai-SMCH_sub-019', 'Chiba-CHB_sub-MADHC037', 'Yale-Pittinger-Yale-2014_sub-Yale2014AdultOCDtr8123', 'Seoul-SNU_sub-NOR56YIW', 'Shanghai-SMCH_sub-070', 'Barcelone-Bellvitge-RESP-CBT-3T_sub-RESPCBT03CTRPOST']
at spatial_difference loss
['Barcelone-Bellvitge-RESP-CBT-3T_sub-RESPCBT35CTRPRE', 'New-York_sub-subSEQ1NKISENR52', 'Shanghai-SMCH_sub-019', 'Chiba-CHB_sub-MADHC037', 'Yale-Pittinger-Yale-2014_sub-Yale2014AdultOCDtr8123', 'Seoul-SNU_sub-NOR56YIW', 'Shanghai-SMCH_sub-070', 'Barcelone-Bellvitge-RESP-CBT-3T_sub-RESPCBT03CTRPOST']
current_nan_list: {'Bergen_sub-00047', 'Bergen_sub-00067', 'Chiba-CHB_sub-OCD098', 'Bergen_sub-00002', 'New-York_sub-subSEQ1NKISENR52', 'Chiba-CHB_sub-OCD099', 'Shanghai-SMCH_sub-019', 'Barcelona-HCPB_sub-C0111', 'Brazil_sub-P00328420170524', 'Bangalore-NIMHANS_sub-C0075', 'NYSPI-Columbia-Adults_sub-simpaocd847000', 'Yale-Pittinger-HCP-Prisma_sub-YaleHCPPrismapb3546', 'Chiba-CHBC_sub-OCDC047', 'Barcelone-Bellvitge-PROV-1.5T_sub-subIDIBELL15268', 'Yale-Pittinger-Yale-2014_sub-Yale2014AdultOCDtb0221', 'NYSPI-Columbia-Adults_sub-simpaocd307000', 'Barcelona-HCPB_sub-052', 'Shanghai-SMCH_sub-056', 'Bangalore-NIMHANS_sub-C0108', 'Kyoto-KPU-Kyoto3T_sub-subKyoto3Tsubj0015', 'Yale-Pittinger-Yale-2014_sub-Yale2014AdultOCDtr9746', 'Seoul-SNU_sub-DNO11LSH', 'Barcelone-Bellvitge-ANTIGA-1.5T_sub-subIDIBELL15P18', 'Vancouver-BCCHR_sub-064', 'Amsterdam-VUmc_sub-916053', 'Yale-Pittinger-Yale-2014_sub-Yale2014AdultOCDtr8123', 'Bangalore-NIMHANS_sub-ODP173', 'Barcelone-Bellvitge-RESP-CBT-3T_sub-RESPCBT03CTRPOST', 'Bangalore-NIMHANS_sub-ODP054', 'Seoul-SNU_sub-NOR56YIW', 'Braga-UMinho-Braga-3T_sub-MRI202002211BART053', 'Bangalore-NIMHANS_sub-C0034', 'Barcelona-HCPB_sub-C0056', 'Seoul-SNU_sub-DNO23LSM', 'Bangalore-NIMHANS_sub-ODP048', 'Shanghai-SMCH_sub-016', 'Seoul-SNU_sub-DNO10YBI', 'Seoul-SNU_sub-NOR16KYJ', 'Bangalore-NIMHANS_sub-ODP020', 'Brazil_sub-C001906', 'Braga-UMinho-Braga-1.5T_sub-MRI201506082OCD079', 'Kyoto-KPU-Kyoto3T_sub-subKyoto3Tsubj0004', 'Shanghai-SMCH_sub-070', 'Bangalore-NIMHANS_sub-C0222', 'Seoul-SNU_sub-UMO61JCY', 'Braga-UMinho-Braga-1.5T-act_sub-MRI201205234OCD001', 'NYSPI-Columbia-Adults_sub-simpaocd817000', 'Barcelona-HCPB_sub-C0077', 'Barcelone-Bellvitge-RESP-CBT-3T_sub-RESPCBT35CTRPRE', 'Bangalore-NIMHANS_sub-C0055', 'Seoul-SNU_sub-UMO68LSJ', 'Seoul-SNU_sub-NOR112YIS', 'Chiba-CHBSRPB_sub-MADHC059', 'Vancouver-BCCHR_sub-059', 'Braga-UMinho-Braga-3T_sub-MRI201911082BART042', 'Seoul-SNU_sub-UMO58PYA', 'Bangalore-NIMHANS_sub-ODP064', 'Bangalore-NIMHANS_sub-C0230', 'UCLA_sub-AOCD068', 'Chiba-CHBC_sub-HCC030', 'Barcelone-Bellvitge-PROV-1.5T_sub-subIDIBELL15C307', 'Chiba-CHBC_sub-HCC003', 'Shanghai-SMCH_sub-020', 'Seoul-SNU_sub-UMO27LCY', 'Kyoto-KPU-Kyoto1.5T_sub-subKyoto15Tsubj0124', 'Yale-Pittinger-Yale-2014_sub-Yale2014AdultOCDtb0314', 'Seoul-SNU_sub-DNO38BYU', 'Seoul-SNU_sub-NOR62MBS', 'Barcelona-HCPB_sub-050', 'Barcelone-Bellvitge-PROV-1.5T_sub-subIDIBELL15C293', 'Dresden_sub-GEROME3008', 'Barcelone-Bellvitge-ANTIGA-1.5T_sub-subIDIBELL15P16', 'Chiba-CHB_sub-MADHC037', 'Bangalore-NIMHANS_sub-C0071', 'Braga-UMinho-Braga-1.5T-act_sub-MRI201510071OCD096', 'Amsterdam-VUmc_sub-916065', 'Chiba-CHBSRPB_sub-OCD088', 'Braga-UMinho-Braga-3T_sub-MRI201906172OCDREP001', 'Dresden_sub-GEROME3028', 'Barcelone-Bellvitge-ANTIGA-1.5T_sub-subIDIBELL15C011', 'Barcelone-Bellvitge-ANTIGA-1.5T_sub-subIDIBELL15P38', 'Braga-UMinho-Braga-1.5T-act_sub-MRI201302132OCDREP002', 'Seoul-SNU_sub-NOR66LDY', 'Bangalore-NIMHANS_sub-ODP219', 'Braga-UMinho-Braga-3T_sub-MRI201909032BART029', 'Chiba-CHBSRPB_sub-MADHC067', 'Barcelona-HCPB_sub-019', 'Amsterdam-VUmc_sub-916092'}
NaN detected in loss spatial_difference for ['Bangalore-NIMHANS_sub-ODP016', 'Braga-UMinho-Braga-3T_sub-MRI201905211BART003', 'NYSPI-Columbia-Pediatric_sub-DJLMYG', 'UCLA_sub-AOCD015', 'Bangalore-NIMHANS_sub-ODP234', 'Seoul-SNU_sub-NOR73KMS', 'Yale-Gruner_sub-S0377SEA', 'Chiba-CHB_sub-MADHC024']
at spatial_difference loss
['Bangalore-NIMHANS_sub-ODP016', 'Braga-UMinho-Braga-3T_sub-MRI201905211BART003', 'NYSPI-Columbia-Pediatric_sub-DJLMYG', 'UCLA_sub-AOCD015', 'Bangalore-NIMHANS_sub-ODP234', 'Seoul-SNU_sub-NOR73KMS', 'Yale-Gruner_sub-S0377SEA', 'Chiba-CHB_sub-MADHC024']
current_nan_list: {'Bangalore-NIMHANS_sub-ODP016', 'Bergen_sub-00047', 'Bergen_sub-00067', 'Chiba-CHB_sub-OCD098', 'Bergen_sub-00002', 'Chiba-CHB_sub-MADHC024', 'NYSPI-Columbia-Pediatric_sub-DJLMYG', 'New-York_sub-subSEQ1NKISENR52', 'Chiba-CHB_sub-OCD099', 'Shanghai-SMCH_sub-019', 'Barcelona-HCPB_sub-C0111', 'Brazil_sub-P00328420170524', 'Bangalore-NIMHANS_sub-C0075', 'NYSPI-Columbia-Adults_sub-simpaocd847000', 'Yale-Pittinger-HCP-Prisma_sub-YaleHCPPrismapb3546', 'Chiba-CHBC_sub-OCDC047', 'Barcelone-Bellvitge-PROV-1.5T_sub-subIDIBELL15268', 'Yale-Pittinger-Yale-2014_sub-Yale2014AdultOCDtb0221', 'NYSPI-Columbia-Adults_sub-simpaocd307000', 'Barcelona-HCPB_sub-052', 'Shanghai-SMCH_sub-056', 'Bangalore-NIMHANS_sub-C0108', 'Kyoto-KPU-Kyoto3T_sub-subKyoto3Tsubj0015', 'Yale-Pittinger-Yale-2014_sub-Yale2014AdultOCDtr9746', 'Seoul-SNU_sub-DNO11LSH', 'Barcelone-Bellvitge-ANTIGA-1.5T_sub-subIDIBELL15P18', 'Vancouver-BCCHR_sub-064', 'Amsterdam-VUmc_sub-916053', 'Yale-Pittinger-Yale-2014_sub-Yale2014AdultOCDtr8123', 'Bangalore-NIMHANS_sub-ODP173', 'Barcelone-Bellvitge-RESP-CBT-3T_sub-RESPCBT03CTRPOST', 'Bangalore-NIMHANS_sub-ODP054', 'Seoul-SNU_sub-NOR56YIW', 'Braga-UMinho-Braga-3T_sub-MRI202002211BART053', 'Braga-UMinho-Braga-3T_sub-MRI201905211BART003', 'Bangalore-NIMHANS_sub-C0034', 'Barcelona-HCPB_sub-C0056', 'Seoul-SNU_sub-DNO23LSM', 'Bangalore-NIMHANS_sub-ODP048', 'Shanghai-SMCH_sub-016', 'Seoul-SNU_sub-DNO10YBI', 'Seoul-SNU_sub-NOR16KYJ', 'Bangalore-NIMHANS_sub-ODP020', 'Brazil_sub-C001906', 'Braga-UMinho-Braga-1.5T_sub-MRI201506082OCD079', 'Kyoto-KPU-Kyoto3T_sub-subKyoto3Tsubj0004', 'Shanghai-SMCH_sub-070', 'Bangalore-NIMHANS_sub-C0222', 'Seoul-SNU_sub-UMO61JCY', 'Braga-UMinho-Braga-1.5T-act_sub-MRI201205234OCD001', 'Seoul-SNU_sub-NOR73KMS', 'NYSPI-Columbia-Adults_sub-simpaocd817000', 'Barcelona-HCPB_sub-C0077', 'Barcelone-Bellvitge-RESP-CBT-3T_sub-RESPCBT35CTRPRE', 'Bangalore-NIMHANS_sub-C0055', 'Seoul-SNU_sub-UMO68LSJ', 'Seoul-SNU_sub-NOR112YIS', 'Bangalore-NIMHANS_sub-ODP234', 'Chiba-CHBSRPB_sub-MADHC059', 'Vancouver-BCCHR_sub-059', 'Braga-UMinho-Braga-3T_sub-MRI201911082BART042', 'Seoul-SNU_sub-UMO58PYA', 'Bangalore-NIMHANS_sub-ODP064', 'Bangalore-NIMHANS_sub-C0230', 'UCLA_sub-AOCD068', 'Chiba-CHBC_sub-HCC030', 'Barcelone-Bellvitge-PROV-1.5T_sub-subIDIBELL15C307', 'Chiba-CHBC_sub-HCC003', 'Yale-Gruner_sub-S0377SEA', 'Shanghai-SMCH_sub-020', 'Seoul-SNU_sub-UMO27LCY', 'Kyoto-KPU-Kyoto1.5T_sub-subKyoto15Tsubj0124', 'Yale-Pittinger-Yale-2014_sub-Yale2014AdultOCDtb0314', 'Seoul-SNU_sub-DNO38BYU', 'Seoul-SNU_sub-NOR62MBS', 'Barcelona-HCPB_sub-050', 'Barcelone-Bellvitge-PROV-1.5T_sub-subIDIBELL15C293', 'Dresden_sub-GEROME3008', 'Barcelone-Bellvitge-ANTIGA-1.5T_sub-subIDIBELL15P16', 'Chiba-CHB_sub-MADHC037', 'Bangalore-NIMHANS_sub-C0071', 'Braga-UMinho-Braga-1.5T-act_sub-MRI201510071OCD096', 'Amsterdam-VUmc_sub-916065', 'Chiba-CHBSRPB_sub-OCD088', 'Braga-UMinho-Braga-3T_sub-MRI201906172OCDREP001', 'Dresden_sub-GEROME3028', 'Barcelone-Bellvitge-ANTIGA-1.5T_sub-subIDIBELL15C011', 'Barcelone-Bellvitge-ANTIGA-1.5T_sub-subIDIBELL15P38', 'Braga-UMinho-Braga-1.5T-act_sub-MRI201302132OCDREP002', 'UCLA_sub-AOCD015', 'Seoul-SNU_sub-NOR66LDY', 'Bangalore-NIMHANS_sub-ODP219', 'Braga-UMinho-Braga-3T_sub-MRI201909032BART029', 'Chiba-CHBSRPB_sub-MADHC067', 'Barcelona-HCPB_sub-019', 'Amsterdam-VUmc_sub-916092'}
Traceback (most recent call last):
  File "/scratch/connectome/ycryu/ENIGMA_OCD_MBBN/MBBN-main/main.py", line 263, in <module>
    run_phase(args,model_weights_path,step,task)
  File "/scratch/connectome/ycryu/ENIGMA_OCD_MBBN/MBBN-main/main.py", line 219, in run_phase
    trainer.training()
  File "/scratch/connectome/ycryu/ENIGMA_OCD_MBBN/MBBN-main/trainer.py", line 355, in training
    self.train_epoch(epoch)
  File "/scratch/connectome/ycryu/ENIGMA_OCD_MBBN/MBBN-main/trainer.py", line 463, in train_epoch
    self.scaler.scale(loss).backward()
  File "/scratch/connectome/ycryu/.conda/envs/mbbn-env/lib/python3.9/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/scratch/connectome/ycryu/.conda/envs/mbbn-env/lib/python3.9/site-packages/torch/autograd/__init__.py", line 266, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
Traceback (most recent call last):
  File "/scratch/connectome/ycryu/ENIGMA_OCD_MBBN/MBBN-main/main.py", line 263, in <module>
    run_phase(args,model_weights_path,step,task)
  File "/scratch/connectome/ycryu/ENIGMA_OCD_MBBN/MBBN-main/main.py", line 219, in run_phase
    trainer.training()
  File "/scratch/connectome/ycryu/ENIGMA_OCD_MBBN/MBBN-main/trainer.py", line 355, in training
    self.train_epoch(epoch)
  File "/scratch/connectome/ycryu/ENIGMA_OCD_MBBN/MBBN-main/trainer.py", line 463, in train_epoch
    self.scaler.scale(loss).backward()
  File "/scratch/connectome/ycryu/.conda/envs/mbbn-env/lib/python3.9/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/scratch/connectome/ycryu/.conda/envs/mbbn-env/lib/python3.9/site-packages/torch/autograd/__init__.py", line 266, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
wandb: Waiting for W&B process to finish... (failed 255). Press Control-C to abort syncing.
wandb: - 0.013 MB of 0.013 MB uploaded (0.000 MB deduped)wandb: \ 0.013 MB of 0.013 MB uploaded (0.000 MB deduped)wandb: | 0.013 MB of 0.013 MB uploaded (0.000 MB deduped)wandb: / 0.013 MB of 0.013 MB uploaded (0.000 MB deduped)wandb: - 0.013 MB of 0.013 MB uploaded (0.000 MB deduped)wandb: \ 0.013 MB of 0.013 MB uploaded (0.000 MB deduped)wandb: | 0.013 MB of 0.013 MB uploaded (0.000 MB deduped)wandb: üöÄ View run ENIGMA_OCD_mbbn_OCD_layer_check_seed101 at: https://wandb.ai/youngchanryu-seoul-national-university/enigma-ocd_mbbn/runs/h7hs8fwd
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250130_074735-h7hs8fwd/logs
